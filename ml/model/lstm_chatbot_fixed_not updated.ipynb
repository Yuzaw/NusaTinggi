{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aE9QrVCxKcRc"
   },
   "source": [
    "Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vAwUIExmIT2u",
    "outputId": "a15b29ce-5ac3-426b-a17f-c49098f80d2c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import Libraries\n",
    "import json\n",
    "import nltk\n",
    "import time\n",
    "import random\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import IPython.display as ipd\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from tensorflow.keras.models import Model\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import Input, Embedding, LSTM, Dropout, Flatten, Dense, Bidirectional\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.models import load_model\n",
    "# import StemmerFactory class\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Mengunduh resource punkt dari NLTK\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hg3GZK-QKj7H"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CcuBs5pzcCHJ",
    "outputId": "2611d8a9-05d6-42cb-9327-f8952d6c2059"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt_tab')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "86FjGISZIrwP",
    "outputId": "cce355de-c388-4f0c-a30a-461579cbffd8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            patterns       tags\n",
      "0  Apa saja peralatan yang wajib dibawa saat mend...  Kebutuhan\n",
      "1   Barang apa saja yang harus ada di tas pendakian?  Kebutuhan\n",
      "2  Apa saja perlengkapan yang tidak boleh terlupa...  Kebutuhan\n",
      "3  Peralatan apa yang perlu dibawa saat mendaki g...  Kebutuhan\n",
      "4  Apa barang yang harus dibawa untuk pendakian g...  Kebutuhan\n",
      "180\n"
     ]
    }
   ],
   "source": [
    "# Load Dataset JSON\n",
    "with open('DatasetTest.json', 'r') as f:\n",
    "    dataset = json.load(f)\n",
    "# Inisialisasi variabel\n",
    "tags = []  # Data tag\n",
    "inputs = []  # Data input atau pattern\n",
    "responses = {}  # Data respon\n",
    "words = []  # Data kata\n",
    "classes = []  # Data kelas atau tag\n",
    "documents = []  # Data dokumen\n",
    "ignore_words = ['?', '!']  # Karakter yang akan diabaikan\n",
    "\n",
    "# Iterasi melalui intents dalam JSON\n",
    "for intent in dataset:\n",
    "    if 'intent' in intent and 'patterns' in intent and 'response' in intent:\n",
    "        responses[intent['intent']] = intent['response']  # Menyimpan respons berdasarkan intent\n",
    "\n",
    "        for pattern in intent['patterns']:\n",
    "            inputs.append(pattern)\n",
    "            tags.append(intent['intent'])\n",
    "\n",
    "            # Tokenisasi pola\n",
    "            w = nltk.word_tokenize(pattern)\n",
    "            words.extend(w)\n",
    "            documents.append((w, intent['intent']))\n",
    "\n",
    "            # Tambahkan intent ke dalam kelas jika belum ada\n",
    "            if intent['intent'] not in classes:\n",
    "                classes.append(intent['intent'])\n",
    "    else:\n",
    "        print(f\"Kesalahan struktur pada intent: {intent}\")\n",
    "\n",
    "# Membuat DataFrame dari data yang sudah diproses\n",
    "data = pd.DataFrame({\"patterns\": inputs, \"tags\": tags})\n",
    "\n",
    "# Menampilkan beberapa baris pertama DataFrame\n",
    "print(data.head())\n",
    "print(len(data))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "td-_2nYdKoKE"
   },
   "source": [
    "Preprocessing The Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AD8UfJXtKxGR"
   },
   "source": [
    "\n",
    "\n",
    "1.   Remove Punctuations\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "x6lINwAUK0jO"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            patterns       tags\n",
      "0  apa saja peralatan yang wajib dibawa saat mend...  Kebutuhan\n",
      "1    barang apa saja yang harus ada di tas pendakian  Kebutuhan\n",
      "2  apa saja perlengkapan yang tidak boleh terlupa...  Kebutuhan\n",
      "3  peralatan apa yang perlu dibawa saat mendaki g...  Kebutuhan\n",
      "4  apa barang yang harus dibawa untuk pendakian g...  Kebutuhan\n"
     ]
    }
   ],
   "source": [
    "# Removing Punctuations (Menghilangkan Punktuasi)\n",
    "data['patterns'] = data['patterns'].apply(lambda wrd:[ltrs.lower() for ltrs in wrd if ltrs not in string.punctuation])\n",
    "data['patterns'] = data['patterns'].apply(lambda wrd: ''.join(wrd))\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zqCokQuEK4It"
   },
   "source": [
    "2. Lemmatization: Converting words to their base or root form.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "x3kwUN4cK8nt",
    "outputId": "5c0e7ea7-5e18-478a-9aa7-c53ce80998de"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1509\n"
     ]
    }
   ],
   "source": [
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "factory = StemmerFactory()\n",
    "stemmer = factory.create_stemmer()\n",
    "data['patterns'] = [stemmer.stem(sentence) for sentence in data['patterns']]\n",
    "print(len(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            patterns       tags\n",
      "0     apa saja alat yang wajib bawa saat daki gunung  Kebutuhan\n",
      "1         barang apa saja yang harus ada di tas daki  Kebutuhan\n",
      "2  apa saja lengkap yang tidak boleh lupa saat hi...  Kebutuhan\n",
      "3          alat apa yang perlu bawa saat daki gunung  Kebutuhan\n",
      "4       apa barang yang harus bawa untuk daki gunung  Kebutuhan\n"
     ]
    }
   ],
   "source": [
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adat\n",
      "alergi\n",
      "nanti\n",
      "seperti\n",
      "tekan\n",
      "awal\n",
      "pandu\n",
      "wajib\n",
      "datang\n",
      "berapa\n",
      "kabar\n",
      "dekat\n",
      "butuh\n",
      "jejak\n",
      "daerah\n",
      "obat\n",
      "mudah\n",
      "pengaruh\n",
      "tentu\n",
      "kerinci\n",
      "festival\n",
      "lain\n",
      "baca\n",
      "serang\n",
      "apakah\n",
      "hari\n",
      "utama\n",
      "baik\n",
      "tradisi\n",
      "hemat\n",
      "patuh\n",
      "antisipasi\n",
      "aplikasi\n",
      "wisatawan\n",
      "budaya\n",
      "prau\n",
      "ritual\n",
      "sangat\n",
      "lupa\n",
      "mendung\n",
      "jumpa\n",
      "jelas\n",
      "selamat\n",
      "ciremai\n",
      "tolong\n",
      "estimasi\n",
      "alam\n",
      "bagus\n",
      "petik\n",
      "barang\n",
      "itu\n",
      "gabung\n",
      "bantu\n",
      "bawa\n",
      "arti\n",
      "dampak\n",
      "terima\n",
      "langgar\n",
      "jalan\n",
      "suka\n",
      "minum\n",
      "cegah\n",
      "interaksi\n",
      "tajam\n",
      "tas\n",
      "iya\n",
      "betul\n",
      "ratarata\n",
      "nali\n",
      "hewan\n",
      "elektrolit\n",
      "boleh\n",
      "malam\n",
      "a\n",
      "total\n",
      "canggih\n",
      "masih\n",
      "thanks\n",
      "tari\n",
      "bahaya\n",
      "legenda\n",
      "biasa\n",
      "yang\n",
      "benar\n",
      "halo\n",
      "basecamp\n",
      "dan\n",
      "usir\n",
      "tutup\n",
      "maksud\n",
      "kenal\n",
      "asalusul\n",
      "satu\n",
      "risiko\n",
      "ya\n",
      "sering\n",
      "selesai\n",
      "hubung\n",
      "sampai\n",
      "kasih\n",
      "nggak\n",
      "umum\n",
      "area\n",
      "jaga\n",
      "ambil\n",
      "ijen\n",
      "apa\n",
      "hujan\n",
      "sewa\n",
      "timbang\n",
      "lama\n",
      "mereka\n",
      "kabut\n",
      "merbabu\n",
      "datar\n",
      "akurat\n",
      "dur\n",
      "masyarakat\n",
      "ikut\n",
      "lindung\n",
      "dalam\n",
      "sedikit\n",
      "aman\n",
      "ke\n",
      "efisien\n",
      "liar\n",
      "atas\n",
      "ada\n",
      "ekstrem\n",
      "longsor\n",
      "bahan\n",
      "komunitas\n",
      "percaya\n",
      "kamu\n",
      "keluar\n",
      "alternatif\n",
      "naik\n",
      "tumbuh\n",
      "dehidrasi\n",
      "makan\n",
      "tahan\n",
      "cukup\n",
      "tibatiba\n",
      "sesuatu\n",
      "tradisional\n",
      "guna\n",
      "buat\n",
      "imbang\n",
      "halal\n",
      "cuaca\n",
      "tunjuk\n",
      "lebih\n",
      "hi\n",
      "jadi\n",
      "jalur\n",
      "banget\n",
      "kondisi\n",
      "lokal\n",
      "tenda\n",
      "tinggal\n",
      "tambah\n",
      "rasa\n",
      "bagai\n",
      "milik\n",
      "cocok\n",
      "tahu\n",
      "hadap\n",
      "kita\n",
      "tempat\n",
      "air\n",
      "saat\n",
      "puncak\n",
      "belum\n",
      "lokasi\n",
      "sejarah\n",
      "oke\n",
      "pakai\n",
      "tanpa\n",
      "bromo\n",
      "informasi\n",
      "sebab\n",
      "musim\n",
      "cerita\n",
      "kawah\n",
      "penting\n",
      "rinjani\n",
      "tubuh\n",
      "angin\n",
      "panjang\n",
      "hiking\n",
      "sore\n",
      "cepat\n",
      "alat\n",
      "oleh\n",
      "ini\n",
      "rendah\n",
      "waktu\n",
      "tuju\n",
      "harga\n",
      "gunung\n",
      "spiritual\n",
      "saja\n",
      "daki\n",
      "makasih\n",
      "racun\n",
      "pantau\n",
      "antara\n",
      "rencana\n",
      "perhati\n",
      "buka\n",
      "istiadat\n",
      "harus\n",
      "lestari\n",
      "porter\n",
      "nonhalal\n",
      "mana\n",
      "pada\n",
      "capai\n",
      "kabarselamat\n",
      "masuk\n",
      "perilaku\n",
      "khas\n",
      "jika\n",
      "mula\n",
      "nih\n",
      "di\n",
      "ukur\n",
      "bye\n",
      "laku\n",
      "banyak\n",
      "acara\n",
      "waspada\n",
      "lagi\n",
      "langkah\n",
      "khusus\n",
      "atur\n",
      "atau\n",
      "hindar\n",
      "tapi\n",
      "identifikasi\n",
      "tidur\n",
      "lot\n",
      "unik\n",
      "pilih\n",
      "ubah\n",
      "tingkat\n",
      "biaya\n",
      "tandatanda\n",
      "panas\n",
      "luar\n",
      "bagi\n",
      "hipotermia\n",
      "untuk\n",
      "dingin\n",
      "bentuk\n",
      "ngerti\n",
      "transportasi\n",
      "semua\n",
      "mitos\n",
      "kencang\n",
      "sulit\n",
      "soal\n",
      "guide\n",
      "efektif\n",
      "kira\n",
      "tidak\n",
      "hormat\n",
      "temu\n",
      "tentang\n",
      "dengan\n",
      "bingung\n",
      "gejala\n",
      "cara\n",
      "pantang\n",
      "salah\n",
      "kurang\n",
      "sekali\n",
      "dong\n",
      "ketemu\n",
      "coba\n",
      "dapat\n",
      "siap\n",
      "hai\n",
      "sekitar\n",
      "suhu\n",
      "prakira\n",
      "pendek\n",
      "saya\n",
      "kawasan\n",
      "bisa\n",
      "buruk\n",
      "deras\n",
      "aktivitas\n",
      "kait\n",
      "perlu\n",
      "tebal\n",
      "mineral\n",
      "rakyat\n",
      "pagi\n",
      "akomodasi\n",
      "diri\n",
      "dari\n",
      "bagaimana\n",
      "lengkap\n",
      "sedia\n",
      "udara\n",
      "agar\n",
      "Jumlah kata unik dalam data: 307\n"
     ]
    }
   ],
   "source": [
    "all_words = ' '.join(data['patterns']).split()\n",
    "unique_words = set(all_words)\n",
    "a = len(unique_words)\n",
    "for word in unique_words:\n",
    "    print(word)\n",
    "print(\"Jumlah kata unik dalam data:\", a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VlmTHGTcK-tO"
   },
   "source": [
    "3.   Tokenization\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YXjbeOhCLE-P",
    "outputId": "44389531-d91d-42b5-dc06-567c8cf0283a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[9, 30, 23, 5, 51, 24, 10, 3, 2],\n",
       " [64, 9, 30, 5, 15, 11, 6, 76, 3],\n",
       " [9, 30, 77, 5, 106, 78, 168, 10, 16],\n",
       " [23, 9, 5, 31, 24, 10, 3, 2],\n",
       " [9, 64, 5, 15, 24, 12, 3, 2],\n",
       " [9, 30, 23, 65, 5, 15, 24, 10, 16, 6, 2],\n",
       " [9, 30, 64, 5, 51, 24, 6, 76, 10, 169, 2],\n",
       " [9, 30, 77, 5, 31, 12, 3, 2, 32, 43],\n",
       " [9, 64, 79, 5, 15, 24, 10, 3, 2],\n",
       " [77, 9, 30, 5, 51, 11, 66, 76, 3, 2],\n",
       " [4, 7, 40, 80, 44, 17, 67, 6, 2],\n",
       " [9, 5, 15, 45, 81, 17, 170, 10, 3],\n",
       " [8, 17, 2, 13, 171, 32, 46],\n",
       " [4, 52, 17, 82, 10, 3, 2],\n",
       " [9, 172, 17, 82, 5, 15, 173, 10, 16],\n",
       " [8, 33, 6, 2, 41, 68, 42, 83, 174, 175],\n",
       " [4, 7, 176, 53, 84, 10, 3, 2],\n",
       " [9, 5, 31, 177, 178, 17, 69, 3, 2],\n",
       " [4, 7, 40, 85, 12, 17, 68, 6, 2],\n",
       " [14, 33, 179, 6, 2, 86, 107, 47],\n",
       " [9, 33, 6, 2, 10, 18],\n",
       " [4, 87, 180, 88, 181, 6, 2],\n",
       " [4, 108, 87, 6, 89, 2],\n",
       " [14, 46, 87, 6, 2, 10, 18],\n",
       " [40, 54, 43, 3],\n",
       " [9, 23, 51, 12, 44, 17, 82, 6, 2],\n",
       " [4, 7, 109, 42, 33, 67, 6, 2],\n",
       " [9, 5, 15, 40, 12, 44, 33, 68, 6, 2],\n",
       " [9, 110, 79, 3, 2, 10, 53, 84],\n",
       " [9, 182, 17, 67, 86, 23, 3],\n",
       " [4, 7, 111, 112, 42, 21, 10, 3, 2],\n",
       " [9, 5, 15, 45, 81, 53, 84, 183, 184, 6, 2],\n",
       " [9, 185, 70, 12, 44, 186, 187, 6, 2],\n",
       " [4, 7, 35, 48, 188, 10, 53, 6, 2],\n",
       " [4, 7, 113, 85, 12, 16],\n",
       " [8, 3, 2, 6, 114, 53, 48],\n",
       " [8, 11, 115, 12, 116, 17, 6, 2],\n",
       " [9, 115, 117, 17, 5, 189, 12, 3, 2],\n",
       " [4, 7, 190, 117, 17, 2, 32, 118],\n",
       " [9, 119, 191, 192, 120, 54, 33, 6, 2],\n",
       " [4, 7, 121, 33, 120, 122, 23, 6, 2],\n",
       " [4, 7, 116, 33, 10, 3, 2, 122, 23, 193],\n",
       " [14, 34, 12, 3],\n",
       " [9, 30, 194, 5, 31, 40, 12, 49],\n",
       " [14, 195, 34, 196],\n",
       " [14, 34, 197, 23, 16],\n",
       " [11, 34, 12, 198, 123, 199, 3],\n",
       " [14, 34, 200, 6, 50, 201],\n",
       " [31, 40, 34, 36, 54, 124, 55, 3],\n",
       " [11, 34, 202, 12, 203, 56, 204],\n",
       " [14, 205, 34, 3],\n",
       " [4, 7, 206, 34, 55, 3],\n",
       " [207],\n",
       " [208, 9, 209, 125],\n",
       " [210, 11, 5, 13, 20, 37],\n",
       " [9, 211, 47, 18],\n",
       " [90, 212],\n",
       " [213],\n",
       " [71, 57, 90, 214],\n",
       " [215, 216],\n",
       " [126, 90, 217, 127],\n",
       " [71, 57, 128],\n",
       " [218, 219, 220],\n",
       " [221, 222, 129],\n",
       " [20, 130, 223, 37],\n",
       " [71, 57, 224, 38, 131],\n",
       " [132, 225],\n",
       " [129, 20, 226],\n",
       " [118, 227],\n",
       " [126, 133, 5, 20, 228],\n",
       " [132, 91, 133],\n",
       " [13, 37, 20],\n",
       " [20, 134, 37, 229],\n",
       " [9, 131, 13, 37],\n",
       " [230, 231, 20, 232, 233, 18],\n",
       " [13, 57, 135, 41],\n",
       " [20, 234, 7, 18, 72, 13, 136, 127],\n",
       " [137, 138, 72, 20, 235, 236, 139],\n",
       " [7, 140, 138, 72, 31, 41, 137],\n",
       " [20, 237, 18, 141, 70, 71, 57],\n",
       " [18, 130, 37, 72, 11, 139, 238],\n",
       " [4, 142, 143, 21],\n",
       " [4, 7, 92, 21, 10, 16],\n",
       " [14, 46, 21, 13, 73, 6, 2],\n",
       " [4, 7, 58, 21],\n",
       " [8, 85, 5, 144, 145, 93, 21],\n",
       " [14, 33, 5, 13, 93, 21],\n",
       " [4, 7, 94, 21, 86, 3, 239],\n",
       " [8, 146, 66, 17, 68, 13, 93, 21],\n",
       " [8, 36, 54, 124, 13, 37, 92, 21],\n",
       " [4, 7, 92, 21, 10, 146, 6, 240],\n",
       " [4, 142, 143, 39, 10, 16],\n",
       " [14, 46, 39, 13, 73, 10, 16],\n",
       " [4, 7, 35, 39, 10, 16],\n",
       " [8, 147, 241, 141, 12, 35, 39],\n",
       " [14, 128, 147, 5, 15, 24, 10, 16],\n",
       " [4, 7, 58, 39, 10, 3],\n",
       " [8, 39, 13, 48, 10, 16],\n",
       " [4, 7, 121, 136, 39, 112],\n",
       " [8, 17, 242, 41, 46, 39],\n",
       " [4, 7, 148, 243, 244, 10, 16],\n",
       " [9, 25, 48, 5, 15, 35, 10, 16],\n",
       " [4, 7, 94, 25, 245],\n",
       " [8, 246, 25, 19, 13, 36],\n",
       " [4, 7, 247, 25, 5, 13, 149, 150, 58],\n",
       " [8, 11, 25, 19, 5, 13, 149, 150, 151, 36],\n",
       " [8, 13, 11, 110, 248, 44, 25, 19],\n",
       " [4, 7, 35, 25, 249, 56, 250],\n",
       " [8, 25, 19, 6, 2, 13, 73, 151, 251, 58, 252],\n",
       " [4, 7, 253, 25, 19, 32, 43],\n",
       " [4, 7, 52, 25, 254, 5, 43, 12, 73, 58],\n",
       " [4, 7, 44, 26, 19, 10, 16],\n",
       " [8, 11, 26, 48, 6, 2],\n",
       " [4, 7, 35, 255, 26, 19],\n",
       " [8, 24, 23, 256, 26, 19, 257],\n",
       " [91, 9, 258, 26, 19, 6, 107, 47],\n",
       " [8, 259, 31, 24, 23, 109, 80, 12, 26, 19],\n",
       " [4, 7, 111, 36, 42, 26, 19],\n",
       " [8, 26, 19, 48, 13, 95, 260],\n",
       " [4, 7, 94, 261, 26, 19],\n",
       " [4, 7, 35, 26, 13, 6, 2],\n",
       " [9, 96, 262, 5, 15, 20, 52, 69, 3, 2, 59],\n",
       " [8, 11, 60, 263, 97, 6, 2, 74, 5, 31, 264],\n",
       " [9, 265, 266, 152, 2, 61],\n",
       " [8, 11, 267, 5, 45, 69, 3, 2, 98],\n",
       " [9, 5, 15, 20, 35, 153, 106, 268, 60, 6, 2, 75],\n",
       " [4, 154, 27, 28, 6, 50, 2, 62],\n",
       " [8, 11, 99, 155, 100, 2, 101],\n",
       " [8, 78, 24, 36, 269, 6, 50, 102, 2, 59],\n",
       " [9, 36, 270, 28, 5, 51, 271, 6, 83, 2, 74],\n",
       " [8, 27, 50, 2, 61, 272, 12, 63],\n",
       " [8, 11, 103, 28, 5, 15, 156, 6, 2, 75],\n",
       " [4, 3, 28, 6, 2, 62, 148, 96, 157],\n",
       " [8, 11, 273, 5, 104, 11, 6, 95, 2, 61],\n",
       " [9, 158, 65, 274, 2, 59, 159, 27, 160],\n",
       " [8, 63, 6, 2, 101, 31, 161, 105, 97, 5, 45, 27],\n",
       " [4, 7, 156, 60, 27, 6, 2, 74, 10, 3],\n",
       " [8, 11, 275, 5, 15, 276, 10, 3, 2, 75],\n",
       " [4, 103, 28, 88, 3, 6, 2, 62],\n",
       " [9, 119, 2, 98, 32, 277, 28, 6, 83, 50],\n",
       " [9, 30, 144, 140, 5, 45, 63, 152, 60, 6, 2, 61],\n",
       " [8, 63, 78, 278, 279, 42, 102, 2, 59],\n",
       " [8, 11, 99, 280, 100, 281, 2, 75],\n",
       " [4, 155, 2, 62, 99, 162, 27, 28],\n",
       " [9, 96, 5, 45, 27, 69, 3, 6, 2, 74],\n",
       " [8, 11, 282, 60, 5, 104, 283, 6, 50, 2, 61],\n",
       " [8, 36, 284, 163, 164, 6, 102, 2, 59],\n",
       " [4, 27, 2, 98, 285, 154, 28, 157],\n",
       " [8, 63, 13, 286, 32, 27, 28, 6, 2, 62],\n",
       " [9, 158, 65, 287, 101, 66, 103, 27, 160],\n",
       " [4, 7, 113, 22, 3, 5, 43],\n",
       " [14, 288, 22, 3, 18],\n",
       " [8, 11, 22, 3, 5, 41, 163, 6, 2, 18],\n",
       " [4, 7, 52, 108, 22, 3],\n",
       " [91, 9, 22, 3, 5, 289, 12, 165],\n",
       " [4, 7, 52, 22, 3, 5, 95],\n",
       " [8, 11, 22, 290, 81, 22, 79, 291],\n",
       " [14, 292, 22, 3, 18],\n",
       " [4, 7, 40, 80, 12, 22, 3, 5, 67],\n",
       " [8, 11, 22, 3, 5, 41, 293, 12, 16, 47],\n",
       " [14, 55, 38, 5, 134, 12, 3, 2, 18],\n",
       " [8, 41, 70, 3, 6, 125, 56, 294, 47],\n",
       " [4, 7, 295, 38, 49, 10, 3],\n",
       " [8, 114, 296, 88, 38, 49],\n",
       " [14, 55, 38, 49, 42, 297, 123, 89],\n",
       " [8, 11, 38, 70, 12, 3, 2, 18],\n",
       " [14, 46, 3, 298, 299, 89],\n",
       " [4, 7, 300, 38, 49, 55, 3],\n",
       " [8, 3, 13, 301, 66, 302, 47],\n",
       " [4, 7, 105, 38, 49, 153, 43, 54, 303],\n",
       " [4, 7, 166, 32, 29, 3],\n",
       " [8, 11, 29, 3, 5, 104, 3, 2, 18],\n",
       " [4, 7, 145, 135, 100, 3, 42, 29],\n",
       " [14, 65, 166, 32, 29, 3],\n",
       " [8, 29, 3, 304, 167, 56, 167],\n",
       " [4, 29, 3, 37, 3, 165],\n",
       " [8, 11, 305, 56, 164, 5, 11, 162, 29, 3],\n",
       " [4, 7, 159, 306, 32, 29, 3],\n",
       " [8, 29, 3, 307, 105, 97, 5, 15, 161],\n",
       " [4, 7, 308, 29, 3, 28]]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenize the data (Tokenisasi Data)\n",
    "vocabulary = 500\n",
    "tokenizer = Tokenizer(num_words=vocabulary, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(data['patterns'])\n",
    "train = tokenizer.texts_to_sequences(data['patterns'])\n",
    "train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0Ph33qa0LHz4"
   },
   "source": [
    "4.    Apply Padding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "S7coF8xZLL1B",
    "outputId": "edcdb753-bba7-43de-a2c0-2f2a376d7eb1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max length:\n",
      "\n",
      " 100\n",
      "[[  9  30  23 ...   0   0   0]\n",
      " [ 64   9  30 ...   0   0   0]\n",
      " [  9  30  77 ...   0   0   0]\n",
      " ...\n",
      " [  4   7 159 ...   0   0   0]\n",
      " [  8  29   3 ...   0   0   0]\n",
      " [  4   7 308 ...   0   0   0]]\n"
     ]
    }
   ],
   "source": [
    "max_len = 100\n",
    "print(\"Max length:\\n\\n\", max_len)\n",
    "# Melakukan proses padding pada data\n",
    "X_trainpad = pad_sequences(train, maxlen=max_len, padding='post', truncating='post')\n",
    "# Menampilkan hasil padding\n",
    "print(X_trainpad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IIGoTXbALNeK"
   },
   "source": [
    "5.   Encoding the Outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mUi_Z8JILSRs",
    "outputId": "903a817c-d033-4062-d8e4-402199701003"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded labels: [10 10 10 10 10 10 10 10 10 10  2  2  2  2  2  2  2  2  2  2  2  2  2  2\n",
      "  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2 12 12 12 12 12 12\n",
      " 12 12 12 12 13 13 13 13  5  5  5  5  5 14 14 14 14 14  1  1  1  1  1  6\n",
      "  6  6  6  6  4  4  4  4  4  8  8  8  8  8  8  8  8  8  8  3  3  3  3  3\n",
      "  3  3  3  3  3 15 15 15 15 15 15 15 15 15 15  7  7  7  7  7  7  7  7  7\n",
      "  7  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  9  9  9  9  9  9  9  9  9  9  9  9  9  9  9  9  9  9\n",
      "  9  9 11 11 11 11 11 11 11 11 11 11]\n",
      "Original tags: ['Budaya' 'Confirmation' 'Cuaca di Gunung' 'Dehidrasi' 'Feedback'\n",
      " 'Goodbye' 'Help' 'Hewan Liar' 'Hipotermia' 'Jalur dan Waktu Pendakian'\n",
      " 'Kebutuhan' 'Komunitas Pendaki' 'Persiapan cost' 'Salam' 'Thank You'\n",
      " 'Tumbuhan liar']\n"
     ]
    }
   ],
   "source": [
    "le = LabelEncoder()\n",
    "\n",
    "# Mengonversi label kategori menjadi angka\n",
    "Tags = le.fit_transform(data['tags'])\n",
    "\n",
    "# Menampilkan hasil encoding\n",
    "print(\"Encoded labels:\", Tags)\n",
    "\n",
    "# Menampilkan label asli yang terhubung dengan angka\n",
    "print(\"Original tags:\", le.classes_)  # Menampilkan kategori yang di-encode menjadi angka\n",
    "output_length = len(le.classes_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pattern: apa saja alat yang wajib bawa saat daki gunung | Encoded Tag: 10 | Original Tag: Kebutuhan\n",
      "Pattern: barang apa saja yang harus ada di tas daki | Encoded Tag: 10 | Original Tag: Kebutuhan\n",
      "Pattern: apa saja lengkap yang tidak boleh lupa saat hiking | Encoded Tag: 10 | Original Tag: Kebutuhan\n",
      "Pattern: alat apa yang perlu bawa saat daki gunung | Encoded Tag: 10 | Original Tag: Kebutuhan\n",
      "Pattern: apa barang yang harus bawa untuk daki gunung | Encoded Tag: 10 | Original Tag: Kebutuhan\n",
      "Pattern: apa saja alat penting yang harus bawa saat hiking di gunung | Encoded Tag: 10 | Original Tag: Kebutuhan\n",
      "Pattern: apa saja barang yang wajib bawa di tas saat naik gunung | Encoded Tag: 10 | Original Tag: Kebutuhan\n",
      "Pattern: apa saja lengkap yang perlu untuk daki gunung dengan aman | Encoded Tag: 10 | Original Tag: Kebutuhan\n",
      "Pattern: apa barang utama yang harus bawa saat daki gunung | Encoded Tag: 10 | Original Tag: Kebutuhan\n",
      "Pattern: lengkap apa saja yang wajib ada dalam tas daki gunung | Encoded Tag: 10 | Original Tag: Kebutuhan\n",
      "Pattern: bagaimana cara siap diri hadap cuaca ekstrem di gunung | Encoded Tag: 2 | Original Tag: Cuaca di Gunung\n",
      "Pattern: apa yang harus laku jika cuaca mendung saat daki | Encoded Tag: 2 | Original Tag: Cuaca di Gunung\n",
      "Pattern: apakah cuaca gunung bisa ubah dengan cepat | Encoded Tag: 2 | Original Tag: Cuaca di Gunung\n",
      "Pattern: bagaimana tahu cuaca buruk saat daki gunung | Encoded Tag: 2 | Original Tag: Cuaca di Gunung\n",
      "Pattern: apa tandatanda cuaca buruk yang harus waspada saat hiking | Encoded Tag: 2 | Original Tag: Cuaca di Gunung\n",
      "Pattern: apakah suhu di gunung lebih dingin dari daerah datar rendah | Encoded Tag: 2 | Original Tag: Cuaca di Gunung\n",
      "Pattern: bagaimana cara antisipasi hujan deras saat daki gunung | Encoded Tag: 2 | Original Tag: Cuaca di Gunung\n",
      "Pattern: apa yang perlu timbang soal cuaca belum daki gunung | Encoded Tag: 2 | Original Tag: Cuaca di Gunung\n",
      "Pattern: bagaimana cara siap pakai untuk cuaca dingin di gunung | Encoded Tag: 2 | Original Tag: Cuaca di Gunung\n",
      "Pattern: berapa suhu ratarata di gunung pada malam hari | Encoded Tag: 2 | Original Tag: Cuaca di Gunung\n",
      "Pattern: apa suhu di gunung saat ini | Encoded Tag: 2 | Original Tag: Cuaca di Gunung\n",
      "Pattern: bagaimana angin kencang pengaruh aktivitas di gunung | Encoded Tag: 2 | Original Tag: Cuaca di Gunung\n",
      "Pattern: bagaimana kondisi angin di puncak gunung | Encoded Tag: 2 | Original Tag: Cuaca di Gunung\n",
      "Pattern: berapa cepat angin di gunung saat ini | Encoded Tag: 2 | Original Tag: Cuaca di Gunung\n",
      "Pattern: siap dan aman daki | Encoded Tag: 2 | Original Tag: Cuaca di Gunung\n",
      "Pattern: apa alat wajib untuk hadap cuaca buruk di gunung | Encoded Tag: 2 | Original Tag: Cuaca di Gunung\n",
      "Pattern: bagaimana cara tahan dari suhu ekstrem di gunung | Encoded Tag: 2 | Original Tag: Cuaca di Gunung\n",
      "Pattern: apa yang harus siap untuk hadap suhu dingin di gunung | Encoded Tag: 2 | Original Tag: Cuaca di Gunung\n",
      "Pattern: apa risiko utama daki gunung saat hujan deras | Encoded Tag: 2 | Original Tag: Cuaca di Gunung\n",
      "Pattern: apa dampak cuaca ekstrem pada alat daki | Encoded Tag: 2 | Original Tag: Cuaca di Gunung\n",
      "Pattern: bagaimana cara lindung tubuh dari hipotermia saat daki gunung | Encoded Tag: 2 | Original Tag: Cuaca di Gunung\n",
      "Pattern: apa yang harus laku jika hujan deras tibatiba datang di gunung | Encoded Tag: 2 | Original Tag: Cuaca di Gunung\n",
      "Pattern: apa langkah baik untuk hadap kabut tebal di gunung | Encoded Tag: 2 | Original Tag: Cuaca di Gunung\n",
      "Pattern: bagaimana cara hindar bahaya longsor saat hujan di gunung | Encoded Tag: 2 | Original Tag: Cuaca di Gunung\n",
      "Pattern: bagaimana cara pilih pakai untuk hiking | Encoded Tag: 2 | Original Tag: Cuaca di Gunung\n",
      "Pattern: apakah daki gunung di musim hujan bahaya | Encoded Tag: 2 | Original Tag: Cuaca di Gunung\n",
      "Pattern: apakah ada aplikasi untuk pantau cuaca di gunung | Encoded Tag: 2 | Original Tag: Cuaca di Gunung\n",
      "Pattern: apa aplikasi prakira cuaca yang akurat untuk daki gunung | Encoded Tag: 2 | Original Tag: Cuaca di Gunung\n",
      "Pattern: bagaimana cara baca prakira cuaca gunung dengan benar | Encoded Tag: 2 | Original Tag: Cuaca di Gunung\n",
      "Pattern: apa hubung antara tekan udara dan suhu di gunung | Encoded Tag: 2 | Original Tag: Cuaca di Gunung\n",
      "Pattern: bagaimana cara ukur suhu udara tanpa alat di gunung | Encoded Tag: 2 | Original Tag: Cuaca di Gunung\n",
      "Pattern: bagaimana cara pantau suhu saat daki gunung tanpa alat canggih | Encoded Tag: 2 | Original Tag: Cuaca di Gunung\n",
      "Pattern: berapa biaya untuk daki | Encoded Tag: 12 | Original Tag: Persiapan cost\n",
      "Pattern: apa saja keluar yang perlu siap untuk jalan | Encoded Tag: 12 | Original Tag: Persiapan cost\n",
      "Pattern: berapa estimasi biaya transportasi | Encoded Tag: 12 | Original Tag: Persiapan cost\n",
      "Pattern: berapa biaya sewa alat hiking | Encoded Tag: 12 | Original Tag: Persiapan cost\n",
      "Pattern: ada biaya untuk masuk ke area daki | Encoded Tag: 12 | Original Tag: Persiapan cost\n",
      "Pattern: berapa biaya akomodasi di sekitar lokasi | Encoded Tag: 12 | Original Tag: Persiapan cost\n",
      "Pattern: perlu siap biaya makan dan minum lama daki | Encoded Tag: 12 | Original Tag: Persiapan cost\n",
      "Pattern: ada biaya tambah untuk guide atau porter | Encoded Tag: 12 | Original Tag: Persiapan cost\n",
      "Pattern: berapa total biaya daki | Encoded Tag: 12 | Original Tag: Persiapan cost\n",
      "Pattern: bagaimana cara hemat biaya lama daki | Encoded Tag: 12 | Original Tag: Persiapan cost\n",
      "Pattern: hi | Encoded Tag: 13 | Original Tag: Salam\n",
      "Pattern: halo apa kabarselamat pagi | Encoded Tag: 13 | Original Tag: Salam\n",
      "Pattern: hai ada yang bisa saya bantu | Encoded Tag: 13 | Original Tag: Salam\n",
      "Pattern: apa kabar hari ini | Encoded Tag: 13 | Original Tag: Salam\n",
      "Pattern: sampai jumpa | Encoded Tag: 5 | Original Tag: Goodbye\n",
      "Pattern: bye | Encoded Tag: 5 | Original Tag: Goodbye\n",
      "Pattern: terima kasih sampai nanti | Encoded Tag: 5 | Original Tag: Goodbye\n",
      "Pattern: selamat tinggal | Encoded Tag: 5 | Original Tag: Goodbye\n",
      "Pattern: oke sampai ketemu lagi | Encoded Tag: 5 | Original Tag: Goodbye\n",
      "Pattern: terima kasih banyak | Encoded Tag: 14 | Original Tag: Thank You\n",
      "Pattern: thanks a lot | Encoded Tag: 14 | Original Tag: Thank You\n",
      "Pattern: makasih banget ya | Encoded Tag: 14 | Original Tag: Thank You\n",
      "Pattern: saya sangat harga bantu | Encoded Tag: 14 | Original Tag: Thank You\n",
      "Pattern: terima kasih atas waktu kamu | Encoded Tag: 14 | Original Tag: Thank You\n",
      "Pattern: iya betul | Encoded Tag: 1 | Original Tag: Confirmation\n",
      "Pattern: ya saya tuju | Encoded Tag: 1 | Original Tag: Confirmation\n",
      "Pattern: benar sekali | Encoded Tag: 1 | Original Tag: Confirmation\n",
      "Pattern: oke itu yang saya maksud | Encoded Tag: 1 | Original Tag: Confirmation\n",
      "Pattern: iya seperti itu | Encoded Tag: 1 | Original Tag: Confirmation\n",
      "Pattern: bisa bantu saya | Encoded Tag: 6 | Original Tag: Help\n",
      "Pattern: saya butuh bantu nih | Encoded Tag: 6 | Original Tag: Help\n",
      "Pattern: apa kamu bisa bantu | Encoded Tag: 6 | Original Tag: Help\n",
      "Pattern: tolong dong saya nggak ngerti ini | Encoded Tag: 6 | Original Tag: Help\n",
      "Pattern: bisa kasih informasi lebih | Encoded Tag: 6 | Original Tag: Help\n",
      "Pattern: saya suka cara ini tapi bisa tingkat lagi | Encoded Tag: 4 | Original Tag: Feedback\n",
      "Pattern: jelas bagus tapi saya masih bingung sedikit | Encoded Tag: 4 | Original Tag: Feedback\n",
      "Pattern: cara umum bagus tapi perlu lebih jelas | Encoded Tag: 4 | Original Tag: Feedback\n",
      "Pattern: saya rasa ini cukup baik terima kasih | Encoded Tag: 4 | Original Tag: Feedback\n",
      "Pattern: ini sangat bantu tapi ada sedikit kurang | Encoded Tag: 4 | Original Tag: Feedback\n",
      "Pattern: bagaimana gejala awal hipotermia | Encoded Tag: 8 | Original Tag: Hipotermia\n",
      "Pattern: bagaimana cara cegah hipotermia saat hiking | Encoded Tag: 8 | Original Tag: Hipotermia\n",
      "Pattern: berapa cepat hipotermia bisa jadi di gunung | Encoded Tag: 8 | Original Tag: Hipotermia\n",
      "Pattern: bagaimana cara obat hipotermia | Encoded Tag: 8 | Original Tag: Hipotermia\n",
      "Pattern: apakah pakai yang salah dapat sebab hipotermia | Encoded Tag: 8 | Original Tag: Hipotermia\n",
      "Pattern: berapa suhu yang bisa sebab hipotermia | Encoded Tag: 8 | Original Tag: Hipotermia\n",
      "Pattern: bagaimana cara nali hipotermia pada daki lain | Encoded Tag: 8 | Original Tag: Hipotermia\n",
      "Pattern: apakah tidur dalam cuaca dingin bisa sebab hipotermia | Encoded Tag: 8 | Original Tag: Hipotermia\n",
      "Pattern: apakah makan dan minum bisa bantu cegah hipotermia | Encoded Tag: 8 | Original Tag: Hipotermia\n",
      "Pattern: bagaimana cara cegah hipotermia saat tidur di luar | Encoded Tag: 8 | Original Tag: Hipotermia\n",
      "Pattern: bagaimana gejala awal dehidrasi saat hiking | Encoded Tag: 3 | Original Tag: Dehidrasi\n",
      "Pattern: berapa cepat dehidrasi bisa jadi saat hiking | Encoded Tag: 3 | Original Tag: Dehidrasi\n",
      "Pattern: bagaimana cara hindar dehidrasi saat hiking | Encoded Tag: 3 | Original Tag: Dehidrasi\n",
      "Pattern: apakah air mineral cukup untuk hindar dehidrasi | Encoded Tag: 3 | Original Tag: Dehidrasi\n",
      "Pattern: berapa banyak air yang harus bawa saat hiking | Encoded Tag: 3 | Original Tag: Dehidrasi\n",
      "Pattern: bagaimana cara obat dehidrasi saat daki | Encoded Tag: 3 | Original Tag: Dehidrasi\n",
      "Pattern: apakah dehidrasi bisa bahaya saat hiking | Encoded Tag: 3 | Original Tag: Dehidrasi\n",
      "Pattern: bagaimana cara ukur tingkat dehidrasi tubuh | Encoded Tag: 3 | Original Tag: Dehidrasi\n",
      "Pattern: apakah cuaca panas lebih cepat dehidrasi | Encoded Tag: 3 | Original Tag: Dehidrasi\n",
      "Pattern: bagaimana cara jaga imbang elektrolit saat hiking | Encoded Tag: 3 | Original Tag: Dehidrasi\n",
      "Pattern: apa tumbuh bahaya yang harus hindar saat hiking | Encoded Tag: 15 | Original Tag: Tumbuhan liar\n",
      "Pattern: bagaimana cara nali tumbuh racun | Encoded Tag: 15 | Original Tag: Tumbuhan liar\n",
      "Pattern: apakah semua tumbuh liar bisa makan | Encoded Tag: 15 | Original Tag: Tumbuhan liar\n",
      "Pattern: bagaimana cara identifikasi tumbuh yang bisa guna bagai obat | Encoded Tag: 15 | Original Tag: Tumbuhan liar\n",
      "Pattern: apakah ada tumbuh liar yang bisa guna bagai bahan makan | Encoded Tag: 15 | Original Tag: Tumbuhan liar\n",
      "Pattern: apakah bisa ada risiko alergi hadap tumbuh liar | Encoded Tag: 15 | Original Tag: Tumbuhan liar\n",
      "Pattern: bagaimana cara hindar tumbuh dur atau tajam | Encoded Tag: 15 | Original Tag: Tumbuhan liar\n",
      "Pattern: apakah tumbuh liar di gunung bisa jadi bahan buat obat tradisional | Encoded Tag: 15 | Original Tag: Tumbuhan liar\n",
      "Pattern: bagaimana cara petik tumbuh liar dengan aman | Encoded Tag: 15 | Original Tag: Tumbuhan liar\n",
      "Pattern: bagaimana cara tahu tumbuh mana yang aman untuk jadi obat | Encoded Tag: 15 | Original Tag: Tumbuhan liar\n",
      "Pattern: bagaimana cara hadap hewan liar saat hiking | Encoded Tag: 7 | Original Tag: Hewan Liar\n",
      "Pattern: apakah ada hewan bahaya di gunung | Encoded Tag: 7 | Original Tag: Hewan Liar\n",
      "Pattern: bagaimana cara hindar serang hewan liar | Encoded Tag: 7 | Original Tag: Hewan Liar\n",
      "Pattern: apakah bawa alat usir hewan liar efektif | Encoded Tag: 7 | Original Tag: Hewan Liar\n",
      "Pattern: seperti apa perilaku hewan liar di malam hari | Encoded Tag: 7 | Original Tag: Hewan Liar\n",
      "Pattern: apakah kita perlu bawa alat tahan diri untuk hewan liar | Encoded Tag: 7 | Original Tag: Hewan Liar\n",
      "Pattern: bagaimana cara lindung makan dari hewan liar | Encoded Tag: 7 | Original Tag: Hewan Liar\n",
      "Pattern: apakah hewan liar bahaya bisa dekat tenda | Encoded Tag: 7 | Original Tag: Hewan Liar\n",
      "Pattern: bagaimana cara nali jejak hewan liar | Encoded Tag: 7 | Original Tag: Hewan Liar\n",
      "Pattern: bagaimana cara hindar hewan bisa di gunung | Encoded Tag: 7 | Original Tag: Hewan Liar\n",
      "Pattern: apa tradisi unik yang harus saya tahu belum daki gunung rinjani | Encoded Tag: 0 | Original Tag: Budaya\n",
      "Pattern: apakah ada adat istiadat khusus di gunung merbabu yang perlu perhati | Encoded Tag: 0 | Original Tag: Budaya\n",
      "Pattern: apa mitos kenal kait gunung bromo | Encoded Tag: 0 | Original Tag: Budaya\n",
      "Pattern: apakah ada ritual yang laku belum daki gunung prau | Encoded Tag: 0 | Original Tag: Budaya\n",
      "Pattern: apa yang harus saya hindar agar tidak langgar adat di gunung ciremai | Encoded Tag: 0 | Original Tag: Budaya\n",
      "Pattern: bagaimana budaya masyarakat lokal di sekitar gunung kerinci | Encoded Tag: 0 | Original Tag: Budaya\n",
      "Pattern: apakah ada cerita legenda tentang gunung ijen | Encoded Tag: 0 | Original Tag: Budaya\n",
      "Pattern: apakah boleh bawa makan nonhalal di sekitar kawasan gunung rinjani | Encoded Tag: 0 | Original Tag: Budaya\n",
      "Pattern: apa makan khas lokal yang wajib coba di daerah gunung merbabu | Encoded Tag: 0 | Original Tag: Budaya\n",
      "Pattern: apakah masyarakat sekitar gunung bromo buka untuk wisatawan | Encoded Tag: 0 | Original Tag: Budaya\n",
      "Pattern: apakah ada percaya lokal yang harus hormat di gunung ciremai | Encoded Tag: 0 | Original Tag: Budaya\n",
      "Pattern: bagaimana daki lokal di gunung kerinci jaga tradisi mereka | Encoded Tag: 0 | Original Tag: Budaya\n",
      "Pattern: apakah ada festival yang sering ada di dekat gunung bromo | Encoded Tag: 0 | Original Tag: Budaya\n",
      "Pattern: apa arti penting spiritual gunung rinjani bagi masyarakat tempat | Encoded Tag: 0 | Original Tag: Budaya\n",
      "Pattern: apakah wisatawan di gunung ijen perlu ikut atur khusus yang laku masyarakat | Encoded Tag: 0 | Original Tag: Budaya\n",
      "Pattern: bagaimana cara hormat adat masyarakat di gunung merbabu saat daki | Encoded Tag: 0 | Original Tag: Budaya\n",
      "Pattern: apakah ada pantang yang harus patuh saat daki gunung ciremai | Encoded Tag: 0 | Original Tag: Budaya\n",
      "Pattern: bagaimana percaya lokal pengaruh daki di gunung kerinci | Encoded Tag: 0 | Original Tag: Budaya\n",
      "Pattern: apa hubung gunung prau dengan sejarah lokal di daerah sekitar | Encoded Tag: 0 | Original Tag: Budaya\n",
      "Pattern: apa saja salah umum yang laku wisatawan kait adat di gunung bromo | Encoded Tag: 0 | Original Tag: Budaya\n",
      "Pattern: apakah wisatawan boleh ambil sesuatu dari kawasan gunung rinjani | Encoded Tag: 0 | Original Tag: Budaya\n",
      "Pattern: apakah ada cerita rakyat tentang asalusul gunung ciremai | Encoded Tag: 0 | Original Tag: Budaya\n",
      "Pattern: bagaimana legenda gunung kerinci cerita oleh masyarakat lokal | Encoded Tag: 0 | Original Tag: Budaya\n",
      "Pattern: apa tradisi yang laku masyarakat belum daki di gunung merbabu | Encoded Tag: 0 | Original Tag: Budaya\n",
      "Pattern: apakah ada tari adat yang sering tunjuk di sekitar gunung bromo | Encoded Tag: 0 | Original Tag: Budaya\n",
      "Pattern: apakah makan halal mudah temu di kawasan gunung rinjani | Encoded Tag: 0 | Original Tag: Budaya\n",
      "Pattern: bagaimana masyarakat gunung prau lestari budaya lokal mereka | Encoded Tag: 0 | Original Tag: Budaya\n",
      "Pattern: apakah wisatawan bisa interaksi dengan masyarakat lokal di gunung kerinci | Encoded Tag: 0 | Original Tag: Budaya\n",
      "Pattern: apa arti penting kawah ijen dalam percaya masyarakat tempat | Encoded Tag: 0 | Original Tag: Budaya\n",
      "Pattern: bagaimana cara pilih jalur daki yang aman | Encoded Tag: 9 | Original Tag: Jalur dan Waktu Pendakian\n",
      "Pattern: berapa sulit jalur daki ini | Encoded Tag: 9 | Original Tag: Jalur dan Waktu Pendakian\n",
      "Pattern: apakah ada jalur daki yang lebih mudah di gunung ini | Encoded Tag: 9 | Original Tag: Jalur dan Waktu Pendakian\n",
      "Pattern: bagaimana cara tahu kondisi jalur daki | Encoded Tag: 9 | Original Tag: Jalur dan Waktu Pendakian\n",
      "Pattern: seperti apa jalur daki yang cocok untuk mula | Encoded Tag: 9 | Original Tag: Jalur dan Waktu Pendakian\n",
      "Pattern: bagaimana cara tahu jalur daki yang dekat | Encoded Tag: 9 | Original Tag: Jalur dan Waktu Pendakian\n",
      "Pattern: apakah ada jalur alternatif jika jalur utama tutup | Encoded Tag: 9 | Original Tag: Jalur dan Waktu Pendakian\n",
      "Pattern: berapa panjang jalur daki ini | Encoded Tag: 9 | Original Tag: Jalur dan Waktu Pendakian\n",
      "Pattern: bagaimana cara siap diri untuk jalur daki yang ekstrem | Encoded Tag: 9 | Original Tag: Jalur dan Waktu Pendakian\n",
      "Pattern: apakah ada jalur daki yang lebih pendek untuk hiking hari | Encoded Tag: 9 | Original Tag: Jalur dan Waktu Pendakian\n",
      "Pattern: berapa lama waktu yang butuh untuk daki gunung ini | Encoded Tag: 9 | Original Tag: Jalur dan Waktu Pendakian\n",
      "Pattern: apakah lebih baik daki di pagi atau sore hari | Encoded Tag: 9 | Original Tag: Jalur dan Waktu Pendakian\n",
      "Pattern: bagaimana cara rencana waktu jalan saat daki | Encoded Tag: 9 | Original Tag: Jalur dan Waktu Pendakian\n",
      "Pattern: apakah musim tentu pengaruh waktu jalan | Encoded Tag: 9 | Original Tag: Jalur dan Waktu Pendakian\n",
      "Pattern: berapa lama waktu jalan dari basecamp ke puncak | Encoded Tag: 9 | Original Tag: Jalur dan Waktu Pendakian\n",
      "Pattern: apakah ada waktu baik untuk daki gunung ini | Encoded Tag: 9 | Original Tag: Jalur dan Waktu Pendakian\n",
      "Pattern: berapa cepat daki biasa capai puncak | Encoded Tag: 9 | Original Tag: Jalur dan Waktu Pendakian\n",
      "Pattern: bagaimana cara kira waktu jalan lama daki | Encoded Tag: 9 | Original Tag: Jalur dan Waktu Pendakian\n",
      "Pattern: apakah daki bisa selesai dalam satu hari | Encoded Tag: 9 | Original Tag: Jalur dan Waktu Pendakian\n",
      "Pattern: bagaimana cara atur waktu jalan agar aman dan efisien | Encoded Tag: 9 | Original Tag: Jalur dan Waktu Pendakian\n",
      "Pattern: bagaimana cara gabung dengan komunitas daki | Encoded Tag: 11 | Original Tag: Komunitas Pendaki\n",
      "Pattern: apakah ada komunitas daki yang sering daki gunung ini | Encoded Tag: 11 | Original Tag: Komunitas Pendaki\n",
      "Pattern: bagaimana cara dapat informasi tentang daki dari komunitas | Encoded Tag: 11 | Original Tag: Komunitas Pendaki\n",
      "Pattern: berapa penting gabung dengan komunitas daki | Encoded Tag: 11 | Original Tag: Komunitas Pendaki\n",
      "Pattern: apakah komunitas daki sedia pandu atau pandu | Encoded Tag: 11 | Original Tag: Komunitas Pendaki\n",
      "Pattern: bagaimana komunitas daki bantu daki mula | Encoded Tag: 11 | Original Tag: Komunitas Pendaki\n",
      "Pattern: apakah ada acara atau temu yang ada oleh komunitas daki | Encoded Tag: 11 | Original Tag: Komunitas Pendaki\n",
      "Pattern: bagaimana cara bagi alam dengan komunitas daki | Encoded Tag: 11 | Original Tag: Komunitas Pendaki\n",
      "Pattern: apakah komunitas daki milik atur khusus yang harus ikut | Encoded Tag: 11 | Original Tag: Komunitas Pendaki\n",
      "Pattern: bagaimana cara bentuk komunitas daki lokal | Encoded Tag: 11 | Original Tag: Komunitas Pendaki\n"
     ]
    }
   ],
   "source": [
    "for pattern, label in zip(data['patterns'], Tags):\n",
    "    print(f'Pattern: {pattern} | Encoded Tag: {label} | Original Tag: {le.inverse_transform([label])[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 564
    },
    "id": "ngQMuSDw-PNK",
    "outputId": "6a02b909-e180-463f-e8e7-e20fec8666bd"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Hitung panjang pola dalam dataset\n",
    "pattern_lengths = [len(pattern.split()) for intent in dataset for pattern in intent[\"patterns\"]]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(pattern_lengths, bins=range(1, max(pattern_lengths) + 1), edgecolor='black')\n",
    "plt.title(\"Distribusi Panjang Pola\")\n",
    "plt.xlabel(\"Jumlah Kata per Pola\")\n",
    "plt.ylabel(\"Frekuensi\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_train: (144, 100)\n",
      "Shape of X_val: (36, 100)\n",
      "Shape of y_train: (144,)\n",
      "Shape of y_test: (36,)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X_trainpad, Tags, test_size=0.2, random_state=42)\n",
    "print(\"Shape of X_train:\", X_train.shape)\n",
    "print(\"Shape of X_val:\", X_val.shape)\n",
    "print(\"Shape of y_train:\", y_train.shape)\n",
    "print(\"Shape of y_test:\",y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fEflfNnXLqAJ"
   },
   "source": [
    "# Saving the Model\n",
    "After completing the text processing in five stages, we can save the text processing model using the pickle format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gDO9d0pMLx0R"
   },
   "source": [
    "# Modeling with LSTM Algorithm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"font-weight: bold\"> Layer (type)                    </span><span style=\"font-weight: bold\"> Output Shape           </span><span style=\"font-weight: bold\">       Param # </span>\n",
       "\n",
       " input_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)         (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)                         <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n",
       "\n",
       " embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)            (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">25,000</span> \n",
       "\n",
       " bidirectional (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)    (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)                 <span style=\"color: #00af00; text-decoration-color: #00af00\">8,576</span> \n",
       "\n",
       " flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)                (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3200</span>)                        <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n",
       "\n",
       " dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3200</span>)                        <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n",
       "\n",
       " dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                    (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)                     <span style=\"color: #00af00; text-decoration-color: #00af00\">51,216</span> \n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m\n",
       "\n",
       " input_layer (\u001b[38;5;33mInputLayer\u001b[0m)         (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)                         \u001b[38;5;34m0\u001b[0m \n",
       "\n",
       " embedding (\u001b[38;5;33mEmbedding\u001b[0m)            (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m50\u001b[0m)                \u001b[38;5;34m25,000\u001b[0m \n",
       "\n",
       " bidirectional (\u001b[38;5;33mBidirectional\u001b[0m)    (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m32\u001b[0m)                 \u001b[38;5;34m8,576\u001b[0m \n",
       "\n",
       " flatten (\u001b[38;5;33mFlatten\u001b[0m)                (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3200\u001b[0m)                        \u001b[38;5;34m0\u001b[0m \n",
       "\n",
       " dropout (\u001b[38;5;33mDropout\u001b[0m)                (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3200\u001b[0m)                        \u001b[38;5;34m0\u001b[0m \n",
       "\n",
       " dense (\u001b[38;5;33mDense\u001b[0m)                    (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)                     \u001b[38;5;34m51,216\u001b[0m \n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">84,792</span> (331.22 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m84,792\u001b[0m (331.22 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">84,792</span> (331.22 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m84,792\u001b[0m (331.22 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Creating the model (Membuat Modelling)\n",
    "i = Input(shape=(max_len,))\n",
    "x = Embedding(vocabulary, 50)(i)\n",
    "x = Bidirectional(LSTM(16, return_sequences=True, recurrent_dropout=0.5,recurrent_regularizer=l2(0.01)))(x)\n",
    "x = Flatten()(x)\n",
    "x = Dropout(0.8)(x)\n",
    "x = Dense(output_length, activation=\"softmax\", kernel_regularizer=l2(0.01))(x)\n",
    "\n",
    "model  = Model(i,x) \n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=Adam(learning_rate=0.007), metrics=['accuracy'])\n",
    "\n",
    "checkpoint = ModelCheckpoint('best_model_1.keras', monitor='val_accuracy', \n",
    "                            save_best_only=True, mode='max', verbose=1)\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "\u001b[1m4/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.1094 - loss: 3.4041\n",
      "Epoch 1: val_accuracy improved from -inf to 0.05556, saving model to best_model_1.keras\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 124ms/step - accuracy: 0.1146 - loss: 3.4210 - val_accuracy: 0.0556 - val_loss: 3.5936\n",
      "Epoch 2/200\n",
      "\u001b[1m4/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.1641 - loss: 3.4470\n",
      "Epoch 2: val_accuracy improved from 0.05556 to 0.19444, saving model to best_model_1.keras\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.1557 - loss: 3.4385 - val_accuracy: 0.1944 - val_loss: 3.4262\n",
      "Epoch 3/200\n",
      "\u001b[1m4/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.1094 - loss: 3.3920\n",
      "Epoch 3: val_accuracy did not improve from 0.19444\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.1123 - loss: 3.3694 - val_accuracy: 0.1944 - val_loss: 3.3188\n",
      "Epoch 4/200\n",
      "\u001b[1m4/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.1777 - loss: 3.1878\n",
      "Epoch 4: val_accuracy did not improve from 0.19444\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.1740 - loss: 3.1895 - val_accuracy: 0.1944 - val_loss: 3.2305\n",
      "Epoch 5/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.2340 - loss: 3.1012\n",
      "Epoch 5: val_accuracy improved from 0.19444 to 0.30556, saving model to best_model_1.keras\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - accuracy: 0.2425 - loss: 3.0939 - val_accuracy: 0.3056 - val_loss: 3.1303\n",
      "Epoch 6/200\n",
      "\u001b[1m3/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.3003 - loss: 2.9230\n",
      "Epoch 6: val_accuracy improved from 0.30556 to 0.36111, saving model to best_model_1.keras\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - accuracy: 0.3051 - loss: 2.9084 - val_accuracy: 0.3611 - val_loss: 2.9704\n",
      "Epoch 7/200\n",
      "\u001b[1m4/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.4056 - loss: 2.6854\n",
      "Epoch 7: val_accuracy improved from 0.36111 to 0.44444, saving model to best_model_1.keras\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - accuracy: 0.4023 - loss: 2.6704 - val_accuracy: 0.4444 - val_loss: 2.7337\n",
      "Epoch 8/200\n",
      "\u001b[1m4/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.5247 - loss: 2.3151\n",
      "Epoch 8: val_accuracy improved from 0.44444 to 0.47222, saving model to best_model_1.keras\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.5142 - loss: 2.3135 - val_accuracy: 0.4722 - val_loss: 2.4684\n",
      "Epoch 9/200\n",
      "\u001b[1m4/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.5501 - loss: 2.1276\n",
      "Epoch 9: val_accuracy improved from 0.47222 to 0.58333, saving model to best_model_1.keras\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.5635 - loss: 2.0844 - val_accuracy: 0.5833 - val_loss: 2.1469\n",
      "Epoch 10/200\n",
      "\u001b[1m4/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.7428 - loss: 1.8089\n",
      "Epoch 10: val_accuracy did not improve from 0.58333\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.7499 - loss: 1.7676 - val_accuracy: 0.5000 - val_loss: 2.0324\n",
      "Epoch 11/200\n",
      "\u001b[1m4/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.8203 - loss: 1.4456\n",
      "Epoch 11: val_accuracy did not improve from 0.58333\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.8108 - loss: 1.4466 - val_accuracy: 0.5833 - val_loss: 1.9172\n",
      "Epoch 12/200\n",
      "\u001b[1m4/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.8092 - loss: 1.3574\n",
      "Epoch 12: val_accuracy improved from 0.58333 to 0.61111, saving model to best_model_1.keras\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.8126 - loss: 1.3328 - val_accuracy: 0.6111 - val_loss: 1.7632\n",
      "Epoch 13/200\n",
      "\u001b[1m4/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.8555 - loss: 1.1233\n",
      "Epoch 13: val_accuracy improved from 0.61111 to 0.69444, saving model to best_model_1.keras\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 0.8643 - loss: 1.1140 - val_accuracy: 0.6944 - val_loss: 1.5989\n",
      "Epoch 14/200\n",
      "\u001b[1m4/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9108 - loss: 0.9678\n",
      "Epoch 14: val_accuracy did not improve from 0.69444\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.8989 - loss: 0.9794 - val_accuracy: 0.6944 - val_loss: 1.5677\n",
      "Epoch 15/200\n",
      "\u001b[1m4/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.9023 - loss: 0.9049\n",
      "Epoch 15: val_accuracy improved from 0.69444 to 0.75000, saving model to best_model_1.keras\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.9025 - loss: 0.9058 - val_accuracy: 0.7500 - val_loss: 1.4461\n",
      "Epoch 16/200\n",
      "\u001b[1m4/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.8984 - loss: 0.9136\n",
      "Epoch 16: val_accuracy did not improve from 0.75000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.8976 - loss: 0.8966 - val_accuracy: 0.6667 - val_loss: 1.3997\n",
      "Epoch 17/200\n",
      "\u001b[1m4/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.9004 - loss: 0.8184\n",
      "Epoch 17: val_accuracy did not improve from 0.75000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.9081 - loss: 0.8007 - val_accuracy: 0.6667 - val_loss: 1.3446\n",
      "Epoch 18/200\n",
      "\u001b[1m4/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.9701 - loss: 0.7003\n",
      "Epoch 18: val_accuracy improved from 0.75000 to 0.80556, saving model to best_model_1.keras\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.9638 - loss: 0.7052 - val_accuracy: 0.8056 - val_loss: 1.1605\n",
      "Epoch 19/200\n",
      "\u001b[1m4/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.9264 - loss: 0.6967\n",
      "Epoch 19: val_accuracy did not improve from 0.80556\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.9301 - loss: 0.6942 - val_accuracy: 0.8056 - val_loss: 1.1865\n",
      "Epoch 20/200\n",
      "\u001b[1m4/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.9648 - loss: 0.6672\n",
      "Epoch 20: val_accuracy did not improve from 0.80556\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9604 - loss: 0.6679 - val_accuracy: 0.6667 - val_loss: 1.3119\n",
      "Epoch 21/200\n",
      "\u001b[1m4/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.9935 - loss: 0.5999\n",
      "Epoch 21: val_accuracy did not improve from 0.80556\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9887 - loss: 0.6062 - val_accuracy: 0.7778 - val_loss: 1.2178\n",
      "Epoch 22/200\n",
      "\u001b[1m4/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.9733 - loss: 0.5755\n",
      "Epoch 22: val_accuracy did not improve from 0.80556\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.9729 - loss: 0.5848 - val_accuracy: 0.8056 - val_loss: 1.1259\n",
      "Epoch 23/200\n",
      "\u001b[1m4/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.9935 - loss: 0.5938\n",
      "Epoch 23: val_accuracy did not improve from 0.80556\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9910 - loss: 0.5868 - val_accuracy: 0.7778 - val_loss: 1.0966\n",
      "Epoch 24/200\n",
      "\u001b[1m4/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.9746 - loss: 0.5324\n",
      "Epoch 24: val_accuracy did not improve from 0.80556\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.9738 - loss: 0.5339 - val_accuracy: 0.7778 - val_loss: 1.0699\n",
      "Epoch 25/200\n",
      "\u001b[1m4/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.9954 - loss: 0.4586\n",
      "Epoch 25: val_accuracy did not improve from 0.80556\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9923 - loss: 0.4654 - val_accuracy: 0.7222 - val_loss: 1.0528\n",
      "Epoch 26/200\n",
      "\u001b[1m4/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.9746 - loss: 0.4524\n",
      "Epoch 26: val_accuracy did not improve from 0.80556\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.9761 - loss: 0.4561 - val_accuracy: 0.7778 - val_loss: 1.0558\n",
      "Epoch 27/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.9747 - loss: 0.4648\n",
      "Epoch 27: val_accuracy did not improve from 0.80556\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.9742 - loss: 0.4649 - val_accuracy: 0.6667 - val_loss: 1.1017\n",
      "Epoch 28/200\n",
      "\u001b[1m4/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.9661 - loss: 0.4343\n",
      "Epoch 28: val_accuracy did not improve from 0.80556\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.9682 - loss: 0.4390 - val_accuracy: 0.7778 - val_loss: 1.0484\n",
      "Epoch 29/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.9905 - loss: 0.3997\n",
      "Epoch 29: val_accuracy did not improve from 0.80556\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - accuracy: 0.9897 - loss: 0.4023 - val_accuracy: 0.7778 - val_loss: 1.0611\n",
      "Epoch 30/200\n",
      "\u001b[1m3/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 1.0000 - loss: 0.4149\n",
      "Epoch 30: val_accuracy did not improve from 0.80556\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 1.0000 - loss: 0.4178 - val_accuracy: 0.8056 - val_loss: 1.0468\n",
      "Epoch 31/200\n",
      "\u001b[1m4/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.9896 - loss: 0.4348\n",
      "Epoch 31: val_accuracy did not improve from 0.80556\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.9884 - loss: 0.4353 - val_accuracy: 0.7778 - val_loss: 1.0582\n",
      "Epoch 32/200\n",
      "\u001b[1m4/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 1.0000 - loss: 0.3854\n",
      "Epoch 32: val_accuracy did not improve from 0.80556\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 1.0000 - loss: 0.3855 - val_accuracy: 0.7778 - val_loss: 1.0258\n",
      "Epoch 33/200\n",
      "\u001b[1m4/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.9896 - loss: 0.4053\n",
      "Epoch 33: val_accuracy did not improve from 0.80556\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9884 - loss: 0.4033 - val_accuracy: 0.6111 - val_loss: 1.0986\n",
      "Epoch 34/200\n",
      "\u001b[1m4/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.9980 - loss: 0.3660\n",
      "Epoch 34: val_accuracy did not improve from 0.80556\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9964 - loss: 0.3666 - val_accuracy: 0.7500 - val_loss: 1.0853\n",
      "Epoch 35/200\n",
      "\u001b[1m4/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.9954 - loss: 0.3274\n",
      "Epoch 35: val_accuracy improved from 0.80556 to 0.88889, saving model to best_model_1.keras\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.9923 - loss: 0.3350 - val_accuracy: 0.8889 - val_loss: 0.9152\n",
      "Epoch 36/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9970 - loss: 0.3150\n",
      "Epoch 36: val_accuracy did not improve from 0.88889\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.9964 - loss: 0.3172 - val_accuracy: 0.8611 - val_loss: 0.8550\n",
      "Epoch 37/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.9918 - loss: 0.3311\n",
      "Epoch 37: val_accuracy did not improve from 0.88889\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.9920 - loss: 0.3311 - val_accuracy: 0.8611 - val_loss: 0.8131\n",
      "Epoch 38/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.9856 - loss: 0.3243\n",
      "Epoch 38: val_accuracy did not improve from 0.88889\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.9868 - loss: 0.3224 - val_accuracy: 0.8333 - val_loss: 0.9644\n",
      "Epoch 39/200\n",
      "\u001b[1m4/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 1.0000 - loss: 0.3220\n",
      "Epoch 39: val_accuracy did not improve from 0.88889\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 1.0000 - loss: 0.3176 - val_accuracy: 0.8056 - val_loss: 0.9704\n",
      "Epoch 40/200\n",
      "\u001b[1m4/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 1.0000 - loss: 0.3136\n",
      "Epoch 40: val_accuracy did not improve from 0.88889\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 1.0000 - loss: 0.3118 - val_accuracy: 0.7778 - val_loss: 0.9845\n",
      "Epoch 41/200\n",
      "\u001b[1m4/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.9954 - loss: 0.2945\n",
      "Epoch 41: val_accuracy did not improve from 0.88889\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.9946 - loss: 0.2945 - val_accuracy: 0.8056 - val_loss: 0.9509\n",
      "Epoch 42/200\n",
      "\u001b[1m4/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 1.0000 - loss: 0.2821\n",
      "Epoch 42: val_accuracy did not improve from 0.88889\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 1.0000 - loss: 0.2790 - val_accuracy: 0.7500 - val_loss: 1.0034\n",
      "Epoch 43/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.9986 - loss: 0.2738\n",
      "Epoch 43: val_accuracy did not improve from 0.88889\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.9977 - loss: 0.2728 - val_accuracy: 0.8611 - val_loss: 0.8551\n",
      "Epoch 44/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 1.0000 - loss: 0.2433\n",
      "Epoch 44: val_accuracy did not improve from 0.88889\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 1.0000 - loss: 0.2433 - val_accuracy: 0.7778 - val_loss: 0.9205\n",
      "Epoch 45/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.9957 - loss: 0.2454\n",
      "Epoch 45: val_accuracy did not improve from 0.88889\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.9941 - loss: 0.2483 - val_accuracy: 0.8333 - val_loss: 0.8928\n",
      "Epoch 46/200\n",
      "\u001b[1m4/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 1.0000 - loss: 0.2308\n",
      "Epoch 46: val_accuracy did not improve from 0.88889\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.9977 - loss: 0.2381 - val_accuracy: 0.8056 - val_loss: 1.0293\n",
      "Epoch 47/200\n",
      "\u001b[1m4/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.9980 - loss: 0.2680\n",
      "Epoch 47: val_accuracy did not improve from 0.88889\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9964 - loss: 0.2776 - val_accuracy: 0.8056 - val_loss: 1.0119\n",
      "Epoch 48/200\n",
      "\u001b[1m3/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 1.0000 - loss: 0.2917\n",
      "Epoch 48: val_accuracy did not improve from 0.88889\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.9928 - loss: 0.3204 - val_accuracy: 0.7778 - val_loss: 1.0493\n",
      "Epoch 49/200\n",
      "\u001b[1m4/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 1.0000 - loss: 0.3407\n",
      "Epoch 49: val_accuracy did not improve from 0.88889\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 1.0000 - loss: 0.3420 - val_accuracy: 0.7222 - val_loss: 1.1391\n",
      "Epoch 50/200\n",
      "\u001b[1m4/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 1.0000 - loss: 0.3556\n",
      "Epoch 50: val_accuracy did not improve from 0.88889\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 1.0000 - loss: 0.3567 - val_accuracy: 0.7778 - val_loss: 1.0352\n",
      "Epoch 51/200\n",
      "\u001b[1m4/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 1.0000 - loss: 0.3543\n",
      "Epoch 51: val_accuracy did not improve from 0.88889\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 1.0000 - loss: 0.3580 - val_accuracy: 0.8333 - val_loss: 0.9751\n",
      "Epoch 52/200\n",
      "\u001b[1m4/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.9980 - loss: 0.3347\n",
      "Epoch 52: val_accuracy did not improve from 0.88889\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9964 - loss: 0.3354 - val_accuracy: 0.8611 - val_loss: 0.9742\n",
      "Epoch 53/200\n",
      "\u001b[1m4/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 1.0000 - loss: 0.3205\n",
      "Epoch 53: val_accuracy did not improve from 0.88889\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 1.0000 - loss: 0.3176 - val_accuracy: 0.8889 - val_loss: 0.8881\n",
      "Epoch 54/200\n",
      "\u001b[1m4/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 1.0000 - loss: 0.2758\n",
      "Epoch 54: val_accuracy did not improve from 0.88889\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 1.0000 - loss: 0.2743 - val_accuracy: 0.8611 - val_loss: 0.8518\n",
      "Epoch 55/200\n",
      "\u001b[1m4/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 1.0000 - loss: 0.2548\n",
      "Epoch 55: val_accuracy did not improve from 0.88889\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 1.0000 - loss: 0.2544 - val_accuracy: 0.7500 - val_loss: 0.9490\n",
      "Epoch 56/200\n",
      "\u001b[1m4/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 1.0000 - loss: 0.2333\n",
      "Epoch 56: val_accuracy did not improve from 0.88889\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 1.0000 - loss: 0.2355 - val_accuracy: 0.8056 - val_loss: 0.9735\n",
      "Epoch 57/200\n",
      "\u001b[1m4/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 1.0000 - loss: 0.2173\n",
      "Epoch 57: val_accuracy did not improve from 0.88889\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 1.0000 - loss: 0.2181 - val_accuracy: 0.8056 - val_loss: 0.9568\n",
      "Epoch 58/200\n",
      "\u001b[1m4/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 1.0000 - loss: 0.2210\n",
      "Epoch 58: val_accuracy did not improve from 0.88889\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 1.0000 - loss: 0.2194 - val_accuracy: 0.8333 - val_loss: 0.9037\n",
      "Epoch 59/200\n",
      "\u001b[1m4/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.9837 - loss: 0.2176\n",
      "Epoch 59: val_accuracy did not improve from 0.88889\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9868 - loss: 0.2136 - val_accuracy: 0.7500 - val_loss: 0.9118\n",
      "Epoch 60/200\n",
      "\u001b[1m4/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 1.0000 - loss: 0.1807\n",
      "Epoch 60: val_accuracy did not improve from 0.88889\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 1.0000 - loss: 0.1795 - val_accuracy: 0.7778 - val_loss: 0.8948\n",
      "Epoch 61/200\n",
      "\u001b[1m4/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 1.0000 - loss: 0.1606\n",
      "Epoch 61: val_accuracy did not improve from 0.88889\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 1.0000 - loss: 0.1599 - val_accuracy: 0.8056 - val_loss: 0.8706\n",
      "Epoch 62/200\n",
      "\u001b[1m4/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 1.0000 - loss: 0.1470\n",
      "Epoch 62: val_accuracy did not improve from 0.88889\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 1.0000 - loss: 0.1458 - val_accuracy: 0.8889 - val_loss: 0.8257\n",
      "Epoch 63/200\n",
      "\u001b[1m4/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 1.0000 - loss: 0.1345\n",
      "Epoch 63: val_accuracy did not improve from 0.88889\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 1.0000 - loss: 0.1345 - val_accuracy: 0.8889 - val_loss: 0.7798\n",
      "Epoch 64/200\n",
      "\u001b[1m4/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.9837 - loss: 0.1327\n",
      "Epoch 64: val_accuracy did not improve from 0.88889\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9868 - loss: 0.1289 - val_accuracy: 0.8889 - val_loss: 0.7948\n",
      "Epoch 65/200\n",
      "\u001b[1m4/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 1.0000 - loss: 0.1126\n",
      "Epoch 65: val_accuracy did not improve from 0.88889\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 1.0000 - loss: 0.1129 - val_accuracy: 0.8889 - val_loss: 0.7942\n",
      "Epoch 66/200\n",
      "\u001b[1m4/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 1.0000 - loss: 0.1055\n",
      "Epoch 66: val_accuracy did not improve from 0.88889\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 1.0000 - loss: 0.1060 - val_accuracy: 0.8889 - val_loss: 0.8095\n",
      "Epoch 67/200\n",
      "\u001b[1m4/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 1.0000 - loss: 0.1101\n",
      "Epoch 67: val_accuracy did not improve from 0.88889\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 1.0000 - loss: 0.1109 - val_accuracy: 0.8889 - val_loss: 0.7564\n",
      "Epoch 68/200\n",
      "\u001b[1m4/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 1.0000 - loss: 0.1035\n",
      "Epoch 68: val_accuracy did not improve from 0.88889\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 1.0000 - loss: 0.1042 - val_accuracy: 0.8889 - val_loss: 0.8325\n",
      "Epoch 69/200\n",
      "\u001b[1m4/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.9915 - loss: 0.1148\n",
      "Epoch 69: val_accuracy did not improve from 0.88889\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9920 - loss: 0.1148 - val_accuracy: 0.8056 - val_loss: 0.9927\n",
      "Epoch 70/200\n",
      "\u001b[1m4/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 1.0000 - loss: 0.1220\n",
      "Epoch 70: val_accuracy did not improve from 0.88889\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 1.0000 - loss: 0.1206 - val_accuracy: 0.7778 - val_loss: 1.0566\n",
      "Epoch 71/200\n",
      "\u001b[1m4/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 1.0000 - loss: 0.1155\n",
      "Epoch 71: val_accuracy did not improve from 0.88889\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 1.0000 - loss: 0.1163 - val_accuracy: 0.8333 - val_loss: 0.9742\n",
      "Epoch 72/200\n",
      "\u001b[1m4/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 1.0000 - loss: 0.1110\n",
      "Epoch 72: val_accuracy did not improve from 0.88889\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 1.0000 - loss: 0.1119 - val_accuracy: 0.8889 - val_loss: 0.9213\n",
      "Epoch 73/200\n",
      "\u001b[1m4/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 1.0000 - loss: 0.1113\n",
      "Epoch 73: val_accuracy did not improve from 0.88889\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 1.0000 - loss: 0.1121 - val_accuracy: 0.8333 - val_loss: 0.9434\n",
      "Epoch 74/200\n",
      "\u001b[1m4/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 1.0000 - loss: 0.1054\n",
      "Epoch 74: val_accuracy did not improve from 0.88889\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 1.0000 - loss: 0.1056 - val_accuracy: 0.8333 - val_loss: 1.0812\n",
      "Epoch 75/200\n",
      "\u001b[1m4/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 1.0000 - loss: 0.1087\n",
      "Epoch 75: val_accuracy did not improve from 0.88889\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 1.0000 - loss: 0.1089 - val_accuracy: 0.8333 - val_loss: 0.9996\n",
      "Epoch 76/200\n",
      "\u001b[1m4/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 1.0000 - loss: 0.1032\n",
      "Epoch 76: val_accuracy did not improve from 0.88889\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 1.0000 - loss: 0.1033 - val_accuracy: 0.7778 - val_loss: 1.0132\n",
      "Epoch 77/200\n",
      "\u001b[1m4/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 1.0000 - loss: 0.1158\n",
      "Epoch 77: val_accuracy did not improve from 0.88889\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 1.0000 - loss: 0.1151 - val_accuracy: 0.7778 - val_loss: 1.0886\n",
      "Epoch 78/200\n",
      "\u001b[1m4/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 1.0000 - loss: 0.1145\n",
      "Epoch 78: val_accuracy did not improve from 0.88889\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 1.0000 - loss: 0.1156 - val_accuracy: 0.8333 - val_loss: 1.0592\n",
      "Epoch 79/200\n",
      "\u001b[1m4/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 1.0000 - loss: 0.1152\n",
      "Epoch 79: val_accuracy did not improve from 0.88889\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 1.0000 - loss: 0.1153 - val_accuracy: 0.8333 - val_loss: 1.0088\n",
      "Epoch 80/200\n",
      "\u001b[1m4/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 1.0000 - loss: 0.1153\n",
      "Epoch 80: val_accuracy did not improve from 0.88889\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 1.0000 - loss: 0.1156 - val_accuracy: 0.7778 - val_loss: 1.1312\n",
      "Epoch 81/200\n",
      "\u001b[1m4/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 1.0000 - loss: 0.1055\n",
      "Epoch 81: val_accuracy did not improve from 0.88889\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 1.0000 - loss: 0.1055 - val_accuracy: 0.7778 - val_loss: 1.3094\n",
      "Epoch 82/200\n",
      "\u001b[1m4/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.9980 - loss: 0.1117\n",
      "Epoch 82: val_accuracy did not improve from 0.88889\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9964 - loss: 0.1176 - val_accuracy: 0.7500 - val_loss: 1.2902\n",
      "Epoch 83/200\n",
      "\u001b[1m4/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 1.0000 - loss: 0.1106\n",
      "Epoch 83: val_accuracy did not improve from 0.88889\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 1.0000 - loss: 0.1145 - val_accuracy: 0.8056 - val_loss: 1.1843\n",
      "Epoch 84/200\n",
      "\u001b[1m4/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.9980 - loss: 0.1447\n",
      "Epoch 84: val_accuracy did not improve from 0.88889\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9964 - loss: 0.1490 - val_accuracy: 0.8056 - val_loss: 1.4003\n",
      "Epoch 85/200\n",
      "\u001b[1m3/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 1.0000 - loss: 0.1736\n",
      "Epoch 85: val_accuracy did not improve from 0.88889\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.9928 - loss: 0.2017 - val_accuracy: 0.7778 - val_loss: 1.7096\n",
      "Epoch 86/200\n",
      "\u001b[1m4/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.9915 - loss: 0.2621\n",
      "Epoch 86: val_accuracy did not improve from 0.88889\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9897 - loss: 0.2713 - val_accuracy: 0.6944 - val_loss: 1.7399\n",
      "Epoch 87/200\n",
      "\u001b[1m4/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.9505 - loss: 0.3931\n",
      "Epoch 87: val_accuracy did not improve from 0.88889\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.9578 - loss: 0.3908 - val_accuracy: 0.5833 - val_loss: 3.4713\n",
      "Epoch 88/200\n",
      "\u001b[1m4/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.9707 - loss: 0.5232\n",
      "Epoch 88: val_accuracy did not improve from 0.88889\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9735 - loss: 0.5221 - val_accuracy: 0.7222 - val_loss: 2.9300\n",
      "Epoch 89/200\n",
      "\u001b[1m4/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.9674 - loss: 0.6108\n",
      "Epoch 89: val_accuracy did not improve from 0.88889\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.9644 - loss: 0.6329 - val_accuracy: 0.7222 - val_loss: 3.1679\n",
      "Epoch 90/200\n",
      "\u001b[1m3/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.9965 - loss: 0.6842\n",
      "Epoch 90: val_accuracy did not improve from 0.88889\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.9755 - loss: 0.7923 - val_accuracy: 0.7222 - val_loss: 2.7086\n",
      "Epoch 91/200\n",
      "\u001b[1m4/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.9759 - loss: 0.8393\n",
      "Epoch 91: val_accuracy did not improve from 0.88889\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.9701 - loss: 0.8530 - val_accuracy: 0.6944 - val_loss: 2.7219\n",
      "Epoch 92/200\n",
      "\u001b[1m4/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.9642 - loss: 0.9868\n",
      "Epoch 92: val_accuracy did not improve from 0.88889\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9646 - loss: 0.9928 - val_accuracy: 0.6944 - val_loss: 2.8594\n",
      "Epoch 93/200\n",
      "\u001b[1m4/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.9310 - loss: 1.3841\n",
      "Epoch 93: val_accuracy did not improve from 0.88889\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9355 - loss: 1.3580 - val_accuracy: 0.6944 - val_loss: 3.3247\n",
      "Epoch 94/200\n",
      "\u001b[1m4/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.9499 - loss: 1.3365\n",
      "Epoch 94: val_accuracy did not improve from 0.88889\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9550 - loss: 1.3325 - val_accuracy: 0.6389 - val_loss: 3.6336\n",
      "Epoch 95/200\n",
      "\u001b[1m4/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.9954 - loss: 1.2395\n",
      "Epoch 95: val_accuracy did not improve from 0.88889\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.9946 - loss: 1.2459 - val_accuracy: 0.5833 - val_loss: 3.9444\n",
      "Epoch 96/200\n",
      "\u001b[1m4/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.9518 - loss: 1.3226\n",
      "Epoch 96: val_accuracy did not improve from 0.88889\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9494 - loss: 1.3237 - val_accuracy: 0.6111 - val_loss: 3.5251\n",
      "Epoch 97/200\n",
      "\u001b[1m4/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.9889 - loss: 1.2160\n",
      "Epoch 97: val_accuracy did not improve from 0.88889\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.9834 - loss: 1.2233 - val_accuracy: 0.6389 - val_loss: 3.1130\n",
      "Epoch 98/200\n",
      "\u001b[1m4/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.9831 - loss: 1.1880\n",
      "Epoch 98: val_accuracy did not improve from 0.88889\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.9841 - loss: 1.1837 - val_accuracy: 0.7222 - val_loss: 3.2664\n",
      "Epoch 99/200\n",
      "\u001b[1m4/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.9648 - loss: 1.2411\n",
      "Epoch 99: val_accuracy did not improve from 0.88889\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.9673 - loss: 1.2474 - val_accuracy: 0.7778 - val_loss: 3.5817\n",
      "Epoch 100/200\n",
      "\u001b[1m3/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9809 - loss: 1.1060\n",
      "Epoch 100: val_accuracy did not improve from 0.88889\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.9868 - loss: 1.0841 - val_accuracy: 0.7222 - val_loss: 3.5312\n",
      "Epoch 101/200\n",
      "\u001b[1m4/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 1.0000 - loss: 0.9976\n",
      "Epoch 101: val_accuracy did not improve from 0.88889\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 1.0000 - loss: 0.9934 - val_accuracy: 0.7500 - val_loss: 3.6062\n",
      "Epoch 102/200\n",
      "\u001b[1m4/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.9954 - loss: 0.9334\n",
      "Epoch 102: val_accuracy did not improve from 0.88889\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.9946 - loss: 0.9290 - val_accuracy: 0.6389 - val_loss: 3.6435\n",
      "Epoch 103/200\n",
      "\u001b[1m4/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.9837 - loss: 0.9057\n",
      "Epoch 103: val_accuracy did not improve from 0.88889\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.9845 - loss: 0.8980 - val_accuracy: 0.6944 - val_loss: 3.2064\n",
      "Epoch 104/200\n",
      "\u001b[1m4/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 1.0000 - loss: 0.7564\n",
      "Epoch 104: val_accuracy did not improve from 0.88889\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 1.0000 - loss: 0.7513 - val_accuracy: 0.6667 - val_loss: 3.1672\n",
      "Epoch 105/200\n",
      "\u001b[1m4/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.9837 - loss: 0.7078\n",
      "Epoch 105: val_accuracy did not improve from 0.88889\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9868 - loss: 0.6998 - val_accuracy: 0.6389 - val_loss: 3.3381\n",
      "Epoch 106/200\n",
      "\u001b[1m3/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.9809 - loss: 0.6431\n",
      "Epoch 106: val_accuracy did not improve from 0.88889\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.9868 - loss: 0.6340 - val_accuracy: 0.6944 - val_loss: 2.9503\n",
      "Epoch 107/200\n",
      "\u001b[1m4/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.9876 - loss: 0.6146\n",
      "Epoch 107: val_accuracy did not improve from 0.88889\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.9848 - loss: 0.6119 - val_accuracy: 0.7222 - val_loss: 2.7667\n",
      "Epoch 108/200\n",
      "\u001b[1m3/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 1.0000 - loss: 0.5215\n",
      "Epoch 108: val_accuracy did not improve from 0.88889\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.9977 - loss: 0.5206 - val_accuracy: 0.7222 - val_loss: 2.7044\n",
      "Epoch 109/200\n",
      "\u001b[1m4/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.9870 - loss: 0.5032\n",
      "Epoch 109: val_accuracy did not improve from 0.88889\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.9844 - loss: 0.5125 - val_accuracy: 0.6944 - val_loss: 2.7250\n",
      "Epoch 110/200\n",
      "\u001b[1m4/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.9746 - loss: 0.5247\n",
      "Epoch 110: val_accuracy did not improve from 0.88889\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9715 - loss: 0.5298 - val_accuracy: 0.6389 - val_loss: 3.1132\n",
      "Epoch 111/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 1.0000 - loss: 0.4988\n",
      "Epoch 111: val_accuracy did not improve from 0.88889\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 1.0000 - loss: 0.4999 - val_accuracy: 0.6667 - val_loss: 3.2505\n",
      "Epoch 112/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.9856 - loss: 0.5685\n",
      "Epoch 112: val_accuracy did not improve from 0.88889\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.9868 - loss: 0.5651 - val_accuracy: 0.6667 - val_loss: 3.0243\n",
      "Epoch 113/200\n",
      "\u001b[1m4/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.9980 - loss: 0.5531\n",
      "Epoch 113: val_accuracy did not improve from 0.88889\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.9964 - loss: 0.5632 - val_accuracy: 0.6667 - val_loss: 2.7467\n",
      "Epoch 114/200\n",
      "\u001b[1m4/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.9954 - loss: 0.5454\n",
      "Epoch 114: val_accuracy did not improve from 0.88889\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.9946 - loss: 0.5424 - val_accuracy: 0.6667 - val_loss: 2.3162\n",
      "Epoch 115/200\n",
      "\u001b[1m4/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.9954 - loss: 0.5182\n",
      "Epoch 115: val_accuracy did not improve from 0.88889\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.9946 - loss: 0.5153 - val_accuracy: 0.6944 - val_loss: 1.9532\n",
      "Epoch 116/200\n",
      "\u001b[1m4/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.9915 - loss: 0.4759\n",
      "Epoch 116: val_accuracy did not improve from 0.88889\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.9920 - loss: 0.4737 - val_accuracy: 0.7222 - val_loss: 1.7905\n",
      "Epoch 117/200\n",
      "\u001b[1m3/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.9809 - loss: 0.4939\n",
      "Epoch 117: val_accuracy did not improve from 0.88889\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.9868 - loss: 0.4705 - val_accuracy: 0.7222 - val_loss: 1.8757\n",
      "Epoch 118/200\n",
      "\u001b[1m4/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 1.0000 - loss: 0.3987\n",
      "Epoch 118: val_accuracy did not improve from 0.88889\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 1.0000 - loss: 0.3971 - val_accuracy: 0.7222 - val_loss: 2.0075\n",
      "Epoch 119/200\n",
      "\u001b[1m3/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 1.0000 - loss: 0.3698\n",
      "Epoch 119: val_accuracy did not improve from 0.88889\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 1.0000 - loss: 0.3676 - val_accuracy: 0.7500 - val_loss: 2.1719\n",
      "Epoch 120/200\n",
      "\u001b[1m4/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 1.0000 - loss: 0.3337\n",
      "Epoch 120: val_accuracy did not improve from 0.88889\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 1.0000 - loss: 0.3324 - val_accuracy: 0.7222 - val_loss: 2.1864\n",
      "Epoch 121/200\n",
      "\u001b[1m4/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.9954 - loss: 0.3077\n",
      "Epoch 121: val_accuracy did not improve from 0.88889\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.9946 - loss: 0.3069 - val_accuracy: 0.7500 - val_loss: 2.1015\n",
      "Epoch 122/200\n",
      "\u001b[1m4/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 1.0000 - loss: 0.2712\n",
      "Epoch 122: val_accuracy did not improve from 0.88889\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 1.0000 - loss: 0.2701 - val_accuracy: 0.7222 - val_loss: 1.9847\n",
      "Epoch 123/200\n",
      "\u001b[1m4/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 1.0000 - loss: 0.2467\n",
      "Epoch 123: val_accuracy did not improve from 0.88889\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 1.0000 - loss: 0.2455 - val_accuracy: 0.7222 - val_loss: 1.8475\n",
      "Epoch 124/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 1.0000 - loss: 0.2239\n",
      "Epoch 124: val_accuracy did not improve from 0.88889\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - accuracy: 1.0000 - loss: 0.2233 - val_accuracy: 0.7500 - val_loss: 1.7578\n",
      "Epoch 125/200\n",
      "\u001b[1m4/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 1.0000 - loss: 0.2059\n",
      "Epoch 125: val_accuracy did not improve from 0.88889\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 1.0000 - loss: 0.2051 - val_accuracy: 0.7222 - val_loss: 1.6598\n",
      "Epoch 126/200\n",
      "\u001b[1m4/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 1.0000 - loss: 0.1790\n",
      "Epoch 126: val_accuracy did not improve from 0.88889\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 1.0000 - loss: 0.1790 - val_accuracy: 0.7500 - val_loss: 1.5600\n",
      "Epoch 127/200\n",
      "\u001b[1m4/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 1.0000 - loss: 0.1653\n",
      "Epoch 127: val_accuracy did not improve from 0.88889\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 1.0000 - loss: 0.1647 - val_accuracy: 0.7500 - val_loss: 1.4037\n",
      "Epoch 128/200\n",
      "\u001b[1m4/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 1.0000 - loss: 0.1550\n",
      "Epoch 128: val_accuracy did not improve from 0.88889\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 1.0000 - loss: 0.1549 - val_accuracy: 0.7500 - val_loss: 1.3244\n",
      "Epoch 129/200\n",
      "\u001b[1m4/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 1.0000 - loss: 0.1517\n",
      "Epoch 129: val_accuracy did not improve from 0.88889\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 1.0000 - loss: 0.1513 - val_accuracy: 0.7500 - val_loss: 1.2719\n",
      "Epoch 130/200\n",
      "\u001b[1m4/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 1.0000 - loss: 0.1344\n",
      "Epoch 130: val_accuracy did not improve from 0.88889\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 1.0000 - loss: 0.1338 - val_accuracy: 0.8056 - val_loss: 1.2581\n",
      "Epoch 131/200\n",
      "\u001b[1m4/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 1.0000 - loss: 0.1312\n",
      "Epoch 131: val_accuracy did not improve from 0.88889\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 1.0000 - loss: 0.1300 - val_accuracy: 0.7778 - val_loss: 1.2369\n",
      "Epoch 132/200\n",
      "\u001b[1m4/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 1.0000 - loss: 0.1167\n",
      "Epoch 132: val_accuracy did not improve from 0.88889\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 1.0000 - loss: 0.1157 - val_accuracy: 0.7222 - val_loss: 1.2165\n",
      "Epoch 133/200\n",
      "\u001b[1m4/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 1.0000 - loss: 0.1061\n",
      "Epoch 133: val_accuracy did not improve from 0.88889\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 1.0000 - loss: 0.1058 - val_accuracy: 0.7222 - val_loss: 1.1855\n",
      "Epoch 134/200\n",
      "\u001b[1m4/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 1.0000 - loss: 0.0952\n",
      "Epoch 134: val_accuracy did not improve from 0.88889\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 1.0000 - loss: 0.0954 - val_accuracy: 0.7500 - val_loss: 1.1618\n",
      "Epoch 135/200\n",
      "\u001b[1m4/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 1.0000 - loss: 0.0873\n",
      "Epoch 135: val_accuracy did not improve from 0.88889\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 1.0000 - loss: 0.0877 - val_accuracy: 0.7222 - val_loss: 1.1487\n",
      "Epoch 136/200\n",
      "\u001b[1m4/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 1.0000 - loss: 0.0871\n",
      "Epoch 136: val_accuracy did not improve from 0.88889\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 1.0000 - loss: 0.0876 - val_accuracy: 0.7500 - val_loss: 1.1934\n",
      "Epoch 137/200\n",
      "\u001b[1m3/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 1.0000 - loss: 0.0838\n",
      "Epoch 137: val_accuracy did not improve from 0.88889\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 1.0000 - loss: 0.0841 - val_accuracy: 0.7500 - val_loss: 1.2272\n",
      "Epoch 138/200\n",
      "\u001b[1m4/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 1.0000 - loss: 0.0817\n",
      "Epoch 138: val_accuracy did not improve from 0.88889\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 1.0000 - loss: 0.0812 - val_accuracy: 0.7500 - val_loss: 1.2513\n",
      "Epoch 139/200\n",
      "\u001b[1m3/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 1.0000 - loss: 0.0805\n",
      "Epoch 139: val_accuracy did not improve from 0.88889\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 1.0000 - loss: 0.0794 - val_accuracy: 0.7778 - val_loss: 1.2473\n",
      "Epoch 140/200\n",
      "\u001b[1m4/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 1.0000 - loss: 0.0737\n",
      "Epoch 140: val_accuracy did not improve from 0.88889\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 1.0000 - loss: 0.0735 - val_accuracy: 0.7778 - val_loss: 1.1990\n",
      "Epoch 141/200\n",
      "\u001b[1m4/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 1.0000 - loss: 0.0727\n",
      "Epoch 141: val_accuracy did not improve from 0.88889\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 1.0000 - loss: 0.0716 - val_accuracy: 0.7778 - val_loss: 1.1521\n",
      "Epoch 142/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 1.0000 - loss: 0.0627\n",
      "Epoch 142: val_accuracy did not improve from 0.88889\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 1.0000 - loss: 0.0626 - val_accuracy: 0.7500 - val_loss: 1.1444\n",
      "Epoch 143/200\n",
      "\u001b[1m4/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 1.0000 - loss: 0.0609\n",
      "Epoch 143: val_accuracy did not improve from 0.88889\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 1.0000 - loss: 0.0599 - val_accuracy: 0.7500 - val_loss: 1.1467\n",
      "Epoch 144/200\n",
      "\u001b[1m4/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 1.0000 - loss: 0.0532\n",
      "Epoch 144: val_accuracy did not improve from 0.88889\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 1.0000 - loss: 0.0527 - val_accuracy: 0.7222 - val_loss: 1.1438\n",
      "Epoch 145/200\n",
      "\u001b[1m4/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 1.0000 - loss: 0.0481\n",
      "Epoch 145: val_accuracy did not improve from 0.88889\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 1.0000 - loss: 0.0479 - val_accuracy: 0.7500 - val_loss: 1.1151\n",
      "Epoch 146/200\n",
      "\u001b[1m4/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 1.0000 - loss: 0.0452\n",
      "Epoch 146: val_accuracy did not improve from 0.88889\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 1.0000 - loss: 0.0451 - val_accuracy: 0.7778 - val_loss: 1.0470\n",
      "Epoch 147/200\n",
      "\u001b[1m4/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 1.0000 - loss: 0.0419\n",
      "Epoch 147: val_accuracy did not improve from 0.88889\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 1.0000 - loss: 0.0418 - val_accuracy: 0.7778 - val_loss: 1.0108\n",
      "Epoch 148/200\n",
      "\u001b[1m4/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 1.0000 - loss: 0.0391\n",
      "Epoch 148: val_accuracy did not improve from 0.88889\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 1.0000 - loss: 0.0391 - val_accuracy: 0.7778 - val_loss: 1.0199\n",
      "Epoch 149/200\n",
      "\u001b[1m4/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 1.0000 - loss: 0.0379\n",
      "Epoch 149: val_accuracy did not improve from 0.88889\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 1.0000 - loss: 0.0380 - val_accuracy: 0.7500 - val_loss: 1.0430\n",
      "Epoch 150/200\n",
      "\u001b[1m4/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 1.0000 - loss: 0.0364\n",
      "Epoch 150: val_accuracy did not improve from 0.88889\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 1.0000 - loss: 0.0363 - val_accuracy: 0.7778 - val_loss: 1.0356\n",
      "Epoch 151/200\n",
      "\u001b[1m4/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 1.0000 - loss: 0.0362\n",
      "Epoch 151: val_accuracy did not improve from 0.88889\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 1.0000 - loss: 0.0360 - val_accuracy: 0.7778 - val_loss: 1.0362\n",
      "Epoch 152/200\n",
      "\u001b[1m4/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 1.0000 - loss: 0.0333\n",
      "Epoch 152: val_accuracy did not improve from 0.88889\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 1.0000 - loss: 0.0334 - val_accuracy: 0.7500 - val_loss: 1.0346\n",
      "Epoch 153/200\n",
      "\u001b[1m4/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 1.0000 - loss: 0.0326\n",
      "Epoch 153: val_accuracy did not improve from 0.88889\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 1.0000 - loss: 0.0325 - val_accuracy: 0.7500 - val_loss: 1.0487\n",
      "Epoch 154/200\n",
      "\u001b[1m4/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 1.0000 - loss: 0.0324\n",
      "Epoch 154: val_accuracy did not improve from 0.88889\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 1.0000 - loss: 0.0322 - val_accuracy: 0.7500 - val_loss: 1.0290\n",
      "Epoch 155/200\n",
      "\u001b[1m4/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 1.0000 - loss: 0.0302\n",
      "Epoch 155: val_accuracy did not improve from 0.88889\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 1.0000 - loss: 0.0302 - val_accuracy: 0.7500 - val_loss: 1.0012\n",
      "Epoch 156/200\n",
      "\u001b[1m4/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 1.0000 - loss: 0.0290\n",
      "Epoch 156: val_accuracy did not improve from 0.88889\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 1.0000 - loss: 0.0289 - val_accuracy: 0.7500 - val_loss: 0.9897\n",
      "Epoch 157/200\n",
      "\u001b[1m4/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 1.0000 - loss: 0.0291\n",
      "Epoch 157: val_accuracy did not improve from 0.88889\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 1.0000 - loss: 0.0289 - val_accuracy: 0.7778 - val_loss: 0.9694\n",
      "Epoch 158/200\n",
      "\u001b[1m4/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 1.0000 - loss: 0.0283\n",
      "Epoch 158: val_accuracy did not improve from 0.88889\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 1.0000 - loss: 0.0285 - val_accuracy: 0.7500 - val_loss: 0.9687\n",
      "Epoch 159/200\n",
      "\u001b[1m4/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 1.0000 - loss: 0.0274\n",
      "Epoch 159: val_accuracy did not improve from 0.88889\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 1.0000 - loss: 0.0277 - val_accuracy: 0.7500 - val_loss: 0.9828\n",
      "Epoch 160/200\n",
      "\u001b[1m4/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 1.0000 - loss: 0.0282\n",
      "Epoch 160: val_accuracy did not improve from 0.88889\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 1.0000 - loss: 0.0279 - val_accuracy: 0.7500 - val_loss: 1.0014\n",
      "Epoch 161/200\n",
      "\u001b[1m4/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 1.0000 - loss: 0.0277\n",
      "Epoch 161: val_accuracy did not improve from 0.88889\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 1.0000 - loss: 0.0275 - val_accuracy: 0.7500 - val_loss: 0.9863\n",
      "Epoch 162/200\n",
      "\u001b[1m4/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 1.0000 - loss: 0.0276\n",
      "Epoch 162: val_accuracy did not improve from 0.88889\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 1.0000 - loss: 0.0275 - val_accuracy: 0.8056 - val_loss: 0.9848\n",
      "Epoch 163/200\n",
      "\u001b[1m4/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 1.0000 - loss: 0.0265\n",
      "Epoch 163: val_accuracy did not improve from 0.88889\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 1.0000 - loss: 0.0265 - val_accuracy: 0.7778 - val_loss: 0.9837\n",
      "Epoch 164/200\n",
      "\u001b[1m4/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 1.0000 - loss: 0.0257\n",
      "Epoch 164: val_accuracy did not improve from 0.88889\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 1.0000 - loss: 0.0258 - val_accuracy: 0.7778 - val_loss: 0.9972\n",
      "Epoch 165/200\n",
      "\u001b[1m4/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 1.0000 - loss: 0.0261\n",
      "Epoch 165: val_accuracy did not improve from 0.88889\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 1.0000 - loss: 0.0261 - val_accuracy: 0.7778 - val_loss: 0.9874\n",
      "Epoch 166/200\n",
      "\u001b[1m4/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 1.0000 - loss: 0.0257\n",
      "Epoch 166: val_accuracy did not improve from 0.88889\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 1.0000 - loss: 0.0257 - val_accuracy: 0.7500 - val_loss: 0.9802\n",
      "Epoch 167/200\n",
      "\u001b[1m4/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 1.0000 - loss: 0.0252\n",
      "Epoch 167: val_accuracy did not improve from 0.88889\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 1.0000 - loss: 0.0251 - val_accuracy: 0.7500 - val_loss: 0.9694\n",
      "Epoch 168/200\n",
      "\u001b[1m4/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 1.0000 - loss: 0.0245\n",
      "Epoch 168: val_accuracy did not improve from 0.88889\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 1.0000 - loss: 0.0248 - val_accuracy: 0.7778 - val_loss: 0.9564\n",
      "Epoch 169/200\n",
      "\u001b[1m3/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 1.0000 - loss: 0.0253\n",
      "Epoch 169: val_accuracy did not improve from 0.88889\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 1.0000 - loss: 0.0254 - val_accuracy: 0.7778 - val_loss: 0.9620\n",
      "Epoch 170/200\n",
      "\u001b[1m4/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 1.0000 - loss: 0.0272\n",
      "Epoch 170: val_accuracy did not improve from 0.88889\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 1.0000 - loss: 0.0274 - val_accuracy: 0.7778 - val_loss: 0.9641\n",
      "Epoch 171/200\n",
      "\u001b[1m4/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 1.0000 - loss: 0.0275\n",
      "Epoch 171: val_accuracy did not improve from 0.88889\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 1.0000 - loss: 0.0276 - val_accuracy: 0.7500 - val_loss: 0.9790\n",
      "Epoch 172/200\n",
      "\u001b[1m4/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 1.0000 - loss: 0.0272\n",
      "Epoch 172: val_accuracy did not improve from 0.88889\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 1.0000 - loss: 0.0274 - val_accuracy: 0.7500 - val_loss: 0.9892\n",
      "Epoch 173/200\n",
      "\u001b[1m4/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 1.0000 - loss: 0.0266\n",
      "Epoch 173: val_accuracy did not improve from 0.88889\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 1.0000 - loss: 0.0265 - val_accuracy: 0.7778 - val_loss: 0.9604\n",
      "Epoch 174/200\n",
      "\u001b[1m4/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 1.0000 - loss: 0.0272\n",
      "Epoch 174: val_accuracy did not improve from 0.88889\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 1.0000 - loss: 0.0276 - val_accuracy: 0.7778 - val_loss: 0.9609\n",
      "Epoch 175/200\n",
      "\u001b[1m4/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 1.0000 - loss: 0.0264\n",
      "Epoch 175: val_accuracy did not improve from 0.88889\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 1.0000 - loss: 0.0267 - val_accuracy: 0.8056 - val_loss: 0.9813\n",
      "Epoch 176/200\n",
      "\u001b[1m3/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 1.0000 - loss: 0.0339\n",
      "Epoch 176: val_accuracy did not improve from 0.88889\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 1.0000 - loss: 0.0331 - val_accuracy: 0.8056 - val_loss: 1.0193\n",
      "Epoch 177/200\n",
      "\u001b[1m4/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 1.0000 - loss: 0.0323\n",
      "Epoch 177: val_accuracy did not improve from 0.88889\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 1.0000 - loss: 0.0326 - val_accuracy: 0.7500 - val_loss: 1.0375\n",
      "Epoch 178/200\n",
      "\u001b[1m4/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 1.0000 - loss: 0.0346\n",
      "Epoch 178: val_accuracy did not improve from 0.88889\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 1.0000 - loss: 0.0346 - val_accuracy: 0.7778 - val_loss: 1.0660\n",
      "Epoch 179/200\n",
      "\u001b[1m4/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 1.0000 - loss: 0.0343\n",
      "Epoch 179: val_accuracy did not improve from 0.88889\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 1.0000 - loss: 0.0341 - val_accuracy: 0.7778 - val_loss: 1.0766\n",
      "Epoch 180/200\n",
      "\u001b[1m4/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 1.0000 - loss: 0.0321\n",
      "Epoch 180: val_accuracy did not improve from 0.88889\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 1.0000 - loss: 0.0318 - val_accuracy: 0.7778 - val_loss: 1.0457\n",
      "Epoch 181/200\n",
      "\u001b[1m4/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 1.0000 - loss: 0.0293\n",
      "Epoch 181: val_accuracy did not improve from 0.88889\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 1.0000 - loss: 0.0292 - val_accuracy: 0.7500 - val_loss: 1.0076\n",
      "Epoch 182/200\n",
      "\u001b[1m4/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 1.0000 - loss: 0.0272\n",
      "Epoch 182: val_accuracy did not improve from 0.88889\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 1.0000 - loss: 0.0271 - val_accuracy: 0.7500 - val_loss: 0.9884\n",
      "Epoch 183/200\n",
      "\u001b[1m4/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 1.0000 - loss: 0.0252\n",
      "Epoch 183: val_accuracy did not improve from 0.88889\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 1.0000 - loss: 0.0253 - val_accuracy: 0.7778 - val_loss: 0.9635\n",
      "Epoch 184/200\n",
      "\u001b[1m4/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 1.0000 - loss: 0.0241\n",
      "Epoch 184: val_accuracy did not improve from 0.88889\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 1.0000 - loss: 0.0241 - val_accuracy: 0.7778 - val_loss: 0.9851\n",
      "Epoch 185/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 1.0000 - loss: 0.0240\n",
      "Epoch 185: val_accuracy did not improve from 0.88889\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 1.0000 - loss: 0.0241 - val_accuracy: 0.7500 - val_loss: 0.9994\n",
      "Epoch 186/200\n",
      "\u001b[1m3/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 1.0000 - loss: 0.0233\n",
      "Epoch 186: val_accuracy did not improve from 0.88889\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 1.0000 - loss: 0.0234 - val_accuracy: 0.7778 - val_loss: 0.9813\n",
      "Epoch 187/200\n",
      "\u001b[1m4/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 1.0000 - loss: 0.0231\n",
      "Epoch 187: val_accuracy did not improve from 0.88889\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 1.0000 - loss: 0.0230 - val_accuracy: 0.7778 - val_loss: 0.9965\n",
      "Epoch 188/200\n",
      "\u001b[1m4/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 1.0000 - loss: 0.0230\n",
      "Epoch 188: val_accuracy did not improve from 0.88889\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 1.0000 - loss: 0.0232 - val_accuracy: 0.7778 - val_loss: 0.9859\n",
      "Epoch 189/200\n",
      "\u001b[1m4/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 1.0000 - loss: 0.0236\n",
      "Epoch 189: val_accuracy did not improve from 0.88889\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 1.0000 - loss: 0.0234 - val_accuracy: 0.8056 - val_loss: 0.9947\n",
      "Epoch 190/200\n",
      "\u001b[1m4/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 1.0000 - loss: 0.0234\n",
      "Epoch 190: val_accuracy did not improve from 0.88889\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 1.0000 - loss: 0.0233 - val_accuracy: 0.7778 - val_loss: 0.9991\n",
      "Epoch 191/200\n",
      "\u001b[1m4/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 1.0000 - loss: 0.0243\n",
      "Epoch 191: val_accuracy did not improve from 0.88889\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 1.0000 - loss: 0.0241 - val_accuracy: 0.8056 - val_loss: 0.9886\n",
      "Epoch 192/200\n",
      "\u001b[1m4/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 1.0000 - loss: 0.0232\n",
      "Epoch 192: val_accuracy did not improve from 0.88889\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 1.0000 - loss: 0.0231 - val_accuracy: 0.7500 - val_loss: 0.9759\n",
      "Epoch 193/200\n",
      "\u001b[1m4/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 1.0000 - loss: 0.0236\n",
      "Epoch 193: val_accuracy did not improve from 0.88889\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 1.0000 - loss: 0.0236 - val_accuracy: 0.7778 - val_loss: 0.9544\n",
      "Epoch 194/200\n",
      "\u001b[1m4/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 1.0000 - loss: 0.0224\n",
      "Epoch 194: val_accuracy did not improve from 0.88889\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 1.0000 - loss: 0.0228 - val_accuracy: 0.7778 - val_loss: 0.9689\n",
      "Epoch 195/200\n",
      "\u001b[1m4/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 1.0000 - loss: 0.0249\n",
      "Epoch 195: val_accuracy did not improve from 0.88889\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 1.0000 - loss: 0.0246 - val_accuracy: 0.8056 - val_loss: 1.0360\n",
      "Epoch 196/200\n",
      "\u001b[1m4/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 1.0000 - loss: 0.0263\n",
      "Epoch 196: val_accuracy did not improve from 0.88889\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 1.0000 - loss: 0.0260 - val_accuracy: 0.7500 - val_loss: 1.0245\n",
      "Epoch 197/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 1.0000 - loss: 0.0251\n",
      "Epoch 197: val_accuracy did not improve from 0.88889\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 1.0000 - loss: 0.0251 - val_accuracy: 0.7778 - val_loss: 0.9771\n",
      "Epoch 198/200\n",
      "\u001b[1m4/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 1.0000 - loss: 0.0247\n",
      "Epoch 198: val_accuracy did not improve from 0.88889\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 1.0000 - loss: 0.0246 - val_accuracy: 0.7778 - val_loss: 0.9686\n",
      "Epoch 199/200\n",
      "\u001b[1m4/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 1.0000 - loss: 0.0238\n",
      "Epoch 199: val_accuracy did not improve from 0.88889\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 1.0000 - loss: 0.0238 - val_accuracy: 0.7778 - val_loss: 1.0063\n",
      "Epoch 200/200\n",
      "\u001b[1m4/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 1.0000 - loss: 0.0233\n",
      "Epoch 200: val_accuracy did not improve from 0.88889\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 1.0000 - loss: 0.0232 - val_accuracy: 0.7500 - val_loss: 1.0182\n",
      "Akurasi tertinggi di training: 1.0\n",
      "Akurasi tertinggi di validation: 0.8888888955116272\n",
      "Akurasi rata rata di validation: 0.7393055588193238\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=200,\n",
    "    batch_size=32,\n",
    "    callbacks=[checkpoint]\n",
    ")\n",
    "# Mendapatkan akurasi tertinggi di training dan validation\n",
    "max_train_acc = max(history.history['accuracy'])  # Akurasi tertinggi selama pelatihan\n",
    "max_val_acc = max(history.history['val_accuracy'])  # Akurasi tertinggi selama validasi\n",
    "average_val_acc = np.mean(history.history['val_accuracy']) # Akurasi\n",
    "\n",
    "print(f\"Akurasi tertinggi di training: {max_train_acc}\")\n",
    "print(f\"Akurasi tertinggi di validation: {max_val_acc}\")\n",
    "print(f\"Akurasi rata rata di validation: {average_val_acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model('best_model_1.keras')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_tag(text, model):\n",
    "    text = [letters.lower() for letters in text if letters not in string.punctuation]\n",
    "    text = ''.join(text)\n",
    "    print(f'After Remove : {text}')\n",
    "    text = stemmer.stem(text)\n",
    "    print(f'After Stem : {text}')\n",
    "    seq = tokenizer.texts_to_sequences([text])\n",
    "    padded_seq = pad_sequences(seq, maxlen=max_len, padding='post', truncating='post')\n",
    "    \n",
    "    # Prediksi\n",
    "    prediction = model.predict(padded_seq)\n",
    "    \n",
    "    # Menemukan index dengan probabilitas tertinggi\n",
    "    predicted_index = np.argmax(prediction, axis=1)[0]\n",
    "    \n",
    "    # Mengonversi index kembali ke tag asli\n",
    "    predicted_tag = le.inverse_transform([predicted_index])[0]\n",
    "    \n",
    "    print(predicted_tag)\n",
    "    return predicted_tag\n",
    "\n",
    "# Fungsi untuk mendapatkan tag berdasarkan index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After Remove : kapan barang yang dibawa\n",
      "After Stem : kapan barang yang bawa\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 250ms/step\n",
      "Kebutuhan\n",
      "\n",
      "Responses:\n",
      "Persiapan Makanan dan Air\n",
      "-Untuk hiking 2 hari 1 malam, bawa makanan ringan seperti energy bar, kacang, dan jerky. Pilih makanan yang bergizi dan ringan.\n",
      "-Bawa setidaknya 2-3 liter air untuk perjalanan sehari penuh, sesuaikan dengan cuaca dan aktivitas. Gunakan botol air atau hydration pack.\n",
      "-Sangat disarankan membawa makanan ringan yang mudah dimakan saat beristirahat atau mengisi energi.\n",
      "Perlengkapan Pelindung Diri\n",
      "-Bawa sunscreen untuk melindungi kulit dari sinar UV, terutama di pegunungan.\n",
      "-Selalu bawa pakaian berlapis untuk menghadapi cuaca yang berubah drastis.\n",
      "-Gunakan obat anti-serangga untuk perlindungan dari gigitan serangga.\n",
      "-Masker berguna untuk melindungi dari debu, polusi, atau angin dingin.\n",
      "Tas dan Perlengkapan Tambahan\n",
      "-Untuk hiking 2 hari 1 malam, gunakan tas kapasitas 40-50 liter.\n",
      "-Gunakan sistem packing efisien seperti roll pakaian dan simpan barang berat di tengah dekat punggung.\n",
      "Kebutuhan Pribadi dan Obat-obatan\n",
      "-Bawa obat-obatan pribadi yang diperlukan dalam jumlah cukup.\n",
      "-Power bank sangat berguna untuk mengisi daya perangkat elektronik. Solar charger bisa menjadi pilihan tambahan.\n",
      "-Tisu basah dan kering penting untuk kebersihan selama perjalanan.\n",
      "-Jangan lupa navigasi seperti peta fisik, kompas, atau GPS untuk orientasi.\n",
      "Keselamatan dan Pencegahan Cedera\n",
      "-Gunakan sepatu hiking yang sesuai dan lakukan pemanasan sebelum hiking.\n",
      "-Bawa trekking pole untuk membantu keseimbangan di medan sulit.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "user_input = input(\"Masukkan pertanyaan: \")\n",
    "predicted_tag = predict_tag(user_input, model)\n",
    "Intent_print = predicted_tag\n",
    "if Intent_print ==  \"Cuaca di Gunung\"  or Intent_print == \"Budaya\" or Intent_print == \"Jalur dan Waktu Pendakian\" or Intent_print == \"Komunitas Pendaki\":\n",
    "    print(\"Pilihlah Subject :\\n\")\n",
    "    print(\"1. Gunung Rinjani :\\n\")\n",
    "    print(\"2. Gunung Bromo :\\n\")\n",
    "    print(\"3. Gunung Merbabu :\\n\")\n",
    "    print(\"4. Gunung Prau :\\n\")\n",
    "    print(\"5. Gunung Ciremai :\\n\")\n",
    "    print(\"6. Gunung Ijen :\\n\")\n",
    "    print(\"7. Gunung Kerinci :\\n\")\n",
    "    nomor_gunung = int(input(\"masukkan nomor gunung: \"))\n",
    "    dictionarty_gunung ={1:\"gunungrinjani\",2:\"gunungbromo\",3:\"gunungmerbabu\",4:\"gunungprau\",5:\"gunungciramai\",6:\"gunungijen\",7:\"gunungkerinci\"}\n",
    "    if Intent_print == \"Budaya\":\n",
    "        if nomor_gunung >= 1 and nomor_gunung <=7:\n",
    "            for item in dataset:\n",
    "                if item[\"intent\"] == Intent_print:\n",
    "                    print(\"\\nResponses:\")\n",
    "                    for response in item[\"response\"]:\n",
    "                        if response == dictionarty_gunung[nomor_gunung]:\n",
    "                            print(item[\"response\"][response])\n",
    "        else:\n",
    "            for item in dataset:\n",
    "                if item[\"intent\"] == Intent_print:\n",
    "                    print(\"\\nResponses:\")\n",
    "                    for response in item[\"response\"]:\n",
    "                        print(f\"{response}\\n\")\n",
    "                        print(f\"{item[\"response\"][response]}\\n\")\n",
    "    else:\n",
    "        if nomor_gunung >= 1 and nomor_gunung <=7:\n",
    "            for item in dataset:\n",
    "                if item[\"intent\"] == Intent_print:\n",
    "                    print(\"\\nResponses:\")\n",
    "                    for mountain in item[\"response\"]:\n",
    "                        if mountain == dictionarty_gunung[nomor_gunung]:\n",
    "                            print(mountain)\n",
    "                            for response in item[\"response\"][mountain]:\n",
    "                                print(f\"{response} : {item[\"response\"][mountain][response]}\")\n",
    "        else:\n",
    "            for item in dataset:\n",
    "                if item[\"intent\"] == Intent_print:\n",
    "                    print(\"\\nResponses:\")\n",
    "                    for mountain in item[\"response\"]:\n",
    "                        print(\"\\n\\n\")\n",
    "                        print(mountain)\n",
    "                        print(\"\\n\")\n",
    "                        for response in item[\"response\"][mountain]:\n",
    "                            print(f\"{response} : {item[\"response\"][mountain][response]}\\n\")\n",
    "    \n",
    "elif Intent_print == \"Salam\" or Intent_print == \"Goodbye\" or Intent_print == \"Thank You\" or Intent_print == \"Confirmation\" or Intent_print == \"Help\" or Intent_print == \"Feedback\": \n",
    "    for item in dataset:\n",
    "        if item[\"intent\"] == Intent_print:\n",
    "            print(\"\\nResponses:\")\n",
    "            random_response = random.choice(item[\"response\"])\n",
    "            print(f'{random_response}')\n",
    "\n",
    "else : \n",
    "    for item in dataset:\n",
    "        if item[\"intent\"] == Intent_print:\n",
    "            print(\"\\nResponses:\")\n",
    "            for response in item[\"response\"]:\n",
    "                print(response)\n",
    "                for poin in item[\"response\"][response]:\n",
    "                    print(f\"-{poin}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pilihlah Subject :\n",
      "\n",
      "1. Gunung Rinjani :\n",
      "\n",
      "2. Gunung Bromo :\n",
      "\n",
      "3. Gunung Merbabu :\n",
      "\n",
      "4. Gunung Prau :\n",
      "\n",
      "5. Gunung Ciremai :\n",
      "\n",
      "6. Gunung Ijen :\n",
      "\n",
      "7. Gunung Kerinci :\n",
      "\n",
      "\n",
      "Responses:\n",
      "jalurpopuler : Gunung Rinjani memiliki jalur populer seperti Senaru dan Sembalun, yang menawarkan pemandangan indah namun menantang.\n",
      "jaluralternatif : Jalur alternatif tersedia di bagian selatan untuk menghindari keramaian, dengan waktu pendakian rata-rata 23 hari.\n",
      "Waktu Pendakian : Pendakian Gunung Rinjani biasanya memakan waktu 23 hari tergantung jalur yang dipilih. Jalur Sembalun lebih landai tetapi lebih panjang, sedangkan jalur Senaru lebih cepat namun menanjak.\n",
      "Waktu terbaik matahari terbit : Waktu terbaik untuk mendaki adalah dini hari jika ingin mencapai Danau Segara Anak saat matahari terbit.\n",
      "Waktu mencapai danau segara anak : Waktu yang diperlukan untuk mencapai Danau Segara Anak adalah sekitar 12 hari tergantung jalur yang dipilih.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Baca file JSON\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
