{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aE9QrVCxKcRc"
   },
   "source": [
    "Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vAwUIExmIT2u",
    "outputId": "a15b29ce-5ac3-426b-a17f-c49098f80d2c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import Libraries\n",
    "import json\n",
    "import nltk\n",
    "import time\n",
    "import random\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import IPython.display as ipd\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from tensorflow.keras.models import Model\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import Input, Embedding, LSTM, Dropout, Flatten, Dense, Bidirectional\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.models import load_model\n",
    "# import StemmerFactory class\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Mengunduh resource punkt dari NLTK\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hg3GZK-QKj7H"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CcuBs5pzcCHJ",
    "outputId": "2611d8a9-05d6-42cb-9327-f8952d6c2059"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt_tab')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "86FjGISZIrwP",
    "outputId": "cce355de-c388-4f0c-a30a-461579cbffd8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            patterns             tags\n",
      "0  Apa saja peralatan yang wajib dibawa saat mend...  Peralatan Wajib\n",
      "1   Barang apa saja yang harus ada di tas pendakian?  Peralatan Wajib\n",
      "2  Apa saja perlengkapan yang tidak boleh terlupa...  Peralatan Wajib\n",
      "3  Peralatan apa yang perlu dibawa saat mendaki g...  Peralatan Wajib\n",
      "4  Apa barang yang harus dibawa untuk pendakian g...  Peralatan Wajib\n",
      "190\n"
     ]
    }
   ],
   "source": [
    "# Load Dataset JSON\n",
    "with open('DatasetFinal.json', 'r') as f:\n",
    "    dataset = json.load(f)\n",
    "# Inisialisasi variabel\n",
    "tags = []  # Data tag\n",
    "inputs = []  # Data input atau pattern\n",
    "responses = {}  # Data respon\n",
    "words = []  # Data kata\n",
    "classes = []  # Data kelas atau tag\n",
    "documents = []  # Data dokumen\n",
    "ignore_words = ['?', '!']  # Karakter yang akan diabaikan\n",
    "\n",
    "# Iterasi melalui intents dalam JSON\n",
    "for intent in dataset:\n",
    "    if 'intent' in intent and 'patterns' in intent and 'response' in intent:\n",
    "        responses[intent['intent']] = intent['response']  # Menyimpan respons berdasarkan intent\n",
    "\n",
    "        for pattern in intent['patterns']:\n",
    "            inputs.append(pattern)\n",
    "            tags.append(intent['intent'])\n",
    "\n",
    "            # Tokenisasi pola\n",
    "            w = nltk.word_tokenize(pattern)\n",
    "            words.extend(w)\n",
    "            documents.append((w, intent['intent']))\n",
    "\n",
    "            # Tambahkan intent ke dalam kelas jika belum ada\n",
    "            if intent['intent'] not in classes:\n",
    "                classes.append(intent['intent'])\n",
    "    else:\n",
    "        print(f\"Kesalahan struktur pada intent: {intent}\")\n",
    "\n",
    "# Membuat DataFrame dari data yang sudah diproses\n",
    "data = pd.DataFrame({\"patterns\": inputs, \"tags\": tags})\n",
    "\n",
    "# Menampilkan beberapa baris pertama DataFrame\n",
    "print(data.head())\n",
    "print(len(data))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "td-_2nYdKoKE"
   },
   "source": [
    "Preprocessing The Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AD8UfJXtKxGR"
   },
   "source": [
    "\n",
    "\n",
    "1.   Remove Punctuations\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "x6lINwAUK0jO"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            patterns             tags\n",
      "0  apa saja peralatan yang wajib dibawa saat mend...  Peralatan Wajib\n",
      "1    barang apa saja yang harus ada di tas pendakian  Peralatan Wajib\n",
      "2  apa saja perlengkapan yang tidak boleh terlupa...  Peralatan Wajib\n",
      "3  peralatan apa yang perlu dibawa saat mendaki g...  Peralatan Wajib\n",
      "4  apa barang yang harus dibawa untuk pendakian g...  Peralatan Wajib\n"
     ]
    }
   ],
   "source": [
    "# Removing Punctuations (Menghilangkan Punktuasi)\n",
    "data['patterns'] = data['patterns'].apply(lambda wrd:[ltrs.lower() for ltrs in wrd if ltrs not in string.punctuation])\n",
    "data['patterns'] = data['patterns'].apply(lambda wrd: ''.join(wrd))\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zqCokQuEK4It"
   },
   "source": [
    "2. Lemmatization: Converting words to their base or root form.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "x3kwUN4cK8nt",
    "outputId": "5c0e7ea7-5e18-478a-9aa7-c53ce80998de"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1613\n"
     ]
    }
   ],
   "source": [
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "factory = StemmerFactory()\n",
    "stemmer = factory.create_stemmer()\n",
    "data['patterns'] = [stemmer.stem(sentence) for sentence in data['patterns']]\n",
    "print(len(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            patterns             tags\n",
      "0     apa saja alat yang wajib bawa saat daki gunung  Peralatan Wajib\n",
      "1         barang apa saja yang harus ada di tas daki  Peralatan Wajib\n",
      "2  apa saja lengkap yang tidak boleh lupa saat hi...  Peralatan Wajib\n",
      "3          alat apa yang perlu bawa saat daki gunung  Peralatan Wajib\n",
      "4       apa barang yang harus bawa untuk daki gunung  Peralatan Wajib\n"
     ]
    }
   ],
   "source": [
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nali\n",
      "udara\n",
      "istiadat\n",
      "siap\n",
      "ritual\n",
      "pilih\n",
      "biasa\n",
      "masyarakat\n",
      "temu\n",
      "kencang\n",
      "tinggal\n",
      "lagi\n",
      "hemat\n",
      "tambah\n",
      "alternatif\n",
      "total\n",
      "pandu\n",
      "cukup\n",
      "ketemu\n",
      "nonhalal\n",
      "guide\n",
      "ya\n",
      "air\n",
      "hindar\n",
      "yang\n",
      "dengan\n",
      "pantau\n",
      "area\n",
      "ambil\n",
      "hipotermia\n",
      "spiritual\n",
      "milik\n",
      "dingin\n",
      "hari\n",
      "ada\n",
      "sejarah\n",
      "bahan\n",
      "dekat\n",
      "malam\n",
      "bawa\n",
      "ngerti\n",
      "keluar\n",
      "perilaku\n",
      "hujan\n",
      "saat\n",
      "kamu\n",
      "hormat\n",
      "sebab\n",
      "sangat\n",
      "salah\n",
      "lindung\n",
      "dapat\n",
      "jalan\n",
      "sampai\n",
      "longsor\n",
      "buruk\n",
      "daki\n",
      "sekali\n",
      "dong\n",
      "biaya\n",
      "bagi\n",
      "kondisi\n",
      "informasi\n",
      "boleh\n",
      "hadap\n",
      "ekstrem\n",
      "awal\n",
      "kabarselamat\n",
      "tari\n",
      "sulit\n",
      "masih\n",
      "tahu\n",
      "estimasi\n",
      "capai\n",
      "liar\n",
      "transportasi\n",
      "efektif\n",
      "sore\n",
      "suhu\n",
      "basecamp\n",
      "rendah\n",
      "percaya\n",
      "tidur\n",
      "sering\n",
      "laku\n",
      "mereka\n",
      "pengaruh\n",
      "angin\n",
      "tandatanda\n",
      "tahan\n",
      "gunung\n",
      "antara\n",
      "nggak\n",
      "sesuatu\n",
      "khas\n",
      "dampak\n",
      "lokal\n",
      "tumbuh\n",
      "cegah\n",
      "acara\n",
      "kabut\n",
      "alam\n",
      "sedikit\n",
      "jaga\n",
      "perhati\n",
      "kurang\n",
      "itu\n",
      "saya\n",
      "antisipasi\n",
      "sekitar\n",
      "kerinci\n",
      "rasa\n",
      "atas\n",
      "naik\n",
      "bingung\n",
      "bagus\n",
      "langgar\n",
      "panjang\n",
      "bisa\n",
      "ijen\n",
      "di\n",
      "canggih\n",
      "asalusul\n",
      "kira\n",
      "mula\n",
      "wisatawan\n",
      "bantu\n",
      "kita\n",
      "tingkat\n",
      "agar\n",
      "aman\n",
      "tradisi\n",
      "arti\n",
      "pada\n",
      "aktivitas\n",
      "banyak\n",
      "banget\n",
      "kenal\n",
      "tunjuk\n",
      "oleh\n",
      "atau\n",
      "jejak\n",
      "bahaya\n",
      "tenda\n",
      "lebih\n",
      "jumpa\n",
      "tebal\n",
      "benar\n",
      "perlu\n",
      "iya\n",
      "petik\n",
      "oke\n",
      "cepat\n",
      "halal\n",
      "barang\n",
      "hiking\n",
      "ini\n",
      "prau\n",
      "makasih\n",
      "tolong\n",
      "penting\n",
      "tekan\n",
      "dari\n",
      "kasih\n",
      "halo\n",
      "bromo\n",
      "apakah\n",
      "ukur\n",
      "terima\n",
      "pakai\n",
      "budaya\n",
      "elektrolit\n",
      "guna\n",
      "hubung\n",
      "suka\n",
      "puncak\n",
      "rinjani\n",
      "waspada\n",
      "maksud\n",
      "mineral\n",
      "lupa\n",
      "coba\n",
      "hai\n",
      "pendek\n",
      "saja\n",
      "kawah\n",
      "betul\n",
      "luar\n",
      "gejala\n",
      "lama\n",
      "hewan\n",
      "jadi\n",
      "pagi\n",
      "tanpa\n",
      "tentu\n",
      "tibatiba\n",
      "datar\n",
      "cara\n",
      "interaksi\n",
      "nanti\n",
      "jalur\n",
      "timbang\n",
      "ikut\n",
      "lot\n",
      "jika\n",
      "ratarata\n",
      "tempat\n",
      "rakyat\n",
      "berapa\n",
      "buat\n",
      "unik\n",
      "tutup\n",
      "nih\n",
      "mana\n",
      "akurat\n",
      "dur\n",
      "lengkap\n",
      "cocok\n",
      "selesai\n",
      "komunitas\n",
      "usir\n",
      "efisien\n",
      "atur\n",
      "tidak\n",
      "sedia\n",
      "waktu\n",
      "baca\n",
      "butuh\n",
      "tapi\n",
      "merbabu\n",
      "alat\n",
      "makan\n",
      "thanks\n",
      "daerah\n",
      "aplikasi\n",
      "seperti\n",
      "festival\n",
      "minum\n",
      "musim\n",
      "baik\n",
      "datang\n",
      "utama\n",
      "belum\n",
      "bentuk\n",
      "bye\n",
      "selamat\n",
      "tas\n",
      "soal\n",
      "dehidrasi\n",
      "lain\n",
      "cerita\n",
      "bagai\n",
      "lokasi\n",
      "a\n",
      "mitos\n",
      "identifikasi\n",
      "alergi\n",
      "khusus\n",
      "rencana\n",
      "tentang\n",
      "cuaca\n",
      "gabung\n",
      "masuk\n",
      "tubuh\n",
      "dalam\n",
      "wajib\n",
      "umum\n",
      "harga\n",
      "adat\n",
      "porter\n",
      "tuju\n",
      "tradisional\n",
      "akomodasi\n",
      "harus\n",
      "langkah\n",
      "ciremai\n",
      "deras\n",
      "mudah\n",
      "patuh\n",
      "kabar\n",
      "dan\n",
      "serang\n",
      "jelas\n",
      "kawasan\n",
      "hi\n",
      "ubah\n",
      "satu\n",
      "bagaimana\n",
      "legenda\n",
      "sewa\n",
      "mendung\n",
      "apa\n",
      "untuk\n",
      "racun\n",
      "kait\n",
      "risiko\n",
      "obat\n",
      "panas\n",
      "prakira\n",
      "imbang\n",
      "ke\n",
      "diri\n",
      "semua\n",
      "pantang\n",
      "tajam\n",
      "buka\n",
      "lestari\n",
      "Jumlah kata unik dalam data: 307\n"
     ]
    }
   ],
   "source": [
    "all_words = ' '.join(data['patterns']).split()\n",
    "unique_words = set(all_words)\n",
    "a = len(unique_words)\n",
    "for word in unique_words:\n",
    "    print(word)\n",
    "print(\"Jumlah kata unik dalam data:\", a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VlmTHGTcK-tO"
   },
   "source": [
    "3.   Tokenization\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YXjbeOhCLE-P",
    "outputId": "44389531-d91d-42b5-dc06-567c8cf0283a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[7, 17, 20, 4, 35, 18, 10, 3, 2],\n",
       " [36, 7, 17, 4, 14, 11, 6, 46, 3],\n",
       " [7, 17, 47, 4, 80, 69, 107, 10, 15],\n",
       " [20, 7, 4, 24, 18, 10, 3, 2],\n",
       " [7, 36, 4, 14, 18, 12, 3, 2],\n",
       " [7, 17, 20, 55, 4, 14, 18, 10, 15, 6, 2],\n",
       " [7, 17, 36, 4, 35, 18, 6, 46, 10, 108, 2],\n",
       " [7, 17, 47, 4, 24, 12, 3, 2, 27, 42],\n",
       " [7, 36, 70, 4, 14, 18, 10, 3, 2],\n",
       " [47, 7, 17, 4, 35, 11, 56, 46, 3, 2],\n",
       " [7, 17, 20, 4, 35, 18, 10, 3, 2],\n",
       " [36, 7, 17, 4, 14, 11, 6, 46, 3],\n",
       " [7, 17, 47, 4, 80, 69, 107, 10, 15],\n",
       " [20, 7, 4, 24, 18, 10, 3, 2],\n",
       " [7, 36, 4, 14, 18, 12, 3, 2],\n",
       " [7, 17, 20, 55, 4, 14, 18, 10, 15, 6, 2],\n",
       " [7, 17, 36, 4, 35, 18, 6, 46, 10, 108, 2],\n",
       " [7, 17, 47, 4, 24, 12, 3, 2, 27, 42],\n",
       " [7, 36, 70, 4, 14, 18, 10, 3, 2],\n",
       " [47, 7, 17, 4, 35, 11, 56, 46, 3, 2],\n",
       " [5, 8, 43, 81, 48, 19, 71, 6, 2],\n",
       " [7, 4, 14, 49, 82, 19, 170, 10, 3],\n",
       " [9, 19, 2, 13, 171, 27, 50],\n",
       " [5, 57, 19, 83, 10, 3, 2],\n",
       " [7, 172, 19, 83, 4, 14, 173, 10, 15],\n",
       " [9, 33, 6, 2, 44, 72, 45, 84, 174, 175],\n",
       " [5, 8, 176, 58, 85, 10, 3, 2],\n",
       " [7, 4, 24, 177, 178, 19, 73, 3, 2],\n",
       " [5, 8, 43, 86, 12, 19, 72, 6, 2],\n",
       " [16, 33, 179, 6, 2, 87, 109, 51],\n",
       " [7, 33, 6, 2, 10, 21],\n",
       " [5, 88, 180, 89, 181, 6, 2],\n",
       " [5, 110, 88, 6, 90, 2],\n",
       " [16, 50, 88, 6, 2, 10, 21],\n",
       " [43, 59, 42, 3],\n",
       " [7, 20, 35, 12, 48, 19, 83, 6, 2],\n",
       " [5, 8, 111, 45, 33, 71, 6, 2],\n",
       " [7, 4, 14, 43, 12, 48, 33, 72, 6, 2],\n",
       " [7, 112, 70, 3, 2, 10, 58, 85],\n",
       " [7, 182, 19, 71, 87, 20, 3],\n",
       " [5, 8, 113, 114, 45, 25, 10, 3, 2],\n",
       " [7, 4, 14, 49, 82, 58, 85, 183, 184, 6, 2],\n",
       " [7, 185, 74, 12, 48, 186, 187, 6, 2],\n",
       " [5, 8, 37, 52, 188, 10, 58, 6, 2],\n",
       " [5, 8, 115, 86, 12, 15],\n",
       " [9, 3, 2, 6, 116, 58, 52],\n",
       " [9, 11, 117, 12, 118, 19, 6, 2],\n",
       " [7, 117, 119, 19, 4, 189, 12, 3, 2],\n",
       " [5, 8, 190, 119, 19, 2, 27, 120],\n",
       " [7, 121, 191, 192, 122, 59, 33, 6, 2],\n",
       " [5, 8, 123, 33, 122, 124, 20, 6, 2],\n",
       " [5, 8, 118, 33, 10, 3, 2, 124, 20, 193],\n",
       " [16, 34, 12, 3],\n",
       " [7, 17, 194, 4, 24, 43, 12, 53],\n",
       " [16, 195, 34, 196],\n",
       " [16, 34, 197, 20, 15],\n",
       " [11, 34, 12, 198, 125, 199, 3],\n",
       " [16, 34, 200, 6, 54, 201],\n",
       " [24, 43, 34, 38, 59, 126, 60, 3],\n",
       " [11, 34, 202, 12, 203, 61, 204],\n",
       " [16, 205, 34, 3],\n",
       " [5, 8, 206, 34, 60, 3],\n",
       " [207],\n",
       " [208, 7, 209, 127],\n",
       " [210, 11, 4, 13, 23, 39],\n",
       " [7, 211, 51, 21],\n",
       " [91, 212],\n",
       " [213],\n",
       " [75, 62, 91, 214],\n",
       " [215, 216],\n",
       " [128, 91, 217, 129],\n",
       " [75, 62, 130],\n",
       " [218, 219, 220],\n",
       " [221, 222, 131],\n",
       " [23, 132, 223, 39],\n",
       " [75, 62, 224, 40, 133],\n",
       " [134, 225],\n",
       " [131, 23, 226],\n",
       " [120, 227],\n",
       " [128, 135, 4, 23, 228],\n",
       " [134, 92, 135],\n",
       " [13, 39, 23],\n",
       " [23, 136, 39, 229],\n",
       " [7, 133, 13, 39],\n",
       " [230, 231, 23, 232, 233, 21],\n",
       " [13, 62, 137, 44],\n",
       " [23, 234, 8, 21, 76, 13, 138, 129],\n",
       " [139, 140, 76, 23, 235, 236, 141],\n",
       " [8, 142, 140, 76, 24, 44, 139],\n",
       " [23, 237, 21, 143, 74, 75, 62],\n",
       " [21, 132, 39, 76, 11, 141, 238],\n",
       " [5, 144, 145, 25],\n",
       " [5, 8, 93, 25, 10, 15],\n",
       " [16, 50, 25, 13, 77, 6, 2],\n",
       " [5, 8, 63, 25],\n",
       " [9, 86, 4, 146, 147, 94, 25],\n",
       " [16, 33, 4, 13, 94, 25],\n",
       " [5, 8, 95, 25, 87, 3, 239],\n",
       " [9, 148, 56, 19, 72, 13, 94, 25],\n",
       " [9, 38, 59, 126, 13, 39, 93, 25],\n",
       " [5, 8, 93, 25, 10, 148, 6, 240],\n",
       " [5, 144, 145, 41, 10, 15],\n",
       " [16, 50, 41, 13, 77, 10, 15],\n",
       " [5, 8, 37, 41, 10, 15],\n",
       " [9, 149, 241, 143, 12, 37, 41],\n",
       " [16, 130, 149, 4, 14, 18, 10, 15],\n",
       " [5, 8, 63, 41, 10, 3],\n",
       " [9, 41, 13, 52, 10, 15],\n",
       " [5, 8, 123, 138, 41, 114],\n",
       " [9, 19, 242, 44, 50, 41],\n",
       " [5, 8, 150, 243, 244, 10, 15],\n",
       " [7, 28, 52, 4, 14, 37, 10, 15],\n",
       " [5, 8, 95, 28, 245],\n",
       " [9, 246, 28, 22, 13, 38],\n",
       " [5, 8, 247, 28, 4, 13, 151, 152, 63],\n",
       " [9, 11, 28, 22, 4, 13, 151, 152, 153, 38],\n",
       " [9, 13, 11, 112, 248, 48, 28, 22],\n",
       " [5, 8, 37, 28, 249, 61, 250],\n",
       " [9, 28, 22, 6, 2, 13, 77, 153, 251, 63, 252],\n",
       " [5, 8, 253, 28, 22, 27, 42],\n",
       " [5, 8, 57, 28, 254, 4, 42, 12, 77, 63],\n",
       " [5, 8, 48, 29, 22, 10, 15],\n",
       " [9, 11, 29, 52, 6, 2],\n",
       " [5, 8, 37, 255, 29, 22],\n",
       " [9, 18, 20, 256, 29, 22, 257],\n",
       " [92, 7, 258, 29, 22, 6, 109, 51],\n",
       " [9, 259, 24, 18, 20, 111, 81, 12, 29, 22],\n",
       " [5, 8, 113, 38, 45, 29, 22],\n",
       " [9, 29, 22, 52, 13, 96, 260],\n",
       " [5, 8, 95, 261, 29, 22],\n",
       " [5, 8, 37, 29, 13, 6, 2],\n",
       " [7, 97, 262, 4, 14, 23, 57, 73, 3, 2, 64],\n",
       " [9, 11, 65, 263, 98, 6, 2, 78, 4, 24, 264],\n",
       " [7, 265, 266, 154, 2, 66],\n",
       " [9, 11, 267, 4, 49, 73, 3, 2, 99],\n",
       " [7, 4, 14, 23, 37, 155, 80, 268, 65, 6, 2, 79],\n",
       " [5, 156, 30, 31, 6, 54, 2, 67],\n",
       " [9, 11, 100, 157, 101, 2, 102],\n",
       " [9, 69, 18, 38, 269, 6, 54, 103, 2, 64],\n",
       " [7, 38, 270, 31, 4, 35, 271, 6, 84, 2, 78],\n",
       " [9, 30, 54, 2, 66, 272, 12, 68],\n",
       " [9, 11, 104, 31, 4, 14, 158, 6, 2, 79],\n",
       " [5, 3, 31, 6, 2, 67, 150, 97, 159],\n",
       " [9, 11, 273, 4, 105, 11, 6, 96, 2, 66],\n",
       " [7, 160, 55, 274, 2, 64, 161, 30, 162],\n",
       " [9, 68, 6, 2, 102, 24, 163, 106, 98, 4, 49, 30],\n",
       " [5, 8, 158, 65, 30, 6, 2, 78, 10, 3],\n",
       " [9, 11, 275, 4, 14, 276, 10, 3, 2, 79],\n",
       " [5, 104, 31, 89, 3, 6, 2, 67],\n",
       " [7, 121, 2, 99, 27, 277, 31, 6, 84, 54],\n",
       " [7, 17, 146, 142, 4, 49, 68, 154, 65, 6, 2, 66],\n",
       " [9, 68, 69, 278, 279, 45, 103, 2, 64],\n",
       " [9, 11, 100, 280, 101, 281, 2, 79],\n",
       " [5, 157, 2, 67, 100, 164, 30, 31],\n",
       " [7, 97, 4, 49, 30, 73, 3, 6, 2, 78],\n",
       " [9, 11, 282, 65, 4, 105, 283, 6, 54, 2, 66],\n",
       " [9, 38, 284, 165, 166, 6, 103, 2, 64],\n",
       " [5, 30, 2, 99, 285, 156, 31, 159],\n",
       " [9, 68, 13, 286, 27, 30, 31, 6, 2, 67],\n",
       " [7, 160, 55, 287, 102, 56, 104, 30, 162],\n",
       " [5, 8, 115, 26, 3, 4, 42],\n",
       " [16, 288, 26, 3, 21],\n",
       " [9, 11, 26, 3, 4, 44, 165, 6, 2, 21],\n",
       " [5, 8, 57, 110, 26, 3],\n",
       " [92, 7, 26, 3, 4, 289, 12, 167],\n",
       " [5, 8, 57, 26, 3, 4, 96],\n",
       " [9, 11, 26, 290, 82, 26, 70, 291],\n",
       " [16, 292, 26, 3, 21],\n",
       " [5, 8, 43, 81, 12, 26, 3, 4, 71],\n",
       " [9, 11, 26, 3, 4, 44, 293, 12, 15, 51],\n",
       " [16, 60, 40, 4, 136, 12, 3, 2, 21],\n",
       " [9, 44, 74, 3, 6, 127, 61, 294, 51],\n",
       " [5, 8, 295, 40, 53, 10, 3],\n",
       " [9, 116, 296, 89, 40, 53],\n",
       " [16, 60, 40, 53, 45, 297, 125, 90],\n",
       " [9, 11, 40, 74, 12, 3, 2, 21],\n",
       " [16, 50, 3, 298, 299, 90],\n",
       " [5, 8, 300, 40, 53, 60, 3],\n",
       " [9, 3, 13, 301, 56, 302, 51],\n",
       " [5, 8, 106, 40, 53, 155, 42, 59, 303],\n",
       " [5, 8, 168, 27, 32, 3],\n",
       " [9, 11, 32, 3, 4, 105, 3, 2, 21],\n",
       " [5, 8, 147, 137, 101, 3, 45, 32],\n",
       " [16, 55, 168, 27, 32, 3],\n",
       " [9, 32, 3, 304, 169, 61, 169],\n",
       " [5, 32, 3, 39, 3, 167],\n",
       " [9, 11, 305, 61, 166, 4, 11, 164, 32, 3],\n",
       " [5, 8, 161, 306, 27, 32, 3],\n",
       " [9, 32, 3, 307, 106, 98, 4, 14, 163],\n",
       " [5, 8, 308, 32, 3, 31]]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenize the data (Tokenisasi Data)\n",
    "vocabulary = 500\n",
    "tokenizer = Tokenizer(num_words=vocabulary, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(data['patterns'])\n",
    "train = tokenizer.texts_to_sequences(data['patterns'])\n",
    "train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0Ph33qa0LHz4"
   },
   "source": [
    "4.    Apply Padding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "S7coF8xZLL1B",
    "outputId": "edcdb753-bba7-43de-a2c0-2f2a376d7eb1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max length:\n",
      "\n",
      " 100\n",
      "[[  7  17  20 ...   0   0   0]\n",
      " [ 36   7  17 ...   0   0   0]\n",
      " [  7  17  47 ...   0   0   0]\n",
      " ...\n",
      " [  5   8 161 ...   0   0   0]\n",
      " [  9  32   3 ...   0   0   0]\n",
      " [  5   8 308 ...   0   0   0]]\n"
     ]
    }
   ],
   "source": [
    "max_len = 100\n",
    "print(\"Max length:\\n\\n\", max_len)\n",
    "# Melakukan proses padding pada data\n",
    "X_trainpad = pad_sequences(train, maxlen=max_len, padding='post', truncating='post')\n",
    "# Menampilkan hasil padding\n",
    "print(X_trainpad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IIGoTXbALNeK"
   },
   "source": [
    "5.   Encoding the Outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mUi_Z8JILSRs",
    "outputId": "903a817c-d033-4062-d8e4-402199701003"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded labels: [12 12 12 12 12 12 12 12 12 12 10 10 10 10 10 10 10 10 10 10  2  2  2  2\n",
      "  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2\n",
      "  2  2  2  2 13 13 13 13 13 13 13 13 13 13 14 14 14 14  5  5  5  5  5 15\n",
      " 15 15 15 15  1  1  1  1  1  6  6  6  6  6  4  4  4  4  4  8  8  8  8  8\n",
      "  8  8  8  8  8  3  3  3  3  3  3  3  3  3  3 16 16 16 16 16 16 16 16 16\n",
      " 16  7  7  7  7  7  7  7  7  7  7  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  9  9  9  9  9  9  9  9\n",
      "  9  9  9  9  9  9  9  9  9  9  9  9 11 11 11 11 11 11 11 11 11 11]\n",
      "Original tags: ['Budaya' 'Confirmation' 'Cuaca di Gunung' 'Dehidrasi' 'Feedback'\n",
      " 'Goodbye' 'Help' 'Hewan Liar' 'Hipotermia' 'Jalur dan Waktu Pendakian'\n",
      " 'Kebutuhan' 'Komunitas Pendaki' 'Peralatan Wajib' 'Persiapan cost'\n",
      " 'Salam' 'Thank You' 'Tumbuhan liar']\n"
     ]
    }
   ],
   "source": [
    "le = LabelEncoder()\n",
    "\n",
    "# Mengonversi label kategori menjadi angka\n",
    "Tags = le.fit_transform(data['tags'])\n",
    "\n",
    "# Menampilkan hasil encoding\n",
    "print(\"Encoded labels:\", Tags)\n",
    "\n",
    "# Menampilkan label asli yang terhubung dengan angka\n",
    "print(\"Original tags:\", le.classes_)  # Menampilkan kategori yang di-encode menjadi angka\n",
    "output_length = len(le.classes_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pattern: apa saja alat yang wajib bawa saat daki gunung | Encoded Tag: 10 | Original Tag: Kebutuhan\n",
      "Pattern: barang apa saja yang harus ada di tas daki | Encoded Tag: 10 | Original Tag: Kebutuhan\n",
      "Pattern: apa saja lengkap yang tidak boleh lupa saat hiking | Encoded Tag: 10 | Original Tag: Kebutuhan\n",
      "Pattern: alat apa yang perlu bawa saat daki gunung | Encoded Tag: 10 | Original Tag: Kebutuhan\n",
      "Pattern: apa barang yang harus bawa untuk daki gunung | Encoded Tag: 10 | Original Tag: Kebutuhan\n",
      "Pattern: apa saja alat penting yang harus bawa saat hiking di gunung | Encoded Tag: 10 | Original Tag: Kebutuhan\n",
      "Pattern: apa saja barang yang wajib bawa di tas saat naik gunung | Encoded Tag: 10 | Original Tag: Kebutuhan\n",
      "Pattern: apa saja lengkap yang perlu untuk daki gunung dengan aman | Encoded Tag: 10 | Original Tag: Kebutuhan\n",
      "Pattern: apa barang utama yang harus bawa saat daki gunung | Encoded Tag: 10 | Original Tag: Kebutuhan\n",
      "Pattern: lengkap apa saja yang wajib ada dalam tas daki gunung | Encoded Tag: 10 | Original Tag: Kebutuhan\n",
      "Pattern: bagaimana cara siap diri hadap cuaca ekstrem di gunung | Encoded Tag: 2 | Original Tag: Cuaca di Gunung\n",
      "Pattern: apa yang harus laku jika cuaca mendung saat daki | Encoded Tag: 2 | Original Tag: Cuaca di Gunung\n",
      "Pattern: apakah cuaca gunung bisa ubah dengan cepat | Encoded Tag: 2 | Original Tag: Cuaca di Gunung\n",
      "Pattern: bagaimana tahu cuaca buruk saat daki gunung | Encoded Tag: 2 | Original Tag: Cuaca di Gunung\n",
      "Pattern: apa tandatanda cuaca buruk yang harus waspada saat hiking | Encoded Tag: 2 | Original Tag: Cuaca di Gunung\n",
      "Pattern: apakah suhu di gunung lebih dingin dari daerah datar rendah | Encoded Tag: 2 | Original Tag: Cuaca di Gunung\n",
      "Pattern: bagaimana cara antisipasi hujan deras saat daki gunung | Encoded Tag: 2 | Original Tag: Cuaca di Gunung\n",
      "Pattern: apa yang perlu timbang soal cuaca belum daki gunung | Encoded Tag: 2 | Original Tag: Cuaca di Gunung\n",
      "Pattern: bagaimana cara siap pakai untuk cuaca dingin di gunung | Encoded Tag: 2 | Original Tag: Cuaca di Gunung\n",
      "Pattern: berapa suhu ratarata di gunung pada malam hari | Encoded Tag: 2 | Original Tag: Cuaca di Gunung\n",
      "Pattern: apa suhu di gunung saat ini | Encoded Tag: 2 | Original Tag: Cuaca di Gunung\n",
      "Pattern: bagaimana angin kencang pengaruh aktivitas di gunung | Encoded Tag: 2 | Original Tag: Cuaca di Gunung\n",
      "Pattern: bagaimana kondisi angin di puncak gunung | Encoded Tag: 2 | Original Tag: Cuaca di Gunung\n",
      "Pattern: berapa cepat angin di gunung saat ini | Encoded Tag: 2 | Original Tag: Cuaca di Gunung\n",
      "Pattern: siap dan aman daki | Encoded Tag: 2 | Original Tag: Cuaca di Gunung\n",
      "Pattern: apa alat wajib untuk hadap cuaca buruk di gunung | Encoded Tag: 2 | Original Tag: Cuaca di Gunung\n",
      "Pattern: bagaimana cara tahan dari suhu ekstrem di gunung | Encoded Tag: 2 | Original Tag: Cuaca di Gunung\n",
      "Pattern: apa yang harus siap untuk hadap suhu dingin di gunung | Encoded Tag: 2 | Original Tag: Cuaca di Gunung\n",
      "Pattern: apa risiko utama daki gunung saat hujan deras | Encoded Tag: 2 | Original Tag: Cuaca di Gunung\n",
      "Pattern: apa dampak cuaca ekstrem pada alat daki | Encoded Tag: 2 | Original Tag: Cuaca di Gunung\n",
      "Pattern: bagaimana cara lindung tubuh dari hipotermia saat daki gunung | Encoded Tag: 2 | Original Tag: Cuaca di Gunung\n",
      "Pattern: apa yang harus laku jika hujan deras tibatiba datang di gunung | Encoded Tag: 2 | Original Tag: Cuaca di Gunung\n",
      "Pattern: apa langkah baik untuk hadap kabut tebal di gunung | Encoded Tag: 2 | Original Tag: Cuaca di Gunung\n",
      "Pattern: bagaimana cara hindar bahaya longsor saat hujan di gunung | Encoded Tag: 2 | Original Tag: Cuaca di Gunung\n",
      "Pattern: bagaimana cara pilih pakai untuk hiking | Encoded Tag: 2 | Original Tag: Cuaca di Gunung\n",
      "Pattern: apakah daki gunung di musim hujan bahaya | Encoded Tag: 2 | Original Tag: Cuaca di Gunung\n",
      "Pattern: apakah ada aplikasi untuk pantau cuaca di gunung | Encoded Tag: 2 | Original Tag: Cuaca di Gunung\n",
      "Pattern: apa aplikasi prakira cuaca yang akurat untuk daki gunung | Encoded Tag: 2 | Original Tag: Cuaca di Gunung\n",
      "Pattern: bagaimana cara baca prakira cuaca gunung dengan benar | Encoded Tag: 2 | Original Tag: Cuaca di Gunung\n",
      "Pattern: apa hubung antara tekan udara dan suhu di gunung | Encoded Tag: 2 | Original Tag: Cuaca di Gunung\n",
      "Pattern: bagaimana cara ukur suhu udara tanpa alat di gunung | Encoded Tag: 2 | Original Tag: Cuaca di Gunung\n",
      "Pattern: bagaimana cara pantau suhu saat daki gunung tanpa alat canggih | Encoded Tag: 2 | Original Tag: Cuaca di Gunung\n",
      "Pattern: berapa biaya untuk daki | Encoded Tag: 12 | Original Tag: Persiapan cost\n",
      "Pattern: apa saja keluar yang perlu siap untuk jalan | Encoded Tag: 12 | Original Tag: Persiapan cost\n",
      "Pattern: berapa estimasi biaya transportasi | Encoded Tag: 12 | Original Tag: Persiapan cost\n",
      "Pattern: berapa biaya sewa alat hiking | Encoded Tag: 12 | Original Tag: Persiapan cost\n",
      "Pattern: ada biaya untuk masuk ke area daki | Encoded Tag: 12 | Original Tag: Persiapan cost\n",
      "Pattern: berapa biaya akomodasi di sekitar lokasi | Encoded Tag: 12 | Original Tag: Persiapan cost\n",
      "Pattern: perlu siap biaya makan dan minum lama daki | Encoded Tag: 12 | Original Tag: Persiapan cost\n",
      "Pattern: ada biaya tambah untuk guide atau porter | Encoded Tag: 12 | Original Tag: Persiapan cost\n",
      "Pattern: berapa total biaya daki | Encoded Tag: 12 | Original Tag: Persiapan cost\n",
      "Pattern: bagaimana cara hemat biaya lama daki | Encoded Tag: 12 | Original Tag: Persiapan cost\n",
      "Pattern: hi | Encoded Tag: 13 | Original Tag: Salam\n",
      "Pattern: halo apa kabarselamat pagi | Encoded Tag: 13 | Original Tag: Salam\n",
      "Pattern: hai ada yang bisa saya bantu | Encoded Tag: 13 | Original Tag: Salam\n",
      "Pattern: apa kabar hari ini | Encoded Tag: 13 | Original Tag: Salam\n",
      "Pattern: sampai jumpa | Encoded Tag: 5 | Original Tag: Goodbye\n",
      "Pattern: bye | Encoded Tag: 5 | Original Tag: Goodbye\n",
      "Pattern: terima kasih sampai nanti | Encoded Tag: 5 | Original Tag: Goodbye\n",
      "Pattern: selamat tinggal | Encoded Tag: 5 | Original Tag: Goodbye\n",
      "Pattern: oke sampai ketemu lagi | Encoded Tag: 5 | Original Tag: Goodbye\n",
      "Pattern: terima kasih banyak | Encoded Tag: 14 | Original Tag: Thank You\n",
      "Pattern: thanks a lot | Encoded Tag: 14 | Original Tag: Thank You\n",
      "Pattern: makasih banget ya | Encoded Tag: 14 | Original Tag: Thank You\n",
      "Pattern: saya sangat harga bantu | Encoded Tag: 14 | Original Tag: Thank You\n",
      "Pattern: terima kasih atas waktu kamu | Encoded Tag: 14 | Original Tag: Thank You\n",
      "Pattern: iya betul | Encoded Tag: 1 | Original Tag: Confirmation\n",
      "Pattern: ya saya tuju | Encoded Tag: 1 | Original Tag: Confirmation\n",
      "Pattern: benar sekali | Encoded Tag: 1 | Original Tag: Confirmation\n",
      "Pattern: oke itu yang saya maksud | Encoded Tag: 1 | Original Tag: Confirmation\n",
      "Pattern: iya seperti itu | Encoded Tag: 1 | Original Tag: Confirmation\n",
      "Pattern: bisa bantu saya | Encoded Tag: 6 | Original Tag: Help\n",
      "Pattern: saya butuh bantu nih | Encoded Tag: 6 | Original Tag: Help\n",
      "Pattern: apa kamu bisa bantu | Encoded Tag: 6 | Original Tag: Help\n",
      "Pattern: tolong dong saya nggak ngerti ini | Encoded Tag: 6 | Original Tag: Help\n",
      "Pattern: bisa kasih informasi lebih | Encoded Tag: 6 | Original Tag: Help\n",
      "Pattern: saya suka cara ini tapi bisa tingkat lagi | Encoded Tag: 4 | Original Tag: Feedback\n",
      "Pattern: jelas bagus tapi saya masih bingung sedikit | Encoded Tag: 4 | Original Tag: Feedback\n",
      "Pattern: cara umum bagus tapi perlu lebih jelas | Encoded Tag: 4 | Original Tag: Feedback\n",
      "Pattern: saya rasa ini cukup baik terima kasih | Encoded Tag: 4 | Original Tag: Feedback\n",
      "Pattern: ini sangat bantu tapi ada sedikit kurang | Encoded Tag: 4 | Original Tag: Feedback\n",
      "Pattern: bagaimana gejala awal hipotermia | Encoded Tag: 8 | Original Tag: Hipotermia\n",
      "Pattern: bagaimana cara cegah hipotermia saat hiking | Encoded Tag: 8 | Original Tag: Hipotermia\n",
      "Pattern: berapa cepat hipotermia bisa jadi di gunung | Encoded Tag: 8 | Original Tag: Hipotermia\n",
      "Pattern: bagaimana cara obat hipotermia | Encoded Tag: 8 | Original Tag: Hipotermia\n",
      "Pattern: apakah pakai yang salah dapat sebab hipotermia | Encoded Tag: 8 | Original Tag: Hipotermia\n",
      "Pattern: berapa suhu yang bisa sebab hipotermia | Encoded Tag: 8 | Original Tag: Hipotermia\n",
      "Pattern: bagaimana cara nali hipotermia pada daki lain | Encoded Tag: 8 | Original Tag: Hipotermia\n",
      "Pattern: apakah tidur dalam cuaca dingin bisa sebab hipotermia | Encoded Tag: 8 | Original Tag: Hipotermia\n",
      "Pattern: apakah makan dan minum bisa bantu cegah hipotermia | Encoded Tag: 8 | Original Tag: Hipotermia\n",
      "Pattern: bagaimana cara cegah hipotermia saat tidur di luar | Encoded Tag: 8 | Original Tag: Hipotermia\n",
      "Pattern: bagaimana gejala awal dehidrasi saat hiking | Encoded Tag: 3 | Original Tag: Dehidrasi\n",
      "Pattern: berapa cepat dehidrasi bisa jadi saat hiking | Encoded Tag: 3 | Original Tag: Dehidrasi\n",
      "Pattern: bagaimana cara hindar dehidrasi saat hiking | Encoded Tag: 3 | Original Tag: Dehidrasi\n",
      "Pattern: apakah air mineral cukup untuk hindar dehidrasi | Encoded Tag: 3 | Original Tag: Dehidrasi\n",
      "Pattern: berapa banyak air yang harus bawa saat hiking | Encoded Tag: 3 | Original Tag: Dehidrasi\n",
      "Pattern: bagaimana cara obat dehidrasi saat daki | Encoded Tag: 3 | Original Tag: Dehidrasi\n",
      "Pattern: apakah dehidrasi bisa bahaya saat hiking | Encoded Tag: 3 | Original Tag: Dehidrasi\n",
      "Pattern: bagaimana cara ukur tingkat dehidrasi tubuh | Encoded Tag: 3 | Original Tag: Dehidrasi\n",
      "Pattern: apakah cuaca panas lebih cepat dehidrasi | Encoded Tag: 3 | Original Tag: Dehidrasi\n",
      "Pattern: bagaimana cara jaga imbang elektrolit saat hiking | Encoded Tag: 3 | Original Tag: Dehidrasi\n",
      "Pattern: apa tumbuh bahaya yang harus hindar saat hiking | Encoded Tag: 15 | Original Tag: Tumbuhan liar\n",
      "Pattern: bagaimana cara nali tumbuh racun | Encoded Tag: 15 | Original Tag: Tumbuhan liar\n",
      "Pattern: apakah semua tumbuh liar bisa makan | Encoded Tag: 15 | Original Tag: Tumbuhan liar\n",
      "Pattern: bagaimana cara identifikasi tumbuh yang bisa guna bagai obat | Encoded Tag: 15 | Original Tag: Tumbuhan liar\n",
      "Pattern: apakah ada tumbuh liar yang bisa guna bagai bahan makan | Encoded Tag: 15 | Original Tag: Tumbuhan liar\n",
      "Pattern: apakah bisa ada risiko alergi hadap tumbuh liar | Encoded Tag: 15 | Original Tag: Tumbuhan liar\n",
      "Pattern: bagaimana cara hindar tumbuh dur atau tajam | Encoded Tag: 15 | Original Tag: Tumbuhan liar\n",
      "Pattern: apakah tumbuh liar di gunung bisa jadi bahan buat obat tradisional | Encoded Tag: 15 | Original Tag: Tumbuhan liar\n",
      "Pattern: bagaimana cara petik tumbuh liar dengan aman | Encoded Tag: 15 | Original Tag: Tumbuhan liar\n",
      "Pattern: bagaimana cara tahu tumbuh mana yang aman untuk jadi obat | Encoded Tag: 15 | Original Tag: Tumbuhan liar\n",
      "Pattern: bagaimana cara hadap hewan liar saat hiking | Encoded Tag: 7 | Original Tag: Hewan Liar\n",
      "Pattern: apakah ada hewan bahaya di gunung | Encoded Tag: 7 | Original Tag: Hewan Liar\n",
      "Pattern: bagaimana cara hindar serang hewan liar | Encoded Tag: 7 | Original Tag: Hewan Liar\n",
      "Pattern: apakah bawa alat usir hewan liar efektif | Encoded Tag: 7 | Original Tag: Hewan Liar\n",
      "Pattern: seperti apa perilaku hewan liar di malam hari | Encoded Tag: 7 | Original Tag: Hewan Liar\n",
      "Pattern: apakah kita perlu bawa alat tahan diri untuk hewan liar | Encoded Tag: 7 | Original Tag: Hewan Liar\n",
      "Pattern: bagaimana cara lindung makan dari hewan liar | Encoded Tag: 7 | Original Tag: Hewan Liar\n",
      "Pattern: apakah hewan liar bahaya bisa dekat tenda | Encoded Tag: 7 | Original Tag: Hewan Liar\n",
      "Pattern: bagaimana cara nali jejak hewan liar | Encoded Tag: 7 | Original Tag: Hewan Liar\n",
      "Pattern: bagaimana cara hindar hewan bisa di gunung | Encoded Tag: 7 | Original Tag: Hewan Liar\n",
      "Pattern: apa tradisi unik yang harus saya tahu belum daki gunung rinjani | Encoded Tag: 0 | Original Tag: Budaya\n",
      "Pattern: apakah ada adat istiadat khusus di gunung merbabu yang perlu perhati | Encoded Tag: 0 | Original Tag: Budaya\n",
      "Pattern: apa mitos kenal kait gunung bromo | Encoded Tag: 0 | Original Tag: Budaya\n",
      "Pattern: apakah ada ritual yang laku belum daki gunung prau | Encoded Tag: 0 | Original Tag: Budaya\n",
      "Pattern: apa yang harus saya hindar agar tidak langgar adat di gunung ciremai | Encoded Tag: 0 | Original Tag: Budaya\n",
      "Pattern: bagaimana budaya masyarakat lokal di sekitar gunung kerinci | Encoded Tag: 0 | Original Tag: Budaya\n",
      "Pattern: apakah ada cerita legenda tentang gunung ijen | Encoded Tag: 0 | Original Tag: Budaya\n",
      "Pattern: apakah boleh bawa makan nonhalal di sekitar kawasan gunung rinjani | Encoded Tag: 0 | Original Tag: Budaya\n",
      "Pattern: apa makan khas lokal yang wajib coba di daerah gunung merbabu | Encoded Tag: 0 | Original Tag: Budaya\n",
      "Pattern: apakah masyarakat sekitar gunung bromo buka untuk wisatawan | Encoded Tag: 0 | Original Tag: Budaya\n",
      "Pattern: apakah ada percaya lokal yang harus hormat di gunung ciremai | Encoded Tag: 0 | Original Tag: Budaya\n",
      "Pattern: bagaimana daki lokal di gunung kerinci jaga tradisi mereka | Encoded Tag: 0 | Original Tag: Budaya\n",
      "Pattern: apakah ada festival yang sering ada di dekat gunung bromo | Encoded Tag: 0 | Original Tag: Budaya\n",
      "Pattern: apa arti penting spiritual gunung rinjani bagi masyarakat tempat | Encoded Tag: 0 | Original Tag: Budaya\n",
      "Pattern: apakah wisatawan di gunung ijen perlu ikut atur khusus yang laku masyarakat | Encoded Tag: 0 | Original Tag: Budaya\n",
      "Pattern: bagaimana cara hormat adat masyarakat di gunung merbabu saat daki | Encoded Tag: 0 | Original Tag: Budaya\n",
      "Pattern: apakah ada pantang yang harus patuh saat daki gunung ciremai | Encoded Tag: 0 | Original Tag: Budaya\n",
      "Pattern: bagaimana percaya lokal pengaruh daki di gunung kerinci | Encoded Tag: 0 | Original Tag: Budaya\n",
      "Pattern: apa hubung gunung prau dengan sejarah lokal di daerah sekitar | Encoded Tag: 0 | Original Tag: Budaya\n",
      "Pattern: apa saja salah umum yang laku wisatawan kait adat di gunung bromo | Encoded Tag: 0 | Original Tag: Budaya\n",
      "Pattern: apakah wisatawan boleh ambil sesuatu dari kawasan gunung rinjani | Encoded Tag: 0 | Original Tag: Budaya\n",
      "Pattern: apakah ada cerita rakyat tentang asalusul gunung ciremai | Encoded Tag: 0 | Original Tag: Budaya\n",
      "Pattern: bagaimana legenda gunung kerinci cerita oleh masyarakat lokal | Encoded Tag: 0 | Original Tag: Budaya\n",
      "Pattern: apa tradisi yang laku masyarakat belum daki di gunung merbabu | Encoded Tag: 0 | Original Tag: Budaya\n",
      "Pattern: apakah ada tari adat yang sering tunjuk di sekitar gunung bromo | Encoded Tag: 0 | Original Tag: Budaya\n",
      "Pattern: apakah makan halal mudah temu di kawasan gunung rinjani | Encoded Tag: 0 | Original Tag: Budaya\n",
      "Pattern: bagaimana masyarakat gunung prau lestari budaya lokal mereka | Encoded Tag: 0 | Original Tag: Budaya\n",
      "Pattern: apakah wisatawan bisa interaksi dengan masyarakat lokal di gunung kerinci | Encoded Tag: 0 | Original Tag: Budaya\n",
      "Pattern: apa arti penting kawah ijen dalam percaya masyarakat tempat | Encoded Tag: 0 | Original Tag: Budaya\n",
      "Pattern: bagaimana cara pilih jalur daki yang aman | Encoded Tag: 9 | Original Tag: Jalur dan Waktu Pendakian\n",
      "Pattern: berapa sulit jalur daki ini | Encoded Tag: 9 | Original Tag: Jalur dan Waktu Pendakian\n",
      "Pattern: apakah ada jalur daki yang lebih mudah di gunung ini | Encoded Tag: 9 | Original Tag: Jalur dan Waktu Pendakian\n",
      "Pattern: bagaimana cara tahu kondisi jalur daki | Encoded Tag: 9 | Original Tag: Jalur dan Waktu Pendakian\n",
      "Pattern: seperti apa jalur daki yang cocok untuk mula | Encoded Tag: 9 | Original Tag: Jalur dan Waktu Pendakian\n",
      "Pattern: bagaimana cara tahu jalur daki yang dekat | Encoded Tag: 9 | Original Tag: Jalur dan Waktu Pendakian\n",
      "Pattern: apakah ada jalur alternatif jika jalur utama tutup | Encoded Tag: 9 | Original Tag: Jalur dan Waktu Pendakian\n",
      "Pattern: berapa panjang jalur daki ini | Encoded Tag: 9 | Original Tag: Jalur dan Waktu Pendakian\n",
      "Pattern: bagaimana cara siap diri untuk jalur daki yang ekstrem | Encoded Tag: 9 | Original Tag: Jalur dan Waktu Pendakian\n",
      "Pattern: apakah ada jalur daki yang lebih pendek untuk hiking hari | Encoded Tag: 9 | Original Tag: Jalur dan Waktu Pendakian\n",
      "Pattern: berapa lama waktu yang butuh untuk daki gunung ini | Encoded Tag: 9 | Original Tag: Jalur dan Waktu Pendakian\n",
      "Pattern: apakah lebih baik daki di pagi atau sore hari | Encoded Tag: 9 | Original Tag: Jalur dan Waktu Pendakian\n",
      "Pattern: bagaimana cara rencana waktu jalan saat daki | Encoded Tag: 9 | Original Tag: Jalur dan Waktu Pendakian\n",
      "Pattern: apakah musim tentu pengaruh waktu jalan | Encoded Tag: 9 | Original Tag: Jalur dan Waktu Pendakian\n",
      "Pattern: berapa lama waktu jalan dari basecamp ke puncak | Encoded Tag: 9 | Original Tag: Jalur dan Waktu Pendakian\n",
      "Pattern: apakah ada waktu baik untuk daki gunung ini | Encoded Tag: 9 | Original Tag: Jalur dan Waktu Pendakian\n",
      "Pattern: berapa cepat daki biasa capai puncak | Encoded Tag: 9 | Original Tag: Jalur dan Waktu Pendakian\n",
      "Pattern: bagaimana cara kira waktu jalan lama daki | Encoded Tag: 9 | Original Tag: Jalur dan Waktu Pendakian\n",
      "Pattern: apakah daki bisa selesai dalam satu hari | Encoded Tag: 9 | Original Tag: Jalur dan Waktu Pendakian\n",
      "Pattern: bagaimana cara atur waktu jalan agar aman dan efisien | Encoded Tag: 9 | Original Tag: Jalur dan Waktu Pendakian\n",
      "Pattern: bagaimana cara gabung dengan komunitas daki | Encoded Tag: 11 | Original Tag: Komunitas Pendaki\n",
      "Pattern: apakah ada komunitas daki yang sering daki gunung ini | Encoded Tag: 11 | Original Tag: Komunitas Pendaki\n",
      "Pattern: bagaimana cara dapat informasi tentang daki dari komunitas | Encoded Tag: 11 | Original Tag: Komunitas Pendaki\n",
      "Pattern: berapa penting gabung dengan komunitas daki | Encoded Tag: 11 | Original Tag: Komunitas Pendaki\n",
      "Pattern: apakah komunitas daki sedia pandu atau pandu | Encoded Tag: 11 | Original Tag: Komunitas Pendaki\n",
      "Pattern: bagaimana komunitas daki bantu daki mula | Encoded Tag: 11 | Original Tag: Komunitas Pendaki\n",
      "Pattern: apakah ada acara atau temu yang ada oleh komunitas daki | Encoded Tag: 11 | Original Tag: Komunitas Pendaki\n",
      "Pattern: bagaimana cara bagi alam dengan komunitas daki | Encoded Tag: 11 | Original Tag: Komunitas Pendaki\n",
      "Pattern: apakah komunitas daki milik atur khusus yang harus ikut | Encoded Tag: 11 | Original Tag: Komunitas Pendaki\n",
      "Pattern: bagaimana cara bentuk komunitas daki lokal | Encoded Tag: 11 | Original Tag: Komunitas Pendaki\n"
     ]
    }
   ],
   "source": [
    "for pattern, label in zip(data['patterns'], Tags):\n",
    "    print(f'Pattern: {pattern} | Encoded Tag: {label} | Original Tag: {le.inverse_transform([label])[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 564
    },
    "id": "ngQMuSDw-PNK",
    "outputId": "6a02b909-e180-463f-e8e7-e20fec8666bd"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Hitung panjang pola dalam dataset\n",
    "pattern_lengths = [len(pattern.split()) for intent in dataset for pattern in intent[\"patterns\"]]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(pattern_lengths, bins=range(1, max(pattern_lengths) + 1), edgecolor='black')\n",
    "plt.title(\"Distribusi Panjang Pola\")\n",
    "plt.xlabel(\"Jumlah Kata per Pola\")\n",
    "plt.ylabel(\"Frekuensi\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_train: (144, 100)\n",
      "Shape of X_val: (36, 100)\n",
      "Shape of y_train: (144,)\n",
      "Shape of y_test: (36,)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X_trainpad, Tags, test_size=0.2, random_state=42)\n",
    "print(\"Shape of X_train:\", X_train.shape)\n",
    "print(\"Shape of X_val:\", X_val.shape)\n",
    "print(\"Shape of y_train:\", y_train.shape)\n",
    "print(\"Shape of y_test:\",y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fEflfNnXLqAJ"
   },
   "source": [
    "# Saving the Model\n",
    "After completing the text processing in five stages, we can save the text processing model using the pickle format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gDO9d0pMLx0R"
   },
   "source": [
    "# Modeling with LSTM Algorithm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"font-weight: bold\"> Layer (type)                    </span><span style=\"font-weight: bold\"> Output Shape           </span><span style=\"font-weight: bold\">       Param # </span>\n",
       "\n",
       " input_layer_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)                         <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n",
       "\n",
       " embedding_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)          (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">25,000</span> \n",
       "\n",
       " bidirectional_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)                 <span style=\"color: #00af00; text-decoration-color: #00af00\">8,576</span> \n",
       "\n",
       " flatten_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)              (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3200</span>)                        <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n",
       "\n",
       " dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)              (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3200</span>)                        <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n",
       "\n",
       " dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)                     <span style=\"color: #00af00; text-decoration-color: #00af00\">51,216</span> \n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m\n",
       "\n",
       " input_layer_1 (\u001b[38;5;33mInputLayer\u001b[0m)       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)                         \u001b[38;5;34m0\u001b[0m \n",
       "\n",
       " embedding_1 (\u001b[38;5;33mEmbedding\u001b[0m)          (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m50\u001b[0m)                \u001b[38;5;34m25,000\u001b[0m \n",
       "\n",
       " bidirectional_1 (\u001b[38;5;33mBidirectional\u001b[0m)  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m32\u001b[0m)                 \u001b[38;5;34m8,576\u001b[0m \n",
       "\n",
       " flatten_1 (\u001b[38;5;33mFlatten\u001b[0m)              (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3200\u001b[0m)                        \u001b[38;5;34m0\u001b[0m \n",
       "\n",
       " dropout_1 (\u001b[38;5;33mDropout\u001b[0m)              (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3200\u001b[0m)                        \u001b[38;5;34m0\u001b[0m \n",
       "\n",
       " dense_1 (\u001b[38;5;33mDense\u001b[0m)                  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)                     \u001b[38;5;34m51,216\u001b[0m \n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">84,792</span> (331.22 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m84,792\u001b[0m (331.22 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">84,792</span> (331.22 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m84,792\u001b[0m (331.22 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Creating the model (Membuat Modelling)\n",
    "i = Input(shape=(max_len,))\n",
    "x = Embedding(vocabulary, 50)(i)\n",
    "x = Bidirectional(LSTM(16, return_sequences=True, recurrent_dropout=0.5,recurrent_regularizer=l2(0.01)))(x)\n",
    "x = Flatten()(x)\n",
    "x = Dropout(0.8)(x)\n",
    "x = Dense(output_length, activation=\"softmax\", kernel_regularizer=l2(0.01))(x)\n",
    "\n",
    "model  = Model(i,x) \n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=Adam(learning_rate=0.007), metrics=['accuracy'])\n",
    "\n",
    "checkpoint = ModelCheckpoint('best_model_1.keras', monitor='val_accuracy', \n",
    "                            save_best_only=True, mode='max', verbose=1)\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "\u001b[1m4/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.1445 - loss: 3.3595\n",
      "Epoch 1: val_accuracy improved from -inf to 0.19444, saving model to best_model_1.keras\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 145ms/step - accuracy: 0.1473 - loss: 3.3650 - val_accuracy: 0.1944 - val_loss: 3.3620\n",
      "Epoch 2/200\n",
      "\u001b[1m4/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.1100 - loss: 3.3370\n",
      "Epoch 2: val_accuracy did not improve from 0.19444\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.1058 - loss: 3.3418 - val_accuracy: 0.1944 - val_loss: 3.2859\n",
      "Epoch 3/200\n",
      "\u001b[1m4/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.1758 - loss: 3.2611\n",
      "Epoch 3: val_accuracy did not improve from 0.19444\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.1774 - loss: 3.2522 - val_accuracy: 0.1944 - val_loss: 3.3803\n",
      "Epoch 4/200\n",
      "\u001b[1m4/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.1654 - loss: 3.1959\n",
      "Epoch 4: val_accuracy improved from 0.19444 to 0.22222, saving model to best_model_1.keras\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - accuracy: 0.1727 - loss: 3.1883 - val_accuracy: 0.2222 - val_loss: 3.2045\n",
      "Epoch 5/200\n",
      "\u001b[1m3/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.2309 - loss: 3.0982\n",
      "Epoch 5: val_accuracy improved from 0.22222 to 0.25000, saving model to best_model_1.keras\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - accuracy: 0.2510 - loss: 3.0832 - val_accuracy: 0.2500 - val_loss: 3.1485\n",
      "Epoch 6/200\n",
      "\u001b[1m4/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.2441 - loss: 2.9002\n",
      "Epoch 6: val_accuracy improved from 0.25000 to 0.36111, saving model to best_model_1.keras\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 0.2484 - loss: 2.8868 - val_accuracy: 0.3611 - val_loss: 2.9650\n",
      "Epoch 7/200\n",
      "\u001b[1m4/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.4538 - loss: 2.6032\n",
      "Epoch 7: val_accuracy did not improve from 0.36111\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.4484 - loss: 2.6004 - val_accuracy: 0.3611 - val_loss: 2.6976\n",
      "Epoch 8/200\n",
      "\u001b[1m4/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.4883 - loss: 2.3529\n",
      "Epoch 8: val_accuracy improved from 0.36111 to 0.41667, saving model to best_model_1.keras\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.5061 - loss: 2.3103 - val_accuracy: 0.4167 - val_loss: 2.4556\n",
      "Epoch 9/200\n",
      "\u001b[1m3/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.4705 - loss: 2.1041\n",
      "Epoch 9: val_accuracy improved from 0.41667 to 0.47222, saving model to best_model_1.keras\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - accuracy: 0.5211 - loss: 2.0433 - val_accuracy: 0.4722 - val_loss: 2.1984\n",
      "Epoch 10/200\n",
      "\u001b[1m4/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.7409 - loss: 1.6804\n",
      "Epoch 10: val_accuracy did not improve from 0.47222\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.7370 - loss: 1.6699 - val_accuracy: 0.4722 - val_loss: 2.0873\n",
      "Epoch 11/200\n",
      "\u001b[1m4/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.7370 - loss: 1.4899\n",
      "Epoch 11: val_accuracy improved from 0.47222 to 0.69444, saving model to best_model_1.keras\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 0.7436 - loss: 1.4707 - val_accuracy: 0.6944 - val_loss: 1.7702\n",
      "Epoch 12/200\n",
      "\u001b[1m4/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.9460 - loss: 1.1156\n",
      "Epoch 12: val_accuracy improved from 0.69444 to 0.72222, saving model to best_model_1.keras\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 0.9431 - loss: 1.1251 - val_accuracy: 0.7222 - val_loss: 1.6563\n",
      "Epoch 13/200\n",
      "\u001b[1m3/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.7812 - loss: 1.2004\n",
      "Epoch 13: val_accuracy did not improve from 0.72222\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.8173 - loss: 1.1529 - val_accuracy: 0.6944 - val_loss: 1.5797\n",
      "Epoch 14/200\n",
      "\u001b[1m4/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.9479 - loss: 0.8808\n",
      "Epoch 14: val_accuracy improved from 0.72222 to 0.75000, saving model to best_model_1.keras\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - accuracy: 0.9398 - loss: 0.9033 - val_accuracy: 0.7500 - val_loss: 1.4859\n",
      "Epoch 15/200\n",
      "\u001b[1m4/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.9036 - loss: 0.8939\n",
      "Epoch 15: val_accuracy did not improve from 0.75000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.9034 - loss: 0.8929 - val_accuracy: 0.7222 - val_loss: 1.3569\n",
      "Epoch 16/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.9061 - loss: 0.8502\n",
      "Epoch 16: val_accuracy improved from 0.75000 to 0.77778, saving model to best_model_1.keras\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - accuracy: 0.9090 - loss: 0.8447 - val_accuracy: 0.7778 - val_loss: 1.2523\n",
      "Epoch 17/200\n",
      "\u001b[1m4/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.9323 - loss: 0.8225\n",
      "Epoch 17: val_accuracy did not improve from 0.77778\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9387 - loss: 0.8084 - val_accuracy: 0.7778 - val_loss: 1.2968\n",
      "Epoch 18/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.9359 - loss: 0.7738\n",
      "Epoch 18: val_accuracy did not improve from 0.77778\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.9362 - loss: 0.7730 - val_accuracy: 0.7778 - val_loss: 1.2473\n",
      "Epoch 19/200\n",
      "\u001b[1m4/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.9850 - loss: 0.6695\n",
      "Epoch 19: val_accuracy did not improve from 0.77778\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9784 - loss: 0.6730 - val_accuracy: 0.7778 - val_loss: 1.1755\n",
      "Epoch 20/200\n",
      "\u001b[1m4/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.9551 - loss: 0.6979\n",
      "Epoch 20: val_accuracy improved from 0.77778 to 0.80556, saving model to best_model_1.keras\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 0.9585 - loss: 0.6918 - val_accuracy: 0.8056 - val_loss: 1.1356\n",
      "Epoch 21/200\n",
      "\u001b[1m4/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.9362 - loss: 0.6744\n",
      "Epoch 21: val_accuracy did not improve from 0.80556\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9389 - loss: 0.6614 - val_accuracy: 0.8056 - val_loss: 1.0945\n",
      "Epoch 22/200\n",
      "\u001b[1m4/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.9499 - loss: 0.6211\n",
      "Epoch 22: val_accuracy did not improve from 0.80556\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9550 - loss: 0.6122 - val_accuracy: 0.7500 - val_loss: 1.1512\n",
      "Epoch 23/200\n",
      "\u001b[1m4/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.9772 - loss: 0.5819\n",
      "Epoch 23: val_accuracy did not improve from 0.80556\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9779 - loss: 0.5796 - val_accuracy: 0.7500 - val_loss: 1.1285\n",
      "Epoch 24/200\n",
      "\u001b[1m4/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.9655 - loss: 0.5529\n",
      "Epoch 24: val_accuracy did not improve from 0.80556\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.9677 - loss: 0.5485 - val_accuracy: 0.7778 - val_loss: 1.0553\n",
      "Epoch 25/200\n",
      "\u001b[1m4/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.9707 - loss: 0.5038\n",
      "Epoch 25: val_accuracy improved from 0.80556 to 0.86111, saving model to best_model_1.keras\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.9735 - loss: 0.4975 - val_accuracy: 0.8611 - val_loss: 0.9999\n",
      "Epoch 26/200\n",
      "\u001b[1m4/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.9688 - loss: 0.4686\n",
      "Epoch 26: val_accuracy did not improve from 0.86111\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9699 - loss: 0.4643 - val_accuracy: 0.7778 - val_loss: 1.0121\n",
      "Epoch 27/200\n",
      "\u001b[1m4/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 1.0000 - loss: 0.4502\n",
      "Epoch 27: val_accuracy did not improve from 0.86111\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 1.0000 - loss: 0.4444 - val_accuracy: 0.8611 - val_loss: 0.8998\n",
      "Epoch 28/200\n",
      "\u001b[1m4/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.9707 - loss: 0.4292\n",
      "Epoch 28: val_accuracy did not improve from 0.86111\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9735 - loss: 0.4291 - val_accuracy: 0.7500 - val_loss: 1.0391\n",
      "Epoch 29/200\n",
      "\u001b[1m4/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.9935 - loss: 0.4072\n",
      "Epoch 29: val_accuracy did not improve from 0.86111\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.9910 - loss: 0.4087 - val_accuracy: 0.7500 - val_loss: 1.1116\n",
      "Epoch 30/200\n",
      "\u001b[1m4/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.9837 - loss: 0.3879\n",
      "Epoch 30: val_accuracy did not improve from 0.86111\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.9868 - loss: 0.3830 - val_accuracy: 0.7778 - val_loss: 0.9406\n",
      "Epoch 31/200\n",
      "\u001b[1m4/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.9954 - loss: 0.3605\n",
      "Epoch 31: val_accuracy did not improve from 0.86111\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9946 - loss: 0.3654 - val_accuracy: 0.8333 - val_loss: 0.8791\n",
      "Epoch 32/200\n",
      "\u001b[1m4/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 1.0000 - loss: 0.3521\n",
      "Epoch 32: val_accuracy did not improve from 0.86111\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 1.0000 - loss: 0.3517 - val_accuracy: 0.8333 - val_loss: 0.8992\n",
      "Epoch 33/200\n",
      "\u001b[1m4/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.9674 - loss: 0.3631\n",
      "Epoch 33: val_accuracy did not improve from 0.86111\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9737 - loss: 0.3567 - val_accuracy: 0.7500 - val_loss: 0.8903\n",
      "Epoch 34/200\n",
      "\u001b[1m4/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 1.0000 - loss: 0.3283\n",
      "Epoch 34: val_accuracy did not improve from 0.86111\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9977 - loss: 0.3283 - val_accuracy: 0.8333 - val_loss: 0.8527\n",
      "Epoch 35/200\n",
      "\u001b[1m4/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.9889 - loss: 0.3164\n",
      "Epoch 35: val_accuracy did not improve from 0.86111\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9834 - loss: 0.3305 - val_accuracy: 0.7778 - val_loss: 0.8883\n",
      "Epoch 36/200\n",
      "\u001b[1m4/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.9954 - loss: 0.3283\n",
      "Epoch 36: val_accuracy did not improve from 0.86111\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9946 - loss: 0.3307 - val_accuracy: 0.8056 - val_loss: 0.8878\n",
      "Epoch 37/200\n",
      "\u001b[1m4/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 1.0000 - loss: 0.3370\n",
      "Epoch 37: val_accuracy did not improve from 0.86111\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 1.0000 - loss: 0.3355 - val_accuracy: 0.8333 - val_loss: 0.9003\n",
      "Epoch 38/200\n",
      "\u001b[1m4/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 1.0000 - loss: 0.3301\n",
      "Epoch 38: val_accuracy did not improve from 0.86111\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 1.0000 - loss: 0.3300 - val_accuracy: 0.8056 - val_loss: 0.9616\n",
      "Epoch 39/200\n",
      "\u001b[1m3/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 1.0000 - loss: 0.3426\n",
      "Epoch 39: val_accuracy did not improve from 0.86111\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 1.0000 - loss: 0.3362 - val_accuracy: 0.8611 - val_loss: 0.8442\n",
      "Epoch 40/200\n",
      "\u001b[1m4/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 1.0000 - loss: 0.3100\n",
      "Epoch 40: val_accuracy did not improve from 0.86111\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 1.0000 - loss: 0.3079 - val_accuracy: 0.8333 - val_loss: 0.8373\n",
      "Epoch 41/200\n",
      "\u001b[1m3/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 1.0000 - loss: 0.2792\n",
      "Epoch 41: val_accuracy did not improve from 0.86111\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 1.0000 - loss: 0.2787 - val_accuracy: 0.8611 - val_loss: 0.7857\n",
      "Epoch 42/200\n",
      "\u001b[1m4/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.9980 - loss: 0.2710\n",
      "Epoch 42: val_accuracy did not improve from 0.86111\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9964 - loss: 0.2700 - val_accuracy: 0.8611 - val_loss: 0.8846\n",
      "Epoch 43/200\n",
      "\u001b[1m4/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 1.0000 - loss: 0.2620\n",
      "Epoch 43: val_accuracy did not improve from 0.86111\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 1.0000 - loss: 0.2623 - val_accuracy: 0.7778 - val_loss: 0.9614\n",
      "Epoch 44/200\n",
      "\u001b[1m4/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.9792 - loss: 0.2691\n",
      "Epoch 44: val_accuracy did not improve from 0.86111\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9815 - loss: 0.2643 - val_accuracy: 0.8056 - val_loss: 0.9164\n",
      "Epoch 45/200\n",
      "\u001b[1m4/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.9961 - loss: 0.2546\n",
      "Epoch 45: val_accuracy did not improve from 0.86111\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.9928 - loss: 0.2595 - val_accuracy: 0.8333 - val_loss: 0.7981\n",
      "Epoch 46/200\n",
      "\u001b[1m4/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 1.0000 - loss: 0.2733\n",
      "Epoch 46: val_accuracy did not improve from 0.86111\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 1.0000 - loss: 0.2708 - val_accuracy: 0.7778 - val_loss: 0.8275\n",
      "Epoch 47/200\n",
      "\u001b[1m4/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 1.0000 - loss: 0.2606\n",
      "Epoch 47: val_accuracy did not improve from 0.86111\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 1.0000 - loss: 0.2594 - val_accuracy: 0.8333 - val_loss: 0.8268\n",
      "Epoch 48/200\n",
      "\u001b[1m4/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.9954 - loss: 0.2546\n",
      "Epoch 48: val_accuracy did not improve from 0.86111\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.9946 - loss: 0.2599 - val_accuracy: 0.8611 - val_loss: 0.7371\n",
      "Epoch 49/200\n",
      "\u001b[1m4/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 1.0000 - loss: 0.2480\n",
      "Epoch 49: val_accuracy did not improve from 0.86111\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 1.0000 - loss: 0.2492 - val_accuracy: 0.8333 - val_loss: 0.7769\n",
      "Epoch 50/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 1.0000 - loss: 0.2389\n",
      "Epoch 50: val_accuracy did not improve from 0.86111\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 1.0000 - loss: 0.2385 - val_accuracy: 0.8056 - val_loss: 0.7210\n",
      "Epoch 51/200\n",
      "\u001b[1m4/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 1.0000 - loss: 0.2282\n",
      "Epoch 51: val_accuracy did not improve from 0.86111\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 1.0000 - loss: 0.2292 - val_accuracy: 0.8333 - val_loss: 0.7191\n",
      "Epoch 52/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.9918 - loss: 0.2182\n",
      "Epoch 52: val_accuracy did not improve from 0.86111\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.9920 - loss: 0.2172 - val_accuracy: 0.8611 - val_loss: 0.7047\n",
      "Epoch 53/200\n",
      "\u001b[1m3/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 1.0000 - loss: 0.2255\n",
      "Epoch 53: val_accuracy did not improve from 0.86111\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.9964 - loss: 0.2201 - val_accuracy: 0.8611 - val_loss: 0.7579\n",
      "Epoch 54/200\n",
      "\u001b[1m4/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 1.0000 - loss: 0.1905\n",
      "Epoch 54: val_accuracy did not improve from 0.86111\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 1.0000 - loss: 0.1938 - val_accuracy: 0.8333 - val_loss: 0.6807\n",
      "Epoch 55/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9950 - loss: 0.1912\n",
      "Epoch 55: val_accuracy did not improve from 0.86111\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.9946 - loss: 0.1910 - val_accuracy: 0.8333 - val_loss: 0.8259\n",
      "Epoch 56/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 1.0000 - loss: 0.1790\n",
      "Epoch 56: val_accuracy did not improve from 0.86111\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 1.0000 - loss: 0.1803 - val_accuracy: 0.8333 - val_loss: 0.7765\n",
      "Epoch 57/200\n",
      "\u001b[1m4/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 1.0000 - loss: 0.1711\n",
      "Epoch 57: val_accuracy did not improve from 0.86111\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 1.0000 - loss: 0.1730 - val_accuracy: 0.8611 - val_loss: 0.6901\n",
      "Epoch 58/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 1.0000 - loss: 0.1776\n",
      "Epoch 58: val_accuracy did not improve from 0.86111\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - accuracy: 1.0000 - loss: 0.1777 - val_accuracy: 0.8611 - val_loss: 0.6781\n",
      "Epoch 59/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 1.0000 - loss: 0.1787\n",
      "Epoch 59: val_accuracy did not improve from 0.86111\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 1.0000 - loss: 0.1789 - val_accuracy: 0.8611 - val_loss: 0.7034\n",
      "Epoch 60/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 1.0000 - loss: 0.1716\n",
      "Epoch 60: val_accuracy did not improve from 0.86111\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 1.0000 - loss: 0.1706 - val_accuracy: 0.8333 - val_loss: 0.7212\n",
      "Epoch 61/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 1.0000 - loss: 0.1551\n",
      "Epoch 61: val_accuracy did not improve from 0.86111\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 1.0000 - loss: 0.1553 - val_accuracy: 0.8333 - val_loss: 0.7057\n",
      "Epoch 62/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 1.0000 - loss: 0.1343\n",
      "Epoch 62: val_accuracy did not improve from 0.86111\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 1.0000 - loss: 0.1343 - val_accuracy: 0.8611 - val_loss: 0.6736\n",
      "Epoch 63/200\n",
      "\u001b[1m4/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.9935 - loss: 0.1505\n",
      "Epoch 63: val_accuracy did not improve from 0.86111\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.9910 - loss: 0.1524 - val_accuracy: 0.8333 - val_loss: 0.6435\n",
      "Epoch 64/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.9826 - loss: 0.1543\n",
      "Epoch 64: val_accuracy did not improve from 0.86111\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - accuracy: 0.9832 - loss: 0.1548 - val_accuracy: 0.8056 - val_loss: 0.6463\n",
      "Epoch 65/200\n",
      "\u001b[1m4/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 1.0000 - loss: 0.1742\n",
      "Epoch 65: val_accuracy did not improve from 0.86111\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 1.0000 - loss: 0.1727 - val_accuracy: 0.8611 - val_loss: 0.7086\n",
      "Epoch 66/200\n",
      "\u001b[1m3/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.9965 - loss: 0.1805\n",
      "Epoch 66: val_accuracy did not improve from 0.86111\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.9946 - loss: 0.1864 - val_accuracy: 0.8611 - val_loss: 0.7004\n",
      "Epoch 67/200\n",
      "\u001b[1m4/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 1.0000 - loss: 0.1779\n",
      "Epoch 67: val_accuracy did not improve from 0.86111\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 1.0000 - loss: 0.1808 - val_accuracy: 0.8611 - val_loss: 0.6592\n",
      "Epoch 68/200\n",
      "\u001b[1m4/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 1.0000 - loss: 0.1855\n",
      "Epoch 68: val_accuracy did not improve from 0.86111\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 1.0000 - loss: 0.1851 - val_accuracy: 0.8611 - val_loss: 0.6689\n",
      "Epoch 69/200\n",
      "\u001b[1m4/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 1.0000 - loss: 0.1762\n",
      "Epoch 69: val_accuracy did not improve from 0.86111\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 1.0000 - loss: 0.1736 - val_accuracy: 0.8611 - val_loss: 0.6316\n",
      "Epoch 70/200\n",
      "\u001b[1m3/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 1.0000 - loss: 0.1565\n",
      "Epoch 70: val_accuracy did not improve from 0.86111\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 1.0000 - loss: 0.1550 - val_accuracy: 0.8333 - val_loss: 0.6696\n",
      "Epoch 71/200\n",
      "\u001b[1m4/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 1.0000 - loss: 0.1378\n",
      "Epoch 71: val_accuracy did not improve from 0.86111\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 1.0000 - loss: 0.1373 - val_accuracy: 0.8056 - val_loss: 0.7059\n",
      "Epoch 72/200\n",
      "\u001b[1m4/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.9837 - loss: 0.1472\n",
      "Epoch 72: val_accuracy improved from 0.86111 to 0.88889, saving model to best_model_1.keras\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.9868 - loss: 0.1433 - val_accuracy: 0.8889 - val_loss: 0.7318\n",
      "Epoch 73/200\n",
      "\u001b[1m4/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 1.0000 - loss: 0.1261\n",
      "Epoch 73: val_accuracy did not improve from 0.88889\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 1.0000 - loss: 0.1261 - val_accuracy: 0.8889 - val_loss: 0.7474\n",
      "Epoch 74/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 1.0000 - loss: 0.1244\n",
      "Epoch 74: val_accuracy did not improve from 0.88889\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 1.0000 - loss: 0.1244 - val_accuracy: 0.8611 - val_loss: 0.7911\n",
      "Epoch 75/200\n",
      "\u001b[1m4/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 1.0000 - loss: 0.1341\n",
      "Epoch 75: val_accuracy did not improve from 0.88889\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 1.0000 - loss: 0.1316 - val_accuracy: 0.8611 - val_loss: 0.8345\n",
      "Epoch 76/200\n",
      "\u001b[1m4/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 1.0000 - loss: 0.1294\n",
      "Epoch 76: val_accuracy did not improve from 0.88889\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 1.0000 - loss: 0.1281 - val_accuracy: 0.8333 - val_loss: 0.8019\n",
      "Epoch 77/200\n",
      "\u001b[1m4/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 1.0000 - loss: 0.1261\n",
      "Epoch 77: val_accuracy did not improve from 0.88889\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 1.0000 - loss: 0.1259 - val_accuracy: 0.8611 - val_loss: 1.0032\n",
      "Epoch 78/200\n",
      "\u001b[1m4/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 1.0000 - loss: 0.1235\n",
      "Epoch 78: val_accuracy did not improve from 0.88889\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 1.0000 - loss: 0.1241 - val_accuracy: 0.8889 - val_loss: 0.9639\n",
      "Epoch 79/200\n",
      "\u001b[1m4/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 1.0000 - loss: 0.1181\n",
      "Epoch 79: val_accuracy did not improve from 0.88889\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 1.0000 - loss: 0.1184 - val_accuracy: 0.8889 - val_loss: 0.7029\n",
      "Epoch 80/200\n",
      "\u001b[1m4/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 1.0000 - loss: 0.1132\n",
      "Epoch 80: val_accuracy did not improve from 0.88889\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 1.0000 - loss: 0.1137 - val_accuracy: 0.8611 - val_loss: 0.6636\n",
      "Epoch 81/200\n",
      "\u001b[1m4/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 1.0000 - loss: 0.1128\n",
      "Epoch 81: val_accuracy did not improve from 0.88889\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 1.0000 - loss: 0.1135 - val_accuracy: 0.8611 - val_loss: 0.7697\n",
      "Epoch 82/200\n",
      "\u001b[1m4/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 1.0000 - loss: 0.1252\n",
      "Epoch 82: val_accuracy did not improve from 0.88889\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 1.0000 - loss: 0.1267 - val_accuracy: 0.8611 - val_loss: 0.7048\n",
      "Epoch 83/200\n",
      "\u001b[1m4/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 1.0000 - loss: 0.1355\n",
      "Epoch 83: val_accuracy did not improve from 0.88889\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 1.0000 - loss: 0.1373 - val_accuracy: 0.8333 - val_loss: 0.8103\n",
      "Epoch 84/200\n",
      "\u001b[1m3/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 1.0000 - loss: 0.1654\n",
      "Epoch 84: val_accuracy did not improve from 0.88889\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 1.0000 - loss: 0.1641 - val_accuracy: 0.8333 - val_loss: 1.0260\n",
      "Epoch 85/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 1.0000 - loss: 0.1747\n",
      "Epoch 85: val_accuracy did not improve from 0.88889\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 1.0000 - loss: 0.1757 - val_accuracy: 0.8333 - val_loss: 0.8931\n",
      "Epoch 86/200\n",
      "\u001b[1m4/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 1.0000 - loss: 0.1818\n",
      "Epoch 86: val_accuracy did not improve from 0.88889\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 1.0000 - loss: 0.1819 - val_accuracy: 0.8333 - val_loss: 1.0487\n",
      "Epoch 87/200\n",
      "\u001b[1m4/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.9954 - loss: 0.1886\n",
      "Epoch 87: val_accuracy did not improve from 0.88889\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9946 - loss: 0.1914 - val_accuracy: 0.8611 - val_loss: 0.9281\n",
      "Epoch 88/200\n",
      "\u001b[1m4/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.9766 - loss: 0.2255\n",
      "Epoch 88: val_accuracy did not improve from 0.88889\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9751 - loss: 0.2292 - val_accuracy: 0.7500 - val_loss: 1.7481\n",
      "Epoch 89/200\n",
      "\u001b[1m4/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.9831 - loss: 0.3149\n",
      "Epoch 89: val_accuracy did not improve from 0.88889\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.9795 - loss: 0.3277 - val_accuracy: 0.8611 - val_loss: 1.3249\n",
      "Epoch 90/200\n",
      "\u001b[1m4/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.9844 - loss: 0.3642\n",
      "Epoch 90: val_accuracy did not improve from 0.88889\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.9803 - loss: 0.3818 - val_accuracy: 0.7778 - val_loss: 1.8367\n",
      "Epoch 91/200\n",
      "\u001b[1m4/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.9850 - loss: 0.5409\n",
      "Epoch 91: val_accuracy did not improve from 0.88889\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9808 - loss: 0.5694 - val_accuracy: 0.7778 - val_loss: 1.4476\n",
      "Epoch 92/200\n",
      "\u001b[1m4/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.9915 - loss: 0.6153\n",
      "Epoch 92: val_accuracy did not improve from 0.88889\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9920 - loss: 0.6220 - val_accuracy: 0.7222 - val_loss: 2.0036\n",
      "Epoch 93/200\n",
      "\u001b[1m4/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.9290 - loss: 0.8445\n",
      "Epoch 93: val_accuracy did not improve from 0.88889\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.9365 - loss: 0.8317 - val_accuracy: 0.8056 - val_loss: 1.9815\n",
      "Epoch 94/200\n",
      "\u001b[1m4/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.9577 - loss: 0.8842\n",
      "Epoch 94: val_accuracy did not improve from 0.88889\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9602 - loss: 0.8837 - val_accuracy: 0.7222 - val_loss: 3.1246\n",
      "Epoch 95/200\n",
      "\u001b[1m4/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.9701 - loss: 0.9553\n",
      "Epoch 95: val_accuracy did not improve from 0.88889\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.9708 - loss: 0.9627 - val_accuracy: 0.6667 - val_loss: 3.7264\n",
      "Epoch 96/200\n",
      "\u001b[1m3/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.9878 - loss: 0.9536\n",
      "Epoch 96: val_accuracy did not improve from 0.88889\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.9867 - loss: 0.9570 - val_accuracy: 0.6944 - val_loss: 3.7323\n",
      "Epoch 97/200\n",
      "\u001b[1m4/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.9772 - loss: 1.0595\n",
      "Epoch 97: val_accuracy did not improve from 0.88889\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9732 - loss: 1.0674 - val_accuracy: 0.7500 - val_loss: 3.2352\n",
      "Epoch 98/200\n",
      "\u001b[1m4/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.9857 - loss: 0.9783\n",
      "Epoch 98: val_accuracy did not improve from 0.88889\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.9742 - loss: 1.0203 - val_accuracy: 0.8056 - val_loss: 3.0222\n",
      "Epoch 99/200\n",
      "\u001b[1m4/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.9818 - loss: 1.1414\n",
      "Epoch 99: val_accuracy did not improve from 0.88889\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9740 - loss: 1.1592 - val_accuracy: 0.7500 - val_loss: 3.3828\n",
      "Epoch 100/200\n",
      "\u001b[1m4/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.9681 - loss: 1.1936\n",
      "Epoch 100: val_accuracy did not improve from 0.88889\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.9672 - loss: 1.2094 - val_accuracy: 0.7500 - val_loss: 3.0514\n",
      "Epoch 101/200\n",
      "\u001b[1m4/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.9622 - loss: 1.2724\n",
      "Epoch 101: val_accuracy did not improve from 0.88889\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9609 - loss: 1.3004 - val_accuracy: 0.7222 - val_loss: 3.3127\n",
      "Epoch 102/200\n",
      "\u001b[1m4/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.9837 - loss: 1.3492\n",
      "Epoch 102: val_accuracy did not improve from 0.88889\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.9868 - loss: 1.3347 - val_accuracy: 0.7500 - val_loss: 3.5742\n",
      "Epoch 103/200\n",
      "\u001b[1m4/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.9727 - loss: 1.3709\n",
      "Epoch 103: val_accuracy did not improve from 0.88889\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.9725 - loss: 1.3703 - val_accuracy: 0.7500 - val_loss: 3.7123\n",
      "Epoch 104/200\n",
      "\u001b[1m4/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.9954 - loss: 1.3312\n",
      "Epoch 104: val_accuracy did not improve from 0.88889\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9946 - loss: 1.3376 - val_accuracy: 0.7500 - val_loss: 3.6940\n",
      "Epoch 105/200\n",
      "\u001b[1m4/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.9714 - loss: 1.3337\n",
      "Epoch 105: val_accuracy did not improve from 0.88889\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9716 - loss: 1.3438 - val_accuracy: 0.7500 - val_loss: 3.3889\n",
      "Epoch 106/200\n",
      "\u001b[1m4/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.9870 - loss: 1.2406\n",
      "Epoch 106: val_accuracy did not improve from 0.88889\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.9867 - loss: 1.2396 - val_accuracy: 0.8056 - val_loss: 3.4097\n",
      "Epoch 107/200\n",
      "\u001b[1m4/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 1.0000 - loss: 1.1682\n",
      "Epoch 107: val_accuracy did not improve from 0.88889\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 1.0000 - loss: 1.1619 - val_accuracy: 0.8333 - val_loss: 3.4990\n",
      "Epoch 108/200\n",
      "\u001b[1m4/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.9818 - loss: 1.1667\n",
      "Epoch 108: val_accuracy did not improve from 0.88889\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9832 - loss: 1.1568 - val_accuracy: 0.8056 - val_loss: 3.3094\n",
      "Epoch 109/200\n",
      "\u001b[1m4/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.9915 - loss: 1.0943\n",
      "Epoch 109: val_accuracy did not improve from 0.88889\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9920 - loss: 1.0837 - val_accuracy: 0.7500 - val_loss: 3.0218\n",
      "Epoch 110/200\n",
      "\u001b[1m4/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.9896 - loss: 0.9633\n",
      "Epoch 110: val_accuracy did not improve from 0.88889\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.9884 - loss: 0.9656 - val_accuracy: 0.8056 - val_loss: 2.7059\n",
      "Epoch 111/200\n",
      "\u001b[1m4/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 1.0000 - loss: 0.8627\n",
      "Epoch 111: val_accuracy did not improve from 0.88889\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 1.0000 - loss: 0.8577 - val_accuracy: 0.8056 - val_loss: 2.5686\n",
      "Epoch 112/200\n",
      "\u001b[1m4/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.9792 - loss: 0.8796\n",
      "Epoch 112: val_accuracy did not improve from 0.88889\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.9815 - loss: 0.8598 - val_accuracy: 0.8056 - val_loss: 2.7740\n",
      "Epoch 113/200\n",
      "\u001b[1m4/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 1.0000 - loss: 0.7319\n",
      "Epoch 113: val_accuracy did not improve from 0.88889\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 1.0000 - loss: 0.7277 - val_accuracy: 0.7778 - val_loss: 2.8650\n",
      "Epoch 114/200\n",
      "\u001b[1m4/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 1.0000 - loss: 0.6719\n",
      "Epoch 114: val_accuracy did not improve from 0.88889\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 1.0000 - loss: 0.6687 - val_accuracy: 0.7778 - val_loss: 2.7866\n",
      "Epoch 115/200\n",
      "\u001b[1m4/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 1.0000 - loss: 0.6111\n",
      "Epoch 115: val_accuracy did not improve from 0.88889\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 1.0000 - loss: 0.6065 - val_accuracy: 0.7778 - val_loss: 2.4189\n",
      "Epoch 116/200\n",
      "\u001b[1m4/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.9837 - loss: 0.5974\n",
      "Epoch 116: val_accuracy did not improve from 0.88889\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.9868 - loss: 0.5847 - val_accuracy: 0.7778 - val_loss: 2.1191\n",
      "Epoch 117/200\n",
      "\u001b[1m3/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 1.0000 - loss: 0.5039\n",
      "Epoch 117: val_accuracy did not improve from 0.88889\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 1.0000 - loss: 0.4991 - val_accuracy: 0.8056 - val_loss: 1.9334\n",
      "Epoch 118/200\n",
      "\u001b[1m4/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.9980 - loss: 0.4670\n",
      "Epoch 118: val_accuracy did not improve from 0.88889\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.9964 - loss: 0.4729 - val_accuracy: 0.8056 - val_loss: 1.7748\n",
      "Epoch 119/200\n",
      "\u001b[1m4/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.9941 - loss: 0.4202\n",
      "Epoch 119: val_accuracy did not improve from 0.88889\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9891 - loss: 0.4236 - val_accuracy: 0.7778 - val_loss: 1.8835\n",
      "Epoch 120/200\n",
      "\u001b[1m4/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 1.0000 - loss: 0.4043\n",
      "Epoch 120: val_accuracy did not improve from 0.88889\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 1.0000 - loss: 0.4022 - val_accuracy: 0.7222 - val_loss: 2.2834\n",
      "Epoch 121/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.9920 - loss: 0.4097\n",
      "Epoch 121: val_accuracy did not improve from 0.88889\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.9910 - loss: 0.4109 - val_accuracy: 0.7222 - val_loss: 2.3334\n",
      "Epoch 122/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 1.0000 - loss: 0.3814\n",
      "Epoch 122: val_accuracy did not improve from 0.88889\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 1.0000 - loss: 0.3819 - val_accuracy: 0.7222 - val_loss: 2.2056\n",
      "Epoch 123/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.9856 - loss: 0.4137\n",
      "Epoch 123: val_accuracy did not improve from 0.88889\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.9868 - loss: 0.4109 - val_accuracy: 0.7222 - val_loss: 2.0758\n",
      "Epoch 124/200\n",
      "\u001b[1m3/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.9826 - loss: 0.4541\n",
      "Epoch 124: val_accuracy did not improve from 0.88889\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.9841 - loss: 0.4462 - val_accuracy: 0.7778 - val_loss: 1.9334\n",
      "Epoch 125/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.9986 - loss: 0.3750\n",
      "Epoch 125: val_accuracy did not improve from 0.88889\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.9977 - loss: 0.3787 - val_accuracy: 0.8056 - val_loss: 1.8102\n",
      "Epoch 126/200\n",
      "\u001b[1m4/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.9837 - loss: 0.3814\n",
      "Epoch 126: val_accuracy did not improve from 0.88889\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.9868 - loss: 0.3791 - val_accuracy: 0.8056 - val_loss: 1.7080\n",
      "Epoch 127/200\n",
      "\u001b[1m3/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 1.0000 - loss: 0.3941\n",
      "Epoch 127: val_accuracy did not improve from 0.88889\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 1.0000 - loss: 0.3890 - val_accuracy: 0.7778 - val_loss: 1.5938\n",
      "Epoch 128/200\n",
      "\u001b[1m3/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 1.0000 - loss: 0.3834\n",
      "Epoch 128: val_accuracy did not improve from 0.88889\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 1.0000 - loss: 0.3857 - val_accuracy: 0.7778 - val_loss: 1.5001\n",
      "Epoch 129/200\n",
      "\u001b[1m4/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 1.0000 - loss: 0.3718\n",
      "Epoch 129: val_accuracy did not improve from 0.88889\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 1.0000 - loss: 0.3706 - val_accuracy: 0.7778 - val_loss: 1.4473\n",
      "Epoch 130/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.9950 - loss: 0.3565\n",
      "Epoch 130: val_accuracy did not improve from 0.88889\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.9946 - loss: 0.3562 - val_accuracy: 0.8056 - val_loss: 1.3603\n",
      "Epoch 131/200\n",
      "\u001b[1m4/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 1.0000 - loss: 0.3325\n",
      "Epoch 131: val_accuracy did not improve from 0.88889\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 1.0000 - loss: 0.3290 - val_accuracy: 0.8056 - val_loss: 1.4157\n",
      "Epoch 132/200\n",
      "\u001b[1m4/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 1.0000 - loss: 0.2966\n",
      "Epoch 132: val_accuracy did not improve from 0.88889\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 1.0000 - loss: 0.2949 - val_accuracy: 0.7778 - val_loss: 1.5158\n",
      "Epoch 133/200\n",
      "\u001b[1m4/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 1.0000 - loss: 0.2700\n",
      "Epoch 133: val_accuracy did not improve from 0.88889\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 1.0000 - loss: 0.2691 - val_accuracy: 0.7778 - val_loss: 1.5040\n",
      "Epoch 134/200\n",
      "\u001b[1m3/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 1.0000 - loss: 0.2437\n",
      "Epoch 134: val_accuracy did not improve from 0.88889\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 1.0000 - loss: 0.2415 - val_accuracy: 0.8056 - val_loss: 1.4213\n",
      "Epoch 135/200\n",
      "\u001b[1m3/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 1.0000 - loss: 0.2208\n",
      "Epoch 135: val_accuracy did not improve from 0.88889\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 1.0000 - loss: 0.2189 - val_accuracy: 0.8333 - val_loss: 1.2707\n",
      "Epoch 136/200\n",
      "\u001b[1m4/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 1.0000 - loss: 0.1971\n",
      "Epoch 136: val_accuracy did not improve from 0.88889\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 1.0000 - loss: 0.1969 - val_accuracy: 0.8333 - val_loss: 1.1571\n",
      "Epoch 137/200\n",
      "\u001b[1m4/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.9954 - loss: 0.1784\n",
      "Epoch 137: val_accuracy did not improve from 0.88889\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.9946 - loss: 0.1782 - val_accuracy: 0.8333 - val_loss: 1.1164\n",
      "Epoch 138/200\n",
      "\u001b[1m3/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 1.0000 - loss: 0.1572\n",
      "Epoch 138: val_accuracy did not improve from 0.88889\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 1.0000 - loss: 0.1570 - val_accuracy: 0.7778 - val_loss: 1.1027\n",
      "Epoch 139/200\n",
      "\u001b[1m4/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 1.0000 - loss: 0.1479\n",
      "Epoch 139: val_accuracy did not improve from 0.88889\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 1.0000 - loss: 0.1478 - val_accuracy: 0.7500 - val_loss: 1.0877\n",
      "Epoch 140/200\n",
      "\u001b[1m4/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 1.0000 - loss: 0.1402\n",
      "Epoch 140: val_accuracy did not improve from 0.88889\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 1.0000 - loss: 0.1399 - val_accuracy: 0.8056 - val_loss: 1.0211\n",
      "Epoch 141/200\n",
      "\u001b[1m4/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 1.0000 - loss: 0.1330\n",
      "Epoch 141: val_accuracy did not improve from 0.88889\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 1.0000 - loss: 0.1325 - val_accuracy: 0.8056 - val_loss: 1.0014\n",
      "Epoch 142/200\n",
      "\u001b[1m4/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 1.0000 - loss: 0.1240\n",
      "Epoch 142: val_accuracy did not improve from 0.88889\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 1.0000 - loss: 0.1233 - val_accuracy: 0.8056 - val_loss: 0.9677\n",
      "Epoch 143/200\n",
      "\u001b[1m4/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 1.0000 - loss: 0.1143\n",
      "Epoch 143: val_accuracy did not improve from 0.88889\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 1.0000 - loss: 0.1133 - val_accuracy: 0.8056 - val_loss: 0.9302\n",
      "Epoch 144/200\n",
      "\u001b[1m4/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 1.0000 - loss: 0.1012\n",
      "Epoch 144: val_accuracy did not improve from 0.88889\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 1.0000 - loss: 0.1012 - val_accuracy: 0.8056 - val_loss: 0.9020\n",
      "Epoch 145/200\n",
      "\u001b[1m4/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 1.0000 - loss: 0.0932\n",
      "Epoch 145: val_accuracy did not improve from 0.88889\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 1.0000 - loss: 0.0922 - val_accuracy: 0.7778 - val_loss: 0.8694\n",
      "Epoch 146/200\n",
      "\u001b[1m4/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.9837 - loss: 0.1104\n",
      "Epoch 146: val_accuracy did not improve from 0.88889\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.9868 - loss: 0.1062 - val_accuracy: 0.7500 - val_loss: 0.8810\n",
      "Epoch 147/200\n",
      "\u001b[1m4/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 1.0000 - loss: 0.0827\n",
      "Epoch 147: val_accuracy did not improve from 0.88889\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 1.0000 - loss: 0.0829 - val_accuracy: 0.7500 - val_loss: 0.9027\n",
      "Epoch 148/200\n",
      "\u001b[1m4/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 1.0000 - loss: 0.0854\n",
      "Epoch 148: val_accuracy did not improve from 0.88889\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 1.0000 - loss: 0.0849 - val_accuracy: 0.7500 - val_loss: 0.8667\n",
      "Epoch 149/200\n",
      "\u001b[1m4/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 1.0000 - loss: 0.0806\n",
      "Epoch 149: val_accuracy did not improve from 0.88889\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 1.0000 - loss: 0.0806 - val_accuracy: 0.8056 - val_loss: 0.8169\n",
      "Epoch 150/200\n",
      "\u001b[1m3/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 1.0000 - loss: 0.0744\n",
      "Epoch 150: val_accuracy did not improve from 0.88889\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 1.0000 - loss: 0.0739 - val_accuracy: 0.8611 - val_loss: 0.8062\n",
      "Epoch 151/200\n",
      "\u001b[1m3/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 1.0000 - loss: 0.0694\n",
      "Epoch 151: val_accuracy did not improve from 0.88889\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 1.0000 - loss: 0.0688 - val_accuracy: 0.8611 - val_loss: 0.7986\n",
      "Epoch 152/200\n",
      "\u001b[1m4/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 1.0000 - loss: 0.0641\n",
      "Epoch 152: val_accuracy did not improve from 0.88889\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 1.0000 - loss: 0.0633 - val_accuracy: 0.8611 - val_loss: 0.7918\n",
      "Epoch 153/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 1.0000 - loss: 0.0567\n",
      "Epoch 153: val_accuracy did not improve from 0.88889\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 1.0000 - loss: 0.0565 - val_accuracy: 0.8056 - val_loss: 0.8059\n",
      "Epoch 154/200\n",
      "\u001b[1m4/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 1.0000 - loss: 0.0521\n",
      "Epoch 154: val_accuracy did not improve from 0.88889\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 1.0000 - loss: 0.0516 - val_accuracy: 0.8056 - val_loss: 0.8004\n",
      "Epoch 155/200\n",
      "\u001b[1m3/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 1.0000 - loss: 0.0474\n",
      "Epoch 155: val_accuracy did not improve from 0.88889\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 1.0000 - loss: 0.0470 - val_accuracy: 0.8056 - val_loss: 0.7983\n",
      "Epoch 156/200\n",
      "\u001b[1m3/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 1.0000 - loss: 0.0442\n",
      "Epoch 156: val_accuracy did not improve from 0.88889\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 1.0000 - loss: 0.0436 - val_accuracy: 0.8333 - val_loss: 0.7653\n",
      "Epoch 157/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 1.0000 - loss: 0.0395\n",
      "Epoch 157: val_accuracy did not improve from 0.88889\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 1.0000 - loss: 0.0394 - val_accuracy: 0.8333 - val_loss: 0.7613\n",
      "Epoch 158/200\n",
      "\u001b[1m3/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 1.0000 - loss: 0.0387\n",
      "Epoch 158: val_accuracy did not improve from 0.88889\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 1.0000 - loss: 0.0384 - val_accuracy: 0.8056 - val_loss: 0.7775\n",
      "Epoch 159/200\n",
      "\u001b[1m3/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 1.0000 - loss: 0.0347\n",
      "Epoch 159: val_accuracy did not improve from 0.88889\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 1.0000 - loss: 0.0348 - val_accuracy: 0.8056 - val_loss: 0.7857\n",
      "Epoch 160/200\n",
      "\u001b[1m4/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 1.0000 - loss: 0.0333\n",
      "Epoch 160: val_accuracy did not improve from 0.88889\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 1.0000 - loss: 0.0334 - val_accuracy: 0.8056 - val_loss: 0.7881\n",
      "Epoch 161/200\n",
      "\u001b[1m4/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 1.0000 - loss: 0.0341\n",
      "Epoch 161: val_accuracy did not improve from 0.88889\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 1.0000 - loss: 0.0338 - val_accuracy: 0.8056 - val_loss: 0.7761\n",
      "Epoch 162/200\n",
      "\u001b[1m3/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 1.0000 - loss: 0.0314\n",
      "Epoch 162: val_accuracy did not improve from 0.88889\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 1.0000 - loss: 0.0311 - val_accuracy: 0.8056 - val_loss: 0.7729\n",
      "Epoch 163/200\n",
      "\u001b[1m3/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 1.0000 - loss: 0.0297\n",
      "Epoch 163: val_accuracy did not improve from 0.88889\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 1.0000 - loss: 0.0296 - val_accuracy: 0.8056 - val_loss: 0.7689\n",
      "Epoch 164/200\n",
      "\u001b[1m3/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 1.0000 - loss: 0.0295\n",
      "Epoch 164: val_accuracy did not improve from 0.88889\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 1.0000 - loss: 0.0293 - val_accuracy: 0.8333 - val_loss: 0.7622\n",
      "Epoch 165/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 1.0000 - loss: 0.0287\n",
      "Epoch 165: val_accuracy did not improve from 0.88889\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 1.0000 - loss: 0.0287 - val_accuracy: 0.8333 - val_loss: 0.7632\n",
      "Epoch 166/200\n",
      "\u001b[1m4/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 1.0000 - loss: 0.0270\n",
      "Epoch 166: val_accuracy did not improve from 0.88889\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 1.0000 - loss: 0.0271 - val_accuracy: 0.8333 - val_loss: 0.7582\n",
      "Epoch 167/200\n",
      "\u001b[1m4/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 1.0000 - loss: 0.0282\n",
      "Epoch 167: val_accuracy did not improve from 0.88889\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 1.0000 - loss: 0.0279 - val_accuracy: 0.8056 - val_loss: 0.7414\n",
      "Epoch 168/200\n",
      "\u001b[1m3/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 1.0000 - loss: 0.0286\n",
      "Epoch 168: val_accuracy did not improve from 0.88889\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 1.0000 - loss: 0.0288 - val_accuracy: 0.8333 - val_loss: 0.7326\n",
      "Epoch 169/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 1.0000 - loss: 0.0292\n",
      "Epoch 169: val_accuracy did not improve from 0.88889\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 1.0000 - loss: 0.0291 - val_accuracy: 0.8333 - val_loss: 0.7455\n",
      "Epoch 170/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 1.0000 - loss: 0.0298\n",
      "Epoch 170: val_accuracy did not improve from 0.88889\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 1.0000 - loss: 0.0301 - val_accuracy: 0.8333 - val_loss: 0.7355\n",
      "Epoch 171/200\n",
      "\u001b[1m4/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 1.0000 - loss: 0.0307\n",
      "Epoch 171: val_accuracy did not improve from 0.88889\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 1.0000 - loss: 0.0310 - val_accuracy: 0.8611 - val_loss: 0.6967\n",
      "Epoch 172/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 1.0000 - loss: 0.0394\n",
      "Epoch 172: val_accuracy did not improve from 0.88889\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 1.0000 - loss: 0.0396 - val_accuracy: 0.8611 - val_loss: 0.6993\n",
      "Epoch 173/200\n",
      "\u001b[1m3/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 1.0000 - loss: 0.0417\n",
      "Epoch 173: val_accuracy did not improve from 0.88889\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 1.0000 - loss: 0.0423 - val_accuracy: 0.8056 - val_loss: 0.7045\n",
      "Epoch 174/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 1.0000 - loss: 0.0459\n",
      "Epoch 174: val_accuracy did not improve from 0.88889\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 1.0000 - loss: 0.0461 - val_accuracy: 0.8056 - val_loss: 0.7162\n",
      "Epoch 175/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 1.0000 - loss: 0.0451\n",
      "Epoch 175: val_accuracy did not improve from 0.88889\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 1.0000 - loss: 0.0454 - val_accuracy: 0.7778 - val_loss: 0.7550\n",
      "Epoch 176/200\n",
      "\u001b[1m4/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 1.0000 - loss: 0.0536\n",
      "Epoch 176: val_accuracy did not improve from 0.88889\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 1.0000 - loss: 0.0544 - val_accuracy: 0.8333 - val_loss: 0.8047\n",
      "Epoch 177/200\n",
      "\u001b[1m3/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 1.0000 - loss: 0.0558\n",
      "Epoch 177: val_accuracy did not improve from 0.88889\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 1.0000 - loss: 0.0565 - val_accuracy: 0.8056 - val_loss: 0.8547\n",
      "Epoch 178/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 1.0000 - loss: 0.0603\n",
      "Epoch 178: val_accuracy did not improve from 0.88889\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 1.0000 - loss: 0.0604 - val_accuracy: 0.7778 - val_loss: 0.8880\n",
      "Epoch 179/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 1.0000 - loss: 0.0602\n",
      "Epoch 179: val_accuracy did not improve from 0.88889\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 1.0000 - loss: 0.0601 - val_accuracy: 0.7500 - val_loss: 0.9148\n",
      "Epoch 180/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 1.0000 - loss: 0.0558\n",
      "Epoch 180: val_accuracy did not improve from 0.88889\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - accuracy: 1.0000 - loss: 0.0557 - val_accuracy: 0.8056 - val_loss: 0.9080\n",
      "Epoch 181/200\n",
      "\u001b[1m4/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 1.0000 - loss: 0.0506\n",
      "Epoch 181: val_accuracy did not improve from 0.88889\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 1.0000 - loss: 0.0503 - val_accuracy: 0.8056 - val_loss: 0.8697\n",
      "Epoch 182/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 1.0000 - loss: 0.0439\n",
      "Epoch 182: val_accuracy did not improve from 0.88889\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 1.0000 - loss: 0.0438 - val_accuracy: 0.8333 - val_loss: 0.8243\n",
      "Epoch 183/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 1.0000 - loss: 0.0395\n",
      "Epoch 183: val_accuracy did not improve from 0.88889\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 1.0000 - loss: 0.0393 - val_accuracy: 0.8333 - val_loss: 0.7754\n",
      "Epoch 184/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 1.0000 - loss: 0.0353\n",
      "Epoch 184: val_accuracy did not improve from 0.88889\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 1.0000 - loss: 0.0352 - val_accuracy: 0.8056 - val_loss: 0.7641\n",
      "Epoch 185/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 1.0000 - loss: 0.0313\n",
      "Epoch 185: val_accuracy did not improve from 0.88889\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 1.0000 - loss: 0.0312 - val_accuracy: 0.8056 - val_loss: 0.7946\n",
      "Epoch 186/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 1.0000 - loss: 0.0289\n",
      "Epoch 186: val_accuracy did not improve from 0.88889\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 1.0000 - loss: 0.0289 - val_accuracy: 0.7778 - val_loss: 0.8141\n",
      "Epoch 187/200\n",
      "\u001b[1m3/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 1.0000 - loss: 0.0272\n",
      "Epoch 187: val_accuracy did not improve from 0.88889\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 1.0000 - loss: 0.0270 - val_accuracy: 0.8333 - val_loss: 0.7926\n",
      "Epoch 188/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 1.0000 - loss: 0.0255\n",
      "Epoch 188: val_accuracy did not improve from 0.88889\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 1.0000 - loss: 0.0256 - val_accuracy: 0.8056 - val_loss: 0.7790\n",
      "Epoch 189/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 1.0000 - loss: 0.0266\n",
      "Epoch 189: val_accuracy did not improve from 0.88889\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - accuracy: 1.0000 - loss: 0.0264 - val_accuracy: 0.8056 - val_loss: 0.7476\n",
      "Epoch 190/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 1.0000 - loss: 0.0243\n",
      "Epoch 190: val_accuracy did not improve from 0.88889\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 1.0000 - loss: 0.0243 - val_accuracy: 0.8056 - val_loss: 0.7649\n",
      "Epoch 191/200\n",
      "\u001b[1m4/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 1.0000 - loss: 0.0250\n",
      "Epoch 191: val_accuracy did not improve from 0.88889\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 1.0000 - loss: 0.0249 - val_accuracy: 0.7778 - val_loss: 0.7802\n",
      "Epoch 192/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 1.0000 - loss: 0.0234\n",
      "Epoch 192: val_accuracy did not improve from 0.88889\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 1.0000 - loss: 0.0235 - val_accuracy: 0.8056 - val_loss: 0.7772\n",
      "Epoch 193/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 1.0000 - loss: 0.0227\n",
      "Epoch 193: val_accuracy did not improve from 0.88889\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 1.0000 - loss: 0.0226 - val_accuracy: 0.8056 - val_loss: 0.7691\n",
      "Epoch 194/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 1.0000 - loss: 0.0228\n",
      "Epoch 194: val_accuracy did not improve from 0.88889\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 1.0000 - loss: 0.0230 - val_accuracy: 0.8333 - val_loss: 0.7726\n",
      "Epoch 195/200\n",
      "\u001b[1m3/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 1.0000 - loss: 0.0222\n",
      "Epoch 195: val_accuracy did not improve from 0.88889\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 1.0000 - loss: 0.0222 - val_accuracy: 0.7778 - val_loss: 0.7786\n",
      "Epoch 196/200\n",
      "\u001b[1m4/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 1.0000 - loss: 0.0221\n",
      "Epoch 196: val_accuracy did not improve from 0.88889\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 1.0000 - loss: 0.0222 - val_accuracy: 0.7778 - val_loss: 0.7734\n",
      "Epoch 197/200\n",
      "\u001b[1m3/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 1.0000 - loss: 0.0228\n",
      "Epoch 197: val_accuracy did not improve from 0.88889\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 1.0000 - loss: 0.0226 - val_accuracy: 0.8056 - val_loss: 0.7661\n",
      "Epoch 198/200\n",
      "\u001b[1m3/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 1.0000 - loss: 0.0216\n",
      "Epoch 198: val_accuracy did not improve from 0.88889\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 1.0000 - loss: 0.0220 - val_accuracy: 0.8056 - val_loss: 0.7619\n",
      "Epoch 199/200\n",
      "\u001b[1m4/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 1.0000 - loss: 0.0215\n",
      "Epoch 199: val_accuracy did not improve from 0.88889\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 1.0000 - loss: 0.0217 - val_accuracy: 0.7778 - val_loss: 0.7639\n",
      "Epoch 200/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 1.0000 - loss: 0.0222\n",
      "Epoch 200: val_accuracy did not improve from 0.88889\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 1.0000 - loss: 0.0221 - val_accuracy: 0.7778 - val_loss: 0.7771\n",
      "Akurasi tertinggi di training: 1.0\n",
      "Akurasi tertinggi di validation: 0.8888888955116272\n",
      "Akurasi rata rata di validation: 0.7800000033527613\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=200,\n",
    "    batch_size=32,\n",
    "    callbacks=[checkpoint]\n",
    ")\n",
    "# Mendapatkan akurasi tertinggi di training dan validation\n",
    "max_train_acc = max(history.history['accuracy'])  # Akurasi tertinggi selama pelatihan\n",
    "max_val_acc = max(history.history['val_accuracy'])  # Akurasi tertinggi selama validasi\n",
    "average_val_acc = np.mean(history.history['val_accuracy']) # Akurasi\n",
    "\n",
    "print(f\"Akurasi tertinggi di training: {max_train_acc}\")\n",
    "print(f\"Akurasi tertinggi di validation: {max_val_acc}\")\n",
    "print(f\"Akurasi rata rata di validation: {average_val_acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer saved as 'tokenizer.pickle'\n",
      "Label encoder saved as 'label_encoder.pickle'\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "with open('tokenizer.pickle', 'wb') as handle:\n",
    "    pickle.dump(tokenizer, handle)\n",
    "print(\"Tokenizer saved as 'tokenizer.pickle'\")\n",
    "\n",
    "with open('label_encoder.pickle', 'wb') as handle:\n",
    "    pickle.dump(le, handle)\n",
    "print(\"Label encoder saved as 'label_encoder.pickle'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model('best_model_1.keras')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_tag(text, model):\n",
    "    text = [letters.lower() for letters in text if letters not in string.punctuation]\n",
    "    text = ''.join(text)\n",
    "    print(f'After Remove : {text}')\n",
    "    text = stemmer.stem(text)\n",
    "    print(f'After Stem : {text}')\n",
    "    seq = tokenizer.texts_to_sequences([text])\n",
    "    padded_seq = pad_sequences(seq, maxlen=max_len, padding='post', truncating='post')\n",
    "    \n",
    "    # Prediksi\n",
    "    prediction = model.predict(padded_seq)\n",
    "    \n",
    "    # Menemukan index dengan probabilitas tertinggi\n",
    "    predicted_index = np.argmax(prediction, axis=1)[0]\n",
    "    \n",
    "    # Mengonversi index kembali ke tag asli\n",
    "    predicted_tag = le.inverse_transform([predicted_index])[0]\n",
    "    \n",
    "    print(predicted_tag)\n",
    "    return predicted_tag\n",
    "\n",
    "# Fungsi untuk mendapatkan tag berdasarkan index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Responses:\n",
      "Pencegahan Hipotermia\n",
      "Untuk mencegah hipotermia saat mendaki gunung, penting untuk menjaga suhu tubuh tetap stabil. Gunakan pakaian berlapis yang sesuai dengan kondisi cuaca, mulai dari lapisan dasar yang menyerap keringat (seperti bahan wol atau sintetis), lapisan isolasi (seperti fleece atau down), hingga lapisan luar yang tahan angin dan air. Selalu pastikan untuk menjaga tubuh tetap kering dan mengganti pakaian basah sesegera mungkin. Selain itu, hindari kelelahan berlebihan dan lakukan aktivitas fisik secara teratur untuk menghasilkan panas tubuh.\n",
      "\n",
      "\n",
      "Makanan dan Minuman Hangat\n",
      "Mengonsumsi makanan berkalori tinggi dan minuman hangat sangat membantu dalam menjaga suhu tubuh. Makanan seperti kacang-kacangan, cokelat, atau makanan tinggi karbohidrat memberikan energi yang dibutuhkan tubuh untuk tetap hangat.\n",
      "\n",
      "\n",
      "Perlindungan dari Cuaca Ekstrem\n",
      "Di gunung dengan suhu yang sangat dingin, seperti Gunung Kerinci atau Gunung Rinjani, pastikan untuk menggunakan pelindung angin dan hujan. Gunakan sleeping bag yang sesuai dengan suhu ekstrem dan matras yang memiliki isolasi untuk mencegah kehilangan panas tubuh saat tidur.\n",
      "\n",
      "\n",
      "Tanda-tanda Hipotermia\n",
      "Hipotermia dapat terjadi secara bertahap. Tanda-tandanya meliputi gemetar hebat, kesulitan berbicara, kehilangan koordinasi tubuh, dan kebingungan. Pada tahap lanjut, seseorang mungkin tidak dapat bergerak dengan baik atau kehilangan kesadaran. Pada pendaki pemula, tanda-tanda awal termasuk kesulitan bergerak dan kurangnya energi, yang perlu segera ditanggapi dengan pemanasan tubuh.\n",
      "\n",
      "\n",
      "Risiko Hipotermia\n",
      "Semakin tinggi gunung yang didaki, semakin besar risiko hipotermia, terutama di malam hari saat suhu turun drastis. Gunung dengan ketinggian seperti Gunung Merbabu dan Gunung Prau memiliki suhu yang cukup rendah di malam hari, meskipun sudah mengenakan pakaian tebal. Pemanasan tubuh dengan aktif bergerak atau menggunakan pemanas tangan dapat membantu.\n",
      "\n",
      "\n",
      "Cara Mengatasi Hipotermia\n",
      "Jika merasa mulai kedinginan, segera cari perlindungan dari angin dan hujan. Untuk menghangatkan tubuh dengan cepat, bergeraklah aktif atau gunakan pakaian tambahan dan alat pemanas tubuh seperti pemanas tangan atau botol air panas. Tidur dalam cuaca dingin tanpa perlindungan yang cukup bisa meningkatkan risiko hipotermia, jadi pastikan tempat tidur terlindung dari angin dan kelembapan.\n",
      "\n",
      "\n",
      "Waspadai Hipotermia pada Malam Hari\n",
      "Hipotermia lebih berbahaya di malam hari karena suhu turun lebih drastis. Gunakan waktu siang hari untuk mempersiapkan perlengkapan dan istirahat, agar tubuh tidak terlalu lelah saat malam tiba.\n",
      "\n",
      "\n",
      "Perbedaan Hipotermia dan Kedinginan Biasa\n",
      "Hipotermia berbeda dengan kedinginan biasa karena dampaknya lebih serius dan bisa berbahaya bagi tubuh. Kedinginan biasa hanya menyebabkan rasa tidak nyaman, sementara hipotermia bisa menyebabkan hilangnya kesadaran dan kegagalan organ tubuh.\n",
      "\n",
      "\n",
      "Persiapan untuk Mengatasi Hipotermia\n",
      "Selalu bawa perlengkapan yang tepat untuk mencegah hipotermia, seperti pakaian hangat, sleeping bag yang cocok dengan suhu ekstrem, dan alat pemanas tangan. Jika mendaki di musim hujan atau cuaca dingin, pastikan Anda membawa perlengkapan anti air dan pelindung tubuh untuk tetap kering.\n",
      "\n",
      "\n",
      "Tindakan Terakhir\n",
      "Jika mendaki pada malam yang sangat dingin atau di ketinggian ekstrem, pastikan untuk memantau kondisi tubuh secara rutin dan segera bertindak jika tanda-tanda hipotermia muncul, baik pada diri sendiri atau teman pendaki.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "user_input = input(\"Masukkan pertanyaan: \")\n",
    "predicted_tag = predict_tag(user_input, model)\n",
    "Intent_print = predicted_tag\n",
    "if Intent_print ==  \"Cuaca di Gunung\"  or Intent_print == \"Budaya\" or Intent_print == \"Jalur dan Waktu Pendakian\" or Intent_print == \"Komunitas Pendaki\":\n",
    "    print(\"Pilihlah Subject :\\n\")\n",
    "    print(\"1. Gunung Rinjani :\\n\")\n",
    "    print(\"2. Gunung Bromo :\\n\")\n",
    "    print(\"3. Gunung Merbabu :\\n\")\n",
    "    print(\"4. Gunung Prau :\\n\")\n",
    "    print(\"5. Gunung Ciremai :\\n\")\n",
    "    print(\"6. Gunung Ijen :\\n\")\n",
    "    print(\"7. Gunung Kerinci :\\n\")\n",
    "    nomor_gunung = int(input(\"masukkan nomor gunung: \"))\n",
    "    dictionarty_gunung ={1:\"Gunung Rinjani\",2:\"Gunung Bromo\",3:\"Gunung Merbabu\",4:\"Gunung Prau\",5:\"Gunung Ciremai\",6:\"Gunung Ijen\",7:\"Gunung Kerinci\"}\n",
    "    if Intent_print == \"Budaya\":\n",
    "        if nomor_gunung >= 1 and nomor_gunung <=7:\n",
    "            for item in dataset:\n",
    "                if item[\"intent\"] == Intent_print:\n",
    "                    print(\"\\nResponses:\")\n",
    "                    for response in item[\"response\"]:\n",
    "                        if response == dictionarty_gunung[nomor_gunung]:\n",
    "                            print(item[\"response\"][response])\n",
    "        else:\n",
    "            for item in dataset:\n",
    "                if item[\"intent\"] == Intent_print:\n",
    "                    print(\"\\nResponses:\")\n",
    "                    for response in item[\"response\"]:\n",
    "                        print(f\"{response}\\n\")\n",
    "                        print(f\"{item[\"response\"][response]}\\n\")\n",
    "    else:\n",
    "        if nomor_gunung >= 1 and nomor_gunung <=7:\n",
    "            for item in dataset:\n",
    "                if item[\"intent\"] == Intent_print:\n",
    "                    print(\"\\nResponses:\")\n",
    "                    for mountain in item[\"response\"]:\n",
    "                        if mountain == dictionarty_gunung[nomor_gunung]:\n",
    "                            print(mountain)\n",
    "                            for response in item[\"response\"][mountain]:\n",
    "                                print(f\"{response} : {item[\"response\"][mountain][response]}\")\n",
    "        else:\n",
    "            for item in dataset:\n",
    "                if item[\"intent\"] == Intent_print:\n",
    "                    print(\"\\nResponses:\")\n",
    "                    for mountain in item[\"response\"]:\n",
    "                        print(\"\\n\\n\")\n",
    "                        print(mountain)\n",
    "                        print(\"\\n\")\n",
    "                        for response in item[\"response\"][mountain]:\n",
    "                            print(f\"{response} : {item[\"response\"][mountain][response]}\\n\")\n",
    "    \n",
    "elif Intent_print == \"Salam\" or Intent_print == \"Goodbye\" or Intent_print == \"Thank You\" or Intent_print == \"Confirmation\" or Intent_print == \"Help\" or Intent_print == \"Feedback\": \n",
    "    for item in dataset:\n",
    "        if item[\"intent\"] == Intent_print:\n",
    "            print(\"\\nResponses:\")\n",
    "            random_response = random.choice(item[\"response\"])\n",
    "            print(f'{random_response}')\n",
    "\n",
    "elif Intent_print == \"Kebutuhan\":\n",
    "    for item in dataset:\n",
    "        if item[\"intent\"] == Intent_print:\n",
    "            print(\"\\nResponses:\")\n",
    "            for response in item[\"response\"]:\n",
    "                print(\"\\n\")\n",
    "                print(response)\n",
    "                for poin in item[\"response\"][response]:\n",
    "                    print(f\"-{poin}\")\n",
    "\n",
    "elif Intent_print == \"Peralatan Wajib\" or Intent_print == \"Persiapan cost\":\n",
    "    for item in dataset:\n",
    "        if item[\"intent\"] == Intent_print:\n",
    "            print(\"\\nResponses:\")\n",
    "            print(item[\"response\"])\n",
    "else: \n",
    "    for item in dataset:        \n",
    "        if item[\"intent\"] == Intent_print:\n",
    "            print(\"\\nResponses:\")\n",
    "            for response in item[\"response\"]:\n",
    "                print(response)\n",
    "                print(item[\"response\"][response])\n",
    "                print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pilihlah Subject :\n",
      "\n",
      "1. Gunung Rinjani :\n",
      "\n",
      "2. Gunung Bromo :\n",
      "\n",
      "3. Gunung Merbabu :\n",
      "\n",
      "4. Gunung Prau :\n",
      "\n",
      "5. Gunung Ciremai :\n",
      "\n",
      "6. Gunung Ijen :\n",
      "\n",
      "7. Gunung Kerinci :\n",
      "\n",
      "\n",
      "Responses:\n",
      "jalurpopuler : Gunung Rinjani memiliki jalur populer seperti Senaru dan Sembalun, yang menawarkan pemandangan indah namun menantang.\n",
      "jaluralternatif : Jalur alternatif tersedia di bagian selatan untuk menghindari keramaian, dengan waktu pendakian rata-rata 23 hari.\n",
      "Waktu Pendakian : Pendakian Gunung Rinjani biasanya memakan waktu 23 hari tergantung jalur yang dipilih. Jalur Sembalun lebih landai tetapi lebih panjang, sedangkan jalur Senaru lebih cepat namun menanjak.\n",
      "Waktu terbaik matahari terbit : Waktu terbaik untuk mendaki adalah dini hari jika ingin mencapai Danau Segara Anak saat matahari terbit.\n",
      "Waktu mencapai danau segara anak : Waktu yang diperlukan untuk mencapai Danau Segara Anak adalah sekitar 12 hari tergantung jalur yang dipilih.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Baca file JSON\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
